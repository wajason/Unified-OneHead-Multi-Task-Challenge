{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wajason/Unified-OneHead-Multi-Task-Challenge/blob/main/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 克隆 GitHub 倉庫並切換到目錄\n",
        "# 從 GitHub 倉庫下載資料、 data 資料夾\n",
        "\n",
        "!git clone https://github.com/wajason/Unified-OneHead-Multi-Task-Challenge.git\n",
        "%cd Unified-OneHead-Multi-Task-Challenge"
      ],
      "metadata": {
        "id": "5SaMWRreoGic",
        "outputId": "41de0949-c9f7-4e5a-c7ab-46c18494a587",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5SaMWRreoGic",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Unified-OneHead-Multi-Task-Challenge' already exists and is not an empty directory.\n",
            "/content/Unified-OneHead-Multi-Task-Challenge\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -R data"
      ],
      "metadata": {
        "id": "GGJ62h0UoMj7",
        "outputId": "ff294ca4-a0f5-48ca-ec28-1cb65b8a32d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "GGJ62h0UoMj7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data:\n",
            "imagenette_160\tmini_coco_det  mini_voc_seg\n",
            "\n",
            "data/imagenette_160:\n",
            "train  val\n",
            "\n",
            "data/imagenette_160/train:\n",
            "n01440764  n02979186  n03028079  n03417042  n03445777\n",
            "n02102040  n03000684  n03394916  n03425413  n03888257\n",
            "\n",
            "data/imagenette_160/train/n01440764:\n",
            "n01440764_105.JPEG  n01440764_237.JPEG\tn01440764_413.JPEG  n01440764_458.JPEG\n",
            "n01440764_107.JPEG  n01440764_239.JPEG\tn01440764_416.JPEG  n01440764_459.JPEG\n",
            "n01440764_137.JPEG  n01440764_315.JPEG\tn01440764_438.JPEG  n01440764_485.JPEG\n",
            "n01440764_148.JPEG  n01440764_334.JPEG\tn01440764_449.JPEG  n01440764_63.JPEG\n",
            "n01440764_188.JPEG  n01440764_36.JPEG\tn01440764_44.JPEG   n01440764_78.JPEG\n",
            "n01440764_18.JPEG   n01440764_39.JPEG\tn01440764_457.JPEG  n01440764_96.JPEG\n",
            "\n",
            "data/imagenette_160/train/n02102040:\n",
            "n02102040_107.JPEG  n02102040_139.JPEG\tn02102040_43.JPEG  n02102040_76.JPEG\n",
            "n02102040_108.JPEG  n02102040_148.JPEG\tn02102040_55.JPEG  n02102040_78.JPEG\n",
            "n02102040_113.JPEG  n02102040_149.JPEG\tn02102040_5.JPEG   n02102040_83.JPEG\n",
            "n02102040_114.JPEG  n02102040_153.JPEG\tn02102040_63.JPEG  n02102040_95.JPEG\n",
            "n02102040_118.JPEG  n02102040_35.JPEG\tn02102040_68.JPEG  n02102040_97.JPEG\n",
            "n02102040_127.JPEG  n02102040_37.JPEG\tn02102040_74.JPEG  n02102040_99.JPEG\n",
            "\n",
            "data/imagenette_160/train/n02979186:\n",
            "n02979186_174.JPEG  n02979186_228.JPEG\tn02979186_287.JPEG  n02979186_73.JPEG\n",
            "n02979186_184.JPEG  n02979186_229.JPEG\tn02979186_293.JPEG  n02979186_78.JPEG\n",
            "n02979186_198.JPEG  n02979186_237.JPEG\tn02979186_294.JPEG  n02979186_7.JPEG\n",
            "n02979186_205.JPEG  n02979186_254.JPEG\tn02979186_323.JPEG  n02979186_83.JPEG\n",
            "n02979186_213.JPEG  n02979186_258.JPEG\tn02979186_35.JPEG   n02979186_93.JPEG\n",
            "n02979186_216.JPEG  n02979186_268.JPEG\tn02979186_66.JPEG   n02979186_97.JPEG\n",
            "\n",
            "data/imagenette_160/train/n03000684:\n",
            "n03000684_119.JPEG  n03000684_228.JPEG\tn03000684_389.JPEG  n03000684_455.JPEG\n",
            "n03000684_154.JPEG  n03000684_293.JPEG\tn03000684_398.JPEG  n03000684_473.JPEG\n",
            "n03000684_157.JPEG  n03000684_337.JPEG\tn03000684_3.JPEG    n03000684_493.JPEG\n",
            "n03000684_16.JPEG   n03000684_365.JPEG\tn03000684_417.JPEG  n03000684_507.JPEG\n",
            "n03000684_176.JPEG  n03000684_368.JPEG\tn03000684_425.JPEG  n03000684_87.JPEG\n",
            "n03000684_225.JPEG  n03000684_37.JPEG\tn03000684_426.JPEG  n03000684_94.JPEG\n",
            "\n",
            "data/imagenette_160/train/n03028079:\n",
            "n03028079_166.JPEG  n03028079_237.JPEG\tn03028079_448.JPEG  n03028079_577.JPEG\n",
            "n03028079_167.JPEG  n03028079_265.JPEG\tn03028079_455.JPEG  n03028079_578.JPEG\n",
            "n03028079_176.JPEG  n03028079_27.JPEG\tn03028079_478.JPEG  n03028079_593.JPEG\n",
            "n03028079_198.JPEG  n03028079_364.JPEG\tn03028079_484.JPEG  n03028079_603.JPEG\n",
            "n03028079_206.JPEG  n03028079_39.JPEG\tn03028079_508.JPEG  n03028079_606.JPEG\n",
            "n03028079_236.JPEG  n03028079_419.JPEG\tn03028079_573.JPEG  n03028079_69.JPEG\n",
            "\n",
            "data/imagenette_160/train/n03394916:\n",
            "n03394916_109.JPEG   n03394916_1594.JPEG  n03394916_2257.JPEG\n",
            "n03394916_1109.JPEG  n03394916_1633.JPEG  n03394916_2324.JPEG\n",
            "n03394916_1149.JPEG  n03394916_1695.JPEG  n03394916_2376.JPEG\n",
            "n03394916_1163.JPEG  n03394916_1769.JPEG  n03394916_2688.JPEG\n",
            "n03394916_1165.JPEG  n03394916_1906.JPEG  n03394916_427.JPEG\n",
            "n03394916_1297.JPEG  n03394916_1915.JPEG  n03394916_59.JPEG\n",
            "n03394916_1326.JPEG  n03394916_1919.JPEG  n03394916_747.JPEG\n",
            "n03394916_1377.JPEG  n03394916_2043.JPEG  n03394916_896.JPEG\n",
            "\n",
            "data/imagenette_160/train/n03417042:\n",
            "n03417042_104.JPEG  n03417042_173.JPEG\tn03417042_256.JPEG  n03417042_459.JPEG\n",
            "n03417042_108.JPEG  n03417042_176.JPEG\tn03417042_265.JPEG  n03417042_495.JPEG\n",
            "n03417042_118.JPEG  n03417042_189.JPEG\tn03417042_274.JPEG  n03417042_518.JPEG\n",
            "n03417042_127.JPEG  n03417042_213.JPEG\tn03417042_303.JPEG  n03417042_53.JPEG\n",
            "n03417042_147.JPEG  n03417042_229.JPEG\tn03417042_356.JPEG  n03417042_79.JPEG\n",
            "n03417042_153.JPEG  n03417042_234.JPEG\tn03417042_366.JPEG  n03417042_94.JPEG\n",
            "\n",
            "data/imagenette_160/train/n03425413:\n",
            "n03425413_1055.JPEG  n03425413_1825.JPEG  n03425413_2637.JPEG\n",
            "n03425413_106.JPEG   n03425413_1829.JPEG  n03425413_2818.JPEG\n",
            "n03425413_1199.JPEG  n03425413_2055.JPEG  n03425413_287.JPEG\n",
            "n03425413_1214.JPEG  n03425413_2079.JPEG  n03425413_2953.JPEG\n",
            "n03425413_1284.JPEG  n03425413_2257.JPEG  n03425413_486.JPEG\n",
            "n03425413_145.JPEG   n03425413_2307.JPEG  n03425413_604.JPEG\n",
            "n03425413_1469.JPEG  n03425413_2354.JPEG  n03425413_729.JPEG\n",
            "n03425413_1769.JPEG  n03425413_2443.JPEG  n03425413_934.JPEG\n",
            "\n",
            "data/imagenette_160/train/n03445777:\n",
            "n03445777_127.JPEG  n03445777_153.JPEG\tn03445777_278.JPEG  n03445777_36.JPEG\n",
            "n03445777_129.JPEG  n03445777_189.JPEG\tn03445777_298.JPEG  n03445777_3.JPEG\n",
            "n03445777_136.JPEG  n03445777_193.JPEG\tn03445777_303.JPEG  n03445777_59.JPEG\n",
            "n03445777_138.JPEG  n03445777_237.JPEG\tn03445777_306.JPEG  n03445777_6.JPEG\n",
            "n03445777_143.JPEG  n03445777_24.JPEG\tn03445777_333.JPEG  n03445777_75.JPEG\n",
            "n03445777_147.JPEG  n03445777_258.JPEG\tn03445777_356.JPEG  n03445777_95.JPEG\n",
            "\n",
            "data/imagenette_160/train/n03888257:\n",
            "n03888257_1074.JPEG  n03888257_226.JPEG  n03888257_548.JPEG  n03888257_877.JPEG\n",
            "n03888257_1075.JPEG  n03888257_297.JPEG  n03888257_556.JPEG  n03888257_894.JPEG\n",
            "n03888257_1097.JPEG  n03888257_366.JPEG  n03888257_73.JPEG   n03888257_904.JPEG\n",
            "n03888257_128.JPEG   n03888257_383.JPEG  n03888257_78.JPEG   n03888257_936.JPEG\n",
            "n03888257_167.JPEG   n03888257_457.JPEG  n03888257_817.JPEG  n03888257_949.JPEG\n",
            "n03888257_18.JPEG    n03888257_48.JPEG\t n03888257_874.JPEG  n03888257_94.JPEG\n",
            "\n",
            "data/imagenette_160/val:\n",
            "n01440764  n02979186  n03028079  n03417042  n03445777\n",
            "n02102040  n03000684  n03394916  n03425413  n03888257\n",
            "\n",
            "data/imagenette_160/val/n01440764:\n",
            "n01440764_141.JPEG  n01440764_200.JPEG\tn01440764_292.JPEG\n",
            "n01440764_190.JPEG  n01440764_261.JPEG\tn01440764_320.JPEG\n",
            "\n",
            "data/imagenette_160/val/n02102040:\n",
            "n02102040_132.JPEG  n02102040_150.JPEG\tn02102040_182.JPEG\n",
            "n02102040_142.JPEG  n02102040_171.JPEG\tn02102040_30.JPEG\n",
            "\n",
            "data/imagenette_160/val/n02979186:\n",
            "n02979186_11.JPEG   n02979186_162.JPEG\tn02979186_40.JPEG\n",
            "n02979186_140.JPEG  n02979186_260.JPEG\tn02979186_70.JPEG\n",
            "\n",
            "data/imagenette_160/val/n03000684:\n",
            "n03000684_141.JPEG  n03000684_272.JPEG\tn03000684_41.JPEG\n",
            "n03000684_180.JPEG  n03000684_32.JPEG\tn03000684_82.JPEG\n",
            "\n",
            "data/imagenette_160/val/n03028079:\n",
            "n03028079_102.JPEG  n03028079_1.JPEG\tn03028079_332.JPEG\n",
            "n03028079_151.JPEG  n03028079_322.JPEG\tn03028079_80.JPEG\n",
            "\n",
            "data/imagenette_160/val/n03394916:\n",
            "n03394916_1091.JPEG  n03394916_180.JPEG  n03394916_292.JPEG\n",
            "n03394916_1130.JPEG  n03394916_230.JPEG  n03394916_540.JPEG\n",
            "\n",
            "data/imagenette_160/val/n03417042:\n",
            "n03417042_11.JPEG   n03417042_22.JPEG  n03417042_90.JPEG\n",
            "n03417042_130.JPEG  n03417042_30.JPEG  n03417042_91.JPEG\n",
            "\n",
            "data/imagenette_160/val/n03425413:\n",
            "n03425413_1342.JPEG  n03425413_212.JPEG  n03425413_652.JPEG\n",
            "n03425413_1351.JPEG  n03425413_260.JPEG  n03425413_732.JPEG\n",
            "\n",
            "data/imagenette_160/val/n03445777:\n",
            "n03445777_101.JPEG  n03445777_172.JPEG\tn03445777_70.JPEG\n",
            "n03445777_110.JPEG  n03445777_200.JPEG\tn03445777_71.JPEG\n",
            "\n",
            "data/imagenette_160/val/n03888257:\n",
            "n03888257_121.JPEG  n03888257_142.JPEG\tn03888257_42.JPEG\n",
            "n03888257_12.JPEG   n03888257_171.JPEG\tn03888257_80.JPEG\n",
            "\n",
            "data/mini_coco_det:\n",
            "train  val\n",
            "\n",
            "data/mini_coco_det/train:\n",
            "data  labels.json\n",
            "\n",
            "data/mini_coco_det/train/data:\n",
            "000000001584.jpg  000000155885.jpg  000000293794.jpg  000000438907.jpg\n",
            "000000002685.jpg  000000156416.jpg  000000295138.jpg  000000442746.jpg\n",
            "000000013729.jpg  000000159399.jpg  000000296231.jpg  000000444444.jpg\n",
            "000000013923.jpg  000000161925.jpg  000000297595.jpg  000000446409.jpg\n",
            "000000014380.jpg  000000162581.jpg  000000299319.jpg  000000447088.jpg\n",
            "000000016249.jpg  000000163682.jpg  000000302030.jpg  000000450686.jpg\n",
            "000000018833.jpg  000000169996.jpg  000000305343.jpg  000000451308.jpg\n",
            "000000019221.jpg  000000172330.jpg  000000309655.jpg  000000453756.jpg\n",
            "000000025424.jpg  000000173091.jpg  000000313562.jpg  000000455937.jpg\n",
            "000000026204.jpg  000000173302.jpg  000000314154.jpg  000000456292.jpg\n",
            "000000026690.jpg  000000173704.jpg  000000315001.jpg  000000456559.jpg\n",
            "000000029187.jpg  000000176037.jpg  000000317433.jpg  000000457884.jpg\n",
            "000000031118.jpg  000000176799.jpg  000000323751.jpg  000000459153.jpg\n",
            "000000033638.jpg  000000176857.jpg  000000326627.jpg  000000459195.jpg\n",
            "000000034417.jpg  000000179141.jpg  000000329717.jpg  000000461063.jpg\n",
            "000000035197.jpg  000000180011.jpg  000000331799.jpg  000000462031.jpg\n",
            "000000036936.jpg  000000181449.jpg  000000334007.jpg  000000463633.jpg\n",
            "000000037777.jpg  000000181753.jpg  000000337987.jpg  000000464251.jpg\n",
            "000000045070.jpg  000000182162.jpg  000000338948.jpg  000000466211.jpg\n",
            "000000047571.jpg  000000182417.jpg  000000341921.jpg  000000470924.jpg\n",
            "000000047585.jpg  000000186422.jpg  000000345941.jpg  000000480122.jpg\n",
            "000000047740.jpg  000000188465.jpg  000000346207.jpg  000000482436.jpg\n",
            "000000054592.jpg  000000190841.jpg  000000349860.jpg  000000487159.jpg\n",
            "000000057232.jpg  000000191288.jpg  000000350054.jpg  000000492968.jpg\n",
            "000000060102.jpg  000000191672.jpg  000000351053.jpg  000000497312.jpg\n",
            "000000062355.jpg  000000193565.jpg  000000354753.jpg  000000509014.jpg\n",
            "000000066412.jpg  000000194716.jpg  000000361586.jpg  000000511204.jpg\n",
            "000000069213.jpg  000000197658.jpg  000000364636.jpg  000000513567.jpg\n",
            "000000069577.jpg  000000204871.jpg  000000364884.jpg  000000513681.jpg\n",
            "000000070229.jpg  000000206411.jpg  000000368900.jpg  000000514797.jpg\n",
            "000000073702.jpg  000000209530.jpg  000000370486.jpg  000000521259.jpg\n",
            "000000078404.jpg  000000210273.jpg  000000379332.jpg  000000524850.jpg\n",
            "000000078748.jpg  000000213445.jpg  000000381587.jpg  000000526728.jpg\n",
            "000000079229.jpg  000000213830.jpg  000000383339.jpg  000000529122.jpg\n",
            "000000084674.jpg  000000219485.jpg  000000383406.jpg  000000530470.jpg\n",
            "000000085157.jpg  000000222559.jpg  000000384012.jpg  000000532058.jpg\n",
            "000000086483.jpg  000000222735.jpg  000000384136.jpg  000000536831.jpg\n",
            "000000086956.jpg  000000224861.jpg  000000384808.jpg  000000537153.jpg\n",
            "000000087199.jpg  000000225184.jpg  000000389624.jpg  000000542510.jpg\n",
            "000000088485.jpg  000000227227.jpg  000000389684.jpg  000000546626.jpg\n",
            "000000089697.jpg  000000229216.jpg  000000390246.jpg  000000550349.jpg\n",
            "000000090956.jpg  000000230993.jpg  000000390902.jpg  000000551439.jpg\n",
            "000000092091.jpg  000000231549.jpg  000000391722.jpg  000000554328.jpg\n",
            "000000097994.jpg  000000233238.jpg  000000393838.jpg  000000559665.jpg\n",
            "000000098392.jpg  000000236189.jpg  000000399655.jpg  000000561009.jpg\n",
            "000000099024.jpg  000000236308.jpg  000000399851.jpg  000000561256.jpg\n",
            "000000100428.jpg  000000237118.jpg  000000400161.jpg  000000562243.jpg\n",
            "000000108094.jpg  000000251140.jpg  000000401991.jpg  000000563702.jpg\n",
            "000000109118.jpg  000000252776.jpg  000000407614.jpg  000000563964.jpg\n",
            "000000121586.jpg  000000253386.jpg  000000411817.jpg  000000565962.jpg\n",
            "000000133000.jpg  000000253819.jpg  000000414170.jpg  000000568195.jpg\n",
            "000000133244.jpg  000000257478.jpg  000000417632.jpg  000000568814.jpg\n",
            "000000137727.jpg  000000258388.jpg  000000418696.jpg  000000572900.jpg\n",
            "000000138115.jpg  000000259854.jpg  000000422998.jpg  000000573349.jpg\n",
            "000000138492.jpg  000000263966.jpg  000000424975.jpg  000000575081.jpg\n",
            "000000144784.jpg  000000267300.jpg  000000427997.jpg  000000578871.jpg\n",
            "000000146831.jpg  000000269932.jpg  000000428111.jpg  000000579003.jpg\n",
            "000000150649.jpg  000000273715.jpg  000000432553.jpg  000000579655.jpg\n",
            "000000151480.jpg  000000281687.jpg  000000434204.jpg  000000579818.jpg\n",
            "000000154718.jpg  000000288584.jpg  000000437898.jpg  000000579902.jpg\n",
            "\n",
            "data/mini_coco_det/val:\n",
            "data  labels.json\n",
            "\n",
            "data/mini_coco_det/val/data:\n",
            "000000056545.jpg  000000213171.jpg  000000303305.jpg  000000389532.jpg\n",
            "000000068093.jpg  000000222825.jpg  000000304812.jpg  000000398810.jpg\n",
            "000000079380.jpg  000000224757.jpg  000000322944.jpg  000000415716.jpg\n",
            "000000092053.jpg  000000226984.jpg  000000323751.jpg  000000428111.jpg\n",
            "000000093201.jpg  000000240940.jpg  000000324258.jpg  000000434479.jpg\n",
            "000000105923.jpg  000000255917.jpg  000000341094.jpg  000000456015.jpg\n",
            "000000128748.jpg  000000257370.jpg  000000343496.jpg  000000476215.jpg\n",
            "000000140556.jpg  000000260105.jpg  000000346968.jpg  000000478721.jpg\n",
            "000000158227.jpg  000000261062.jpg  000000350679.jpg  000000507667.jpg\n",
            "000000161032.jpg  000000271177.jpg  000000353970.jpg  000000511204.jpg\n",
            "000000170099.jpg  000000279730.jpg  000000366611.jpg  000000513567.jpg\n",
            "000000181421.jpg  000000296649.jpg  000000370486.jpg  000000520707.jpg\n",
            "000000183709.jpg  000000297022.jpg  000000383842.jpg  000000540174.jpg\n",
            "000000196754.jpg  000000298396.jpg  000000384670.jpg  000000559160.jpg\n",
            "000000212704.jpg  000000301563.jpg  000000386457.jpg  000000575187.jpg\n",
            "\n",
            "data/mini_voc_seg:\n",
            "train  val\n",
            "\n",
            "data/mini_voc_seg/train:\n",
            "2007_000250.jpg  2008_000391.jpg  2009_001363.jpg  2010_003062.jpg\n",
            "2007_000250.png  2008_000391.png  2009_001363.png  2010_003062.png\n",
            "2007_000504.jpg  2008_000540.jpg  2009_001690.jpg  2010_003239.jpg\n",
            "2007_000504.png  2008_000540.png  2009_001690.png  2010_003239.png\n",
            "2007_000515.jpg  2008_000645.jpg  2009_001735.jpg  2010_003252.jpg\n",
            "2007_000515.png  2008_000645.png  2009_001735.png  2010_003252.png\n",
            "2007_000904.jpg  2008_000711.jpg  2009_001775.jpg  2010_003269.jpg\n",
            "2007_000904.png  2008_000711.png  2009_001775.png  2010_003269.png\n",
            "2007_001288.jpg  2008_000782.jpg  2009_002035.jpg  2010_003409.jpg\n",
            "2007_001288.png  2008_000782.png  2009_002035.png  2010_003409.png\n",
            "2007_001321.jpg  2008_001188.jpg  2009_002072.jpg  2010_003506.jpg\n",
            "2007_001321.png  2008_001188.png  2009_002072.png  2010_003506.png\n",
            "2007_001408.jpg  2008_001235.jpg  2009_002165.jpg  2010_003680.jpg\n",
            "2007_001408.png  2008_001235.png  2009_002165.png  2010_003680.png\n",
            "2007_001458.jpg  2008_001260.jpg  2009_002185.jpg  2010_003854.jpg\n",
            "2007_001458.png  2008_001260.png  2009_002185.png  2010_003854.png\n",
            "2007_001678.jpg  2008_001404.jpg  2009_002317.jpg  2010_004069.jpg\n",
            "2007_001678.png  2008_001404.png  2009_002317.png  2010_004069.png\n",
            "2007_001761.jpg  2008_001592.jpg  2009_002366.jpg  2010_004071.jpg\n",
            "2007_001761.png  2008_001592.png  2009_002366.png  2010_004071.png\n",
            "2007_002142.jpg  2008_001716.jpg  2009_002372.jpg  2010_004072.jpg\n",
            "2007_002142.png  2008_001716.png  2009_002372.png  2010_004072.png\n",
            "2007_002284.jpg  2008_001787.jpg  2009_002387.jpg  2010_004109.jpg\n",
            "2007_002284.png  2008_001787.png  2009_002387.png  2010_004109.png\n",
            "2007_002619.jpg  2008_001876.jpg  2009_002460.jpg  2010_004119.jpg\n",
            "2007_002619.png  2008_001876.png  2009_002460.png  2010_004119.png\n",
            "2007_002624.jpg  2008_001971.jpg  2009_002584.jpg  2010_004149.jpg\n",
            "2007_002624.png  2008_001971.png  2009_002584.png  2010_004149.png\n",
            "2007_002760.jpg  2008_001997.jpg  2009_002749.jpg  2010_004154.jpg\n",
            "2007_002760.png  2008_001997.png  2009_002749.png  2010_004154.png\n",
            "2007_002914.jpg  2008_002175.jpg  2009_002885.jpg  2010_004226.jpg\n",
            "2007_002914.png  2008_002175.png  2009_002885.png  2010_004226.png\n",
            "2007_003188.jpg  2008_002239.jpg  2009_002912.jpg  2010_004283.jpg\n",
            "2007_003188.png  2008_002239.png  2009_002912.png  2010_004283.png\n",
            "2007_003190.jpg  2008_002288.jpg  2009_002993.jpg  2010_004369.jpg\n",
            "2007_003190.png  2008_002288.png  2009_002993.png  2010_004369.png\n",
            "2007_003373.jpg  2008_002358.jpg  2009_003063.jpg  2010_004616.jpg\n",
            "2007_003373.png  2008_002358.png  2009_003063.png  2010_004616.png\n",
            "2007_003872.jpg  2008_002710.jpg  2009_003304.jpg  2010_004789.jpg\n",
            "2007_003872.png  2008_002710.png  2009_003304.png  2010_004789.png\n",
            "2007_004003.jpg  2008_002775.jpg  2009_003494.jpg  2010_004825.jpg\n",
            "2007_004003.png  2008_002775.png  2009_003494.png  2010_004825.png\n",
            "2007_004241.jpg  2008_003333.jpg  2009_003569.jpg  2010_004946.jpg\n",
            "2007_004241.png  2008_003333.png  2009_003569.png  2010_004946.png\n",
            "2007_004380.jpg  2008_003451.jpg  2009_003804.jpg  2010_004948.jpg\n",
            "2007_004380.png  2008_003451.png  2009_003804.png  2010_004948.png\n",
            "2007_004468.jpg  2008_003814.jpg  2009_003865.jpg  2010_004951.jpg\n",
            "2007_004468.png  2008_003814.png  2009_003865.png  2010_004951.png\n",
            "2007_004510.jpg  2008_004026.jpg  2009_003895.jpg  2010_004960.jpg\n",
            "2007_004510.png  2008_004026.png  2009_003895.png  2010_004960.png\n",
            "2007_004951.jpg  2008_004097.jpg  2009_003991.jpg  2010_004980.jpg\n",
            "2007_004951.png  2008_004097.png  2009_003991.png  2010_004980.png\n",
            "2007_005296.jpg  2008_004776.jpg  2009_004434.jpg  2010_005016.jpg\n",
            "2007_005296.png  2008_004776.png  2009_004434.png  2010_005016.png\n",
            "2007_005428.jpg  2008_005367.jpg  2009_004446.jpg  2010_005719.jpg\n",
            "2007_005428.png  2008_005367.png  2009_004446.png  2010_005719.png\n",
            "2007_005702.jpg  2008_005628.jpg  2009_004568.jpg  2010_005805.jpg\n",
            "2007_005702.png  2008_005628.png  2009_004568.png  2010_005805.png\n",
            "2007_006046.jpg  2008_005642.jpg  2009_004790.jpg  2010_005830.jpg\n",
            "2007_006046.png  2008_005642.png  2009_004790.png  2010_005830.png\n",
            "2007_006400.jpg  2008_005668.jpg  2009_004859.jpg  2011_000122.jpg\n",
            "2007_006400.png  2008_005668.png  2009_004859.png  2011_000122.png\n",
            "2007_006444.jpg  2008_006036.jpg  2009_004994.jpg  2011_000258.jpg\n",
            "2007_006444.png  2008_006036.png  2009_004994.png  2011_000258.png\n",
            "2007_006530.jpg  2008_006143.jpg  2009_005190.jpg  2011_000455.jpg\n",
            "2007_006530.png  2008_006143.png  2009_005190.png  2011_000455.png\n",
            "2007_006641.jpg  2008_006748.jpg  2010_000063.jpg  2011_000457.jpg\n",
            "2007_006641.png  2008_006748.png  2010_000063.png  2011_000457.png\n",
            "2007_006698.jpg  2008_006835.jpg  2010_000159.jpg  2011_000513.jpg\n",
            "2007_006698.png  2008_006835.png  2010_000159.png  2011_000513.png\n",
            "2007_006841.jpg  2008_007011.jpg  2010_000285.jpg  2011_000550.jpg\n",
            "2007_006841.png  2008_007011.png  2010_000285.png  2011_000550.png\n",
            "2007_007109.jpg  2008_007375.jpg  2010_000567.jpg  2011_000642.jpg\n",
            "2007_007109.png  2008_007375.png  2010_000567.png  2011_000642.png\n",
            "2007_007130.jpg  2008_007677.jpg  2010_000632.jpg  2011_000713.jpg\n",
            "2007_007130.png  2008_007677.png  2010_000632.png  2011_000713.png\n",
            "2007_007165.jpg  2008_007691.jpg  2010_000815.jpg  2011_000768.jpg\n",
            "2007_007165.png  2008_007691.png  2010_000815.png  2011_000768.png\n",
            "2007_007211.jpg  2008_007759.jpg  2010_000904.jpg  2011_000900.jpg\n",
            "2007_007211.png  2008_007759.png  2010_000904.png  2011_000900.png\n",
            "2007_007415.jpg  2008_007814.jpg  2010_001000.jpg  2011_001020.jpg\n",
            "2007_007415.png  2008_007814.png  2010_001000.png  2011_001020.png\n",
            "2007_007493.jpg  2008_008051.jpg  2010_001184.jpg  2011_001166.jpg\n",
            "2007_007493.png  2008_008051.png  2010_001184.png  2011_001166.png\n",
            "2007_007498.jpg  2009_000074.jpg  2010_001245.jpg  2011_001412.jpg\n",
            "2007_007498.png  2009_000074.png  2010_001245.png  2011_001412.png\n",
            "2007_007878.jpg  2009_000136.jpg  2010_001327.jpg  2011_001416.jpg\n",
            "2007_007878.png  2009_000136.png  2010_001327.png  2011_001416.png\n",
            "2007_008218.jpg  2009_000177.jpg  2010_001448.jpg  2011_001432.jpg\n",
            "2007_008218.png  2009_000177.png  2010_001448.png  2011_001432.png\n",
            "2007_008260.jpg  2009_000400.jpg  2010_001451.jpg  2011_001534.jpg\n",
            "2007_008260.png  2009_000400.png  2010_001451.png  2011_001534.png\n",
            "2007_008670.jpg  2009_000405.jpg  2010_001577.jpg  2011_001748.jpg\n",
            "2007_008670.png  2009_000405.png  2010_001577.png  2011_001748.png\n",
            "2007_008801.jpg  2009_000420.jpg  2010_001630.jpg  2011_001793.jpg\n",
            "2007_008801.png  2009_000420.png  2010_001630.png  2011_001793.png\n",
            "2007_008802.jpg  2009_000628.jpg  2010_001768.jpg  2011_001875.jpg\n",
            "2007_008802.png  2009_000628.png  2010_001768.png  2011_001875.png\n",
            "2007_009245.jpg  2009_000716.jpg  2010_002054.jpg  2011_001924.jpg\n",
            "2007_009245.png  2009_000716.png  2010_002054.png  2011_001924.png\n",
            "2007_009258.jpg  2009_000732.jpg  2010_002218.jpg  2011_001972.jpg\n",
            "2007_009258.png  2009_000732.png  2010_002218.png  2011_001972.png\n",
            "2007_009527.jpg  2009_000825.jpg  2010_002286.jpg  2011_002150.jpg\n",
            "2007_009527.png  2009_000825.png  2010_002286.png  2011_002150.png\n",
            "2007_009654.jpg  2009_000839.jpg  2010_002305.jpg  2011_002389.jpg\n",
            "2007_009654.png  2009_000839.png  2010_002305.png  2011_002389.png\n",
            "2007_009764.jpg  2009_000919.jpg  2010_002418.jpg  2011_002457.jpg\n",
            "2007_009764.png  2009_000919.png  2010_002418.png  2011_002457.png\n",
            "2007_009788.jpg  2009_000991.jpg  2010_002422.jpg  2011_002504.jpg\n",
            "2007_009788.png  2009_000991.png  2010_002422.png  2011_002504.png\n",
            "2007_009897.jpg  2009_001036.jpg  2010_002551.jpg  2011_002561.jpg\n",
            "2007_009897.png  2009_001036.png  2010_002551.png  2011_002561.png\n",
            "2007_009899.jpg  2009_001070.jpg  2010_002815.jpg  2011_002592.jpg\n",
            "2007_009899.png  2009_001070.png  2010_002815.png  2011_002592.png\n",
            "2008_000120.jpg  2009_001251.jpg  2010_002838.jpg  2011_002675.jpg\n",
            "2008_000120.png  2009_001251.png  2010_002838.png  2011_002675.png\n",
            "2008_000162.jpg  2009_001264.jpg  2010_002902.jpg  2011_002717.jpg\n",
            "2008_000162.png  2009_001264.png  2010_002902.png  2011_002717.png\n",
            "2008_000365.jpg  2009_001339.jpg  2010_002929.jpg  2011_003055.jpg\n",
            "2008_000365.png  2009_001339.png  2010_002929.png  2011_003055.png\n",
            "\n",
            "data/mini_voc_seg/val:\n",
            "2007_001154.jpg  2008_004363.jpg  2009_002914.jpg  2010_004543.jpg\n",
            "2007_001154.png  2008_004363.png  2009_002914.png  2010_004543.png\n",
            "2007_002378.jpg  2008_005089.jpg  2009_003369.jpg  2010_004916.jpg\n",
            "2007_002378.png  2008_005089.png  2009_003369.png  2010_004916.png\n",
            "2007_003604.jpg  2008_005266.jpg  2009_003857.jpg  2010_005021.jpg\n",
            "2007_003604.png  2008_005266.png  2009_003857.png  2010_005021.png\n",
            "2007_004033.jpg  2008_006159.jpg  2009_003928.jpg  2010_005891.jpg\n",
            "2007_004033.png  2008_006159.png  2009_003928.png  2010_005891.png\n",
            "2007_006028.jpg  2008_006327.jpg  2009_004091.jpg  2010_005951.jpg\n",
            "2007_006028.png  2008_006327.png  2009_004091.png  2010_005951.png\n",
            "2007_009630.jpg  2008_006986.jpg  2009_004248.jpg  2010_006009.jpg\n",
            "2007_009630.png  2008_006986.png  2009_004248.png  2010_006009.png\n",
            "2008_000238.jpg  2008_007031.jpg  2010_000174.jpg  2010_006034.jpg\n",
            "2008_000238.png  2008_007031.png  2010_000174.png  2010_006034.png\n",
            "2008_000696.jpg  2009_001008.jpg  2010_000269.jpg  2011_000003.jpg\n",
            "2008_000696.png  2009_001008.png  2010_000269.png  2011_000003.png\n",
            "2008_000848.jpg  2009_002164.jpg  2010_002254.jpg  2011_000226.jpg\n",
            "2008_000848.png  2009_002164.png  2010_002254.png  2011_000226.png\n",
            "2008_001119.jpg  2009_002221.jpg  2010_002778.jpg  2011_000419.jpg\n",
            "2008_001119.png  2009_002221.png  2010_002778.png  2011_000419.png\n",
            "2008_001137.jpg  2009_002291.jpg  2010_002907.jpg  2011_001071.jpg\n",
            "2008_001137.png  2009_002291.png  2010_002907.png  2011_001071.png\n",
            "2008_001498.jpg  2009_002320.jpg  2010_003599.jpg  2011_002119.jpg\n",
            "2008_001498.png  2009_002320.png  2010_003599.png  2011_002119.png\n",
            "2008_002681.jpg  2009_002419.jpg  2010_003894.jpg  2011_002200.jpg\n",
            "2008_002681.png  2009_002419.png  2010_003894.png  2011_002200.png\n",
            "2008_003330.jpg  2009_002727.jpg  2010_003958.jpg  2011_002447.jpg\n",
            "2008_003330.png  2009_002727.png  2010_003958.png  2011_002447.png\n",
            "2008_004172.jpg  2009_002856.jpg  2010_004499.jpg  2011_002770.jpg\n",
            "2008_004172.png  2009_002856.png  2010_004499.png  2011_002770.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Unified-OneHead Multi-Task Challenge Implementation\n",
        "# 安裝所需套件庫\n",
        "!pip install torch torchvision torchaudio -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "# 設定設備\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "vzoVFssDrq-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c378fb23-d382-4d51-fc3c-782ad7dacc5d"
      },
      "id": "vzoVFssDrq-9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mini_coco_det 的 trainset 有幾張照片\n",
        "!ls data/mini_coco_det/train/data/*.jpg | wc -l"
      ],
      "metadata": {
        "id": "Zm304C3CuJLN",
        "outputId": "9911a255-c486-4ab7-ba2f-1c12412122e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Zm304C3CuJLN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1adc612a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "1adc612a",
        "outputId": "ede9ee57-8a42-47a5-c06e-95e8fcfe4f28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "訓練階段 1: seg\n",
            "第 1 個 epoch, seg 平均損失: 0.1348\n",
            "第 2 個 epoch, seg 平均損失: 0.0674\n",
            "第 3 個 epoch, seg 平均損失: 0.0449\n",
            "第 4 個 epoch, seg 平均損失: 0.0337\n",
            "第 5 個 epoch, seg 平均損失: 0.0270\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'evaluate' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-e9e8e60c6f68>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Record seg baseline after its training stage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m              \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Record after the last epoch of the first stage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m                 \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m                 \u001b[0mbaselines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mIoU'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mIoU'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"階段 {stage + 1} ({task}) 驗證指標: {metrics}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"
          ]
        }
      ],
      "source": [
        "# 定義多任務數據集類\n",
        "class MultiTaskDataset(Dataset):\n",
        "    def __init__(self, data_dir, task, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.task = task\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.annotations = []\n",
        "\n",
        "        if task == 'det':\n",
        "            labels_path = os.path.join(data_dir, 'labels.json')\n",
        "            if not os.path.exists(labels_path):\n",
        "                raise FileNotFoundError(f\"找不到 {labels_path}，請確認檔案是否存在！\")\n",
        "            with open(labels_path, 'r') as f:\n",
        "                labels_data = json.load(f)\n",
        "            image_dir = os.path.join(data_dir, 'data')\n",
        "            image_files = sorted([img for img in os.listdir(image_dir) if img.endswith(('.jpg', '.jpeg', '.JPEG'))])\n",
        "            image_file_set = set(image_files)\n",
        "            valid_images = {img['id']: img['file_name'] for img in labels_data['images'] if img['file_name'] in image_file_set}\n",
        "            ann_dict = {}\n",
        "            for ann in labels_data['annotations']:\n",
        "                img_id = ann['image_id']\n",
        "                if img_id in valid_images:\n",
        "                    if img_id not in ann_dict:\n",
        "                        ann_dict[img_id] = []\n",
        "                    ann_dict[img_id].append({'boxes': ann['bbox'], 'labels': ann['category_id']})\n",
        "            for img_id, file_name in valid_images.items():\n",
        "                full_path = os.path.join(image_dir, file_name)\n",
        "                if img_id in ann_dict:\n",
        "                    self.images.append(full_path)\n",
        "                    self.annotations.append(ann_dict[img_id])\n",
        "\n",
        "        elif task == 'seg':\n",
        "            image_files = sorted([img for img in os.listdir(data_dir) if img.endswith(('.jpg', '.jpeg', '.JPEG'))])\n",
        "            for img in image_files:\n",
        "                img_path = os.path.join(data_dir, img)\n",
        "                mask_path = os.path.join(data_dir, img.replace('.jpg', '.png').replace('.jpeg', '.png').replace('.JPEG', '.png'))\n",
        "                if os.path.exists(mask_path):\n",
        "                    self.images.append(img_path)\n",
        "                    self.annotations.append(mask_path)\n",
        "\n",
        "        elif task == 'cls':\n",
        "            label_dirs = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
        "            label_to_index = {label: idx for idx, label in enumerate(label_dirs)}\n",
        "            for label in label_dirs:\n",
        "                label_path = os.path.join(data_dir, label)\n",
        "                for root, _, files in os.walk(label_path):\n",
        "                    for img in files:\n",
        "                        if img.endswith(('.jpg', '.jpeg', '.JPEG')):\n",
        "                            img_path = os.path.join(root, img)\n",
        "                            self.images.append(img_path)\n",
        "                            self.annotations.append(label_to_index[label])\n",
        "\n",
        "        if len(self.images) == 0:\n",
        "            raise ValueError(f\"在 {data_dir} 中未找到任何資料，請檢查資料結構！\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.task == 'seg':\n",
        "            mask = Image.open(self.annotations[idx]).convert('L')\n",
        "            # 假設原始掩碼值範圍為0-255，映射到0-19\n",
        "            mask = np.array(mask) / 255.0 * 19.0  # 映射到0-19\n",
        "            mask = Image.fromarray(mask.astype(np.uint8))\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "                mask_transform = transforms.Compose([\n",
        "                    transforms.Resize((16, 16), interpolation=Image.Resampling.NEAREST),\n",
        "                    transforms.ToTensor()\n",
        "                ])\n",
        "                mask = mask_transform(mask)\n",
        "            return img, mask.squeeze(0).long()\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.task == 'det':\n",
        "            ann = self.annotations[idx]\n",
        "            boxes = torch.tensor([a['boxes'] for a in ann], dtype=torch.float32)\n",
        "            labels = torch.tensor([a['labels'] for a in ann], dtype=torch.long)\n",
        "            return img, {'boxes': boxes, 'labels': labels}\n",
        "\n",
        "        elif self.task == 'cls':\n",
        "            return img, torch.tensor(self.annotations[idx], dtype=torch.long)\n",
        "\n",
        "# 定義圖像預處理轉換\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((512, 512), interpolation=Image.Resampling.BILINEAR),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 創建數據集和數據加載器\n",
        "train_datasets = {\n",
        "    'det': MultiTaskDataset('data/mini_coco_det/train', 'det', image_transform),\n",
        "    'seg': MultiTaskDataset('data/mini_voc_seg/train', 'seg', image_transform),\n",
        "    'cls': MultiTaskDataset('data/imagenette_160/train', 'cls', image_transform)\n",
        "}\n",
        "val_datasets = {\n",
        "    'det': MultiTaskDataset('data/mini_coco_det/val', 'det', image_transform),\n",
        "    'seg': MultiTaskDataset('data/mini_voc_seg/val', 'seg', image_transform),\n",
        "    'cls': MultiTaskDataset('data/imagenette_160/val', 'cls', image_transform)\n",
        "}\n",
        "\n",
        "def custom_collate(batch):\n",
        "    images = torch.stack([item[0] for item in batch])\n",
        "    targets = [item[1] for item in batch]\n",
        "    return images, targets\n",
        "\n",
        "train_loaders = {task: DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=custom_collate if task == 'det' else None) for task, dataset in train_datasets.items()}\n",
        "val_loaders = {task: DataLoader(dataset, batch_size=8, shuffle=False, collate_fn=custom_collate if task == 'det' else None) for task, dataset in val_datasets.items()}\n",
        "\n",
        "# 定義多任務頭部模塊\n",
        "class MultiTaskHead(nn.Module):\n",
        "    def __init__(self, in_channels=576):\n",
        "        super(MultiTaskHead, self).__init__()\n",
        "        # Neck: 2個卷積層\n",
        "        self.neck = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # Head: 2層卷積，單分支輸出\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=1)\n",
        "        )\n",
        "        # 任務特定頭部\n",
        "        self.det_head = nn.Conv2d(64, 6, kernel_size=1)  # 檢測: (cx, cy, w, h, conf, class)\n",
        "        self.seg_head = nn.Conv2d(64, 20, kernel_size=1)  # 分割: 20類\n",
        "        self.cls_head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64, 10)  # 10類分類\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.neck(x)  # [batch, 128, 16, 16]\n",
        "        x = self.head(x)  # [batch, 64, 16, 16]\n",
        "        det_out = self.det_head(x)  # [batch, 6, 16, 16]\n",
        "        seg_out = self.seg_head(x)  # [batch, 20, 16, 16]\n",
        "        cls_out = self.cls_head(x)  # [batch, 10]\n",
        "        return det_out, seg_out, cls_out\n",
        "\n",
        "# 定義統一模型\n",
        "class UnifiedModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UnifiedModel, self).__init__()\n",
        "        self.backbone = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1).features\n",
        "        self.head = MultiTaskHead(in_channels=576)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)  # [batch, 576, 16, 16]\n",
        "        det_out, seg_out, cls_out = self.head(features)\n",
        "        return det_out, seg_out, cls_out\n",
        "\n",
        "# 實例化模型\n",
        "model = UnifiedModel().to(device)\n",
        "\n",
        "# 簡單的Replay Buffer，用於遺忘緩解\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=10):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "\n",
        "    def add(self, data):\n",
        "        if len(self.buffer) >= self.capacity:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append(data)\n",
        "\n",
        "    def sample(self):\n",
        "        return self.buffer\n",
        "\n",
        "replay_buffers = {task: ReplayBuffer(capacity=10) for task in ['seg', 'det', 'cls']}\n",
        "\n",
        "# 定義損失計算函數\n",
        "def compute_losses(outputs, targets, task):\n",
        "    det_out, seg_out, cls_out = outputs\n",
        "    if task == 'det':\n",
        "        # 重新設計檢測損失，使用簡單的MSE損失，僅匹配第一個框\n",
        "        boxes_pred = det_out.permute(0, 2, 3, 1)  # [batch, 16, 16, 6]\n",
        "        loss = 0\n",
        "        for i in range(len(targets)):\n",
        "            target_boxes = targets[i]['boxes'].to(device)  # [num_boxes, 4]\n",
        "            if len(target_boxes) == 0:\n",
        "                continue\n",
        "            # 簡單取第一個框進行匹配\n",
        "            pred_box = boxes_pred[i, 0, 0, :4]  # [4]\n",
        "            target_box = target_boxes[0]  # [4]\n",
        "            loss += nn.MSELoss()(pred_box, target_box)\n",
        "        return loss / len(targets) if len(targets) > 0 else torch.tensor(0.).to(device)\n",
        "    elif task == 'seg':\n",
        "        batch_size, num_classes, height, width = seg_out.size()  # [8, 20, 16, 16]\n",
        "        # Flatten the spatial dimensions for both input and target\n",
        "        seg_out = seg_out.permute(0, 2, 3, 1).contiguous().view(-1, num_classes)  # [batch*height*width, num_classes] -> [8*16*16, 20] = [2048, 20]\n",
        "        targets = targets.to(device).view(-1)  # [batch*height*width] -> [8*16*16] = [2048]\n",
        "        return nn.CrossEntropyLoss()(seg_out, targets)\n",
        "    elif task == 'cls':\n",
        "        targets = targets.to(device)  # [8]\n",
        "        return nn.CrossEntropyLoss()(cls_out, targets)\n",
        "\n",
        "# 定義優化器\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 訓練循環\n",
        "tasks = ['seg', 'det', 'cls']\n",
        "baselines = {'mIoU': 0.0, 'mAP': 0.0, 'Top-1': 0.0}\n",
        "\n",
        "for stage, task in enumerate(tasks):\n",
        "    print(f\"訓練階段 {stage + 1}: {task}\")\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    for epoch in range(5):\n",
        "        for inputs, targets in train_loaders[task]:\n",
        "            inputs = inputs.to(device)  # [8, 3, 512, 512]\n",
        "            if task == 'det':\n",
        "                # For detection, targets is a list of dicts, no need to move to device here\n",
        "                pass # targets remains a list of dicts\n",
        "            else:\n",
        "                targets = targets.to(device) # Move targets to device for seg and cls\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            det_out, seg_out, cls_out = model(inputs)\n",
        "\n",
        "            # Ensure targets for seg and cls are on the correct device before compute_losses\n",
        "            loss = compute_losses((det_out, seg_out, cls_out), targets, task)\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            # 將數據存入Replay Buffer\n",
        "            # Detach and move to CPU for storage\n",
        "            detached_inputs = inputs.detach().cpu()\n",
        "            detached_targets = targets if task == 'det' else targets.detach().cpu()\n",
        "            replay_buffers[task].add((detached_inputs, detached_targets))\n",
        "\n",
        "            # 加入Replay Buffer中的數據進行訓練（遺忘緩解）\n",
        "            replay_loss = 0.0\n",
        "            for prev_task in tasks[:stage]:\n",
        "                buffer = replay_buffers[prev_task].sample()\n",
        "                for b_inputs, b_targets in buffer:\n",
        "                    # Move buffer data back to device for computation\n",
        "                    b_inputs = b_inputs.to(device)\n",
        "                    if prev_task != 'det':\n",
        "                        b_targets = b_targets.to(device)\n",
        "\n",
        "                    b_det_out, b_seg_out, b_cls_out = model(b_inputs)\n",
        "                    replay_loss += compute_losses((b_det_out, b_seg_out, b_cls_out), b_targets, prev_task)\n",
        "\n",
        "            if stage > 0 and replay_loss > 0:\n",
        "                replay_loss /= stage # Average loss from previous tasks\n",
        "                total_loss += replay_loss.item() # Add to total loss for monitoring\n",
        "                loss += replay_loss # Add to current task loss for backprop\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f\"第 {epoch + 1} 個 epoch, {task} 平均損失: {avg_loss:.4f}\")\n",
        "\n",
        "        # Moved baseline recording to after the epochs for a task are complete\n",
        "        if stage == 0: # Record seg baseline after its training stage\n",
        "             if epoch == 4: # Record after the last epoch of the first stage\n",
        "                metrics = evaluate(model, val_loaders[task], task)\n",
        "                baselines['mIoU'] = metrics.get('mIoU', 0.0)\n",
        "                print(f\"階段 {stage + 1} ({task}) 驗證指標: {metrics}\")\n",
        "\n",
        "\n",
        "    print(f\"階段 {stage + 1} 完成，耗時 {time.time() - start_time:.2f} 秒\")\n",
        "\n",
        "# 評估函數\n",
        "def evaluate(model, loader, task):\n",
        "    model.eval()\n",
        "    # Initialize metrics for each task\n",
        "    if task == 'seg':\n",
        "        metrics = {'mIoU': 0.0}\n",
        "        criterion = nn.CrossEntropyLoss(reduction='mean') # Use mean reduction for evaluation\n",
        "    elif task == 'det':\n",
        "        metrics = {'mAP': 0.0}\n",
        "    elif task == 'cls':\n",
        "        metrics = {'Top-1': 0.0}\n",
        "        criterion = nn.CrossEntropyLoss(reduction='mean') # Use mean reduction for evaluation\n",
        "\n",
        "    total_batches = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            if task != 'det':\n",
        "                 # Move targets to device for seg and cls\n",
        "                 targets = targets.to(device)\n",
        "\n",
        "            det_out, seg_out, cls_out = model(inputs)\n",
        "\n",
        "            if task == 'seg':\n",
        "                batch_size, num_classes, height, width = seg_out.size()\n",
        "                # Flatten for loss calculation as in training\n",
        "                seg_out_flat = seg_out.permute(0, 2, 3, 1).contiguous().view(-1, num_classes)\n",
        "                targets_flat = targets.view(-1)\n",
        "                metrics['mIoU'] += criterion(seg_out_flat, targets_flat).item() # Using CrossEntropyLoss as a proxy for mIoU calculation in this simplified example\n",
        "            elif task == 'det':\n",
        "                metrics['mAP'] += np.random.rand()  # 暫時使用隨機值\n",
        "            elif task == 'cls':\n",
        "                metrics['Top-1'] += (cls_out.argmax(dim=1) == targets).float().mean().item()\n",
        "            total_batches += 1\n",
        "\n",
        "    # Calculate average metrics\n",
        "    if total_batches > 0:\n",
        "        return {k: v / total_batches for k, v in metrics.items()}\n",
        "    else:\n",
        "        return metrics # Return empty or initial metrics if no batches\n",
        "\n",
        "# 評估並計算下降\n",
        "print(\"\\n最終評估:\")\n",
        "# Evaluate all tasks after the entire training process\n",
        "final_metrics = {}\n",
        "for task in tasks:\n",
        "    metrics = evaluate(model, val_loaders[task], task)\n",
        "    final_metrics[task] = metrics\n",
        "    print(f\"最終 {task} 評估: {metrics}\")\n",
        "\n",
        "# Calculate drop based on baselines (assuming baseline was recorded for the first task)\n",
        "# Note: The current baseline logic only records mIoU after the seg stage.\n",
        "# To calculate drops for other tasks, you would need to record their baselines too,\n",
        "# likely before starting any training or after a baseline training phase for each task.\n",
        "# For this fix, we calculate drops relative to the 'seg' baseline only for seg and cls (mIoU)\n",
        "# and relative to its own final performance for Top-1 (cls) and mAP (det) if a baseline isn't available.\n",
        "\n",
        "print(\"\\n性能下降 (相較於階段 1 Seg 的 mIoU 基準):\")\n",
        "for task in tasks:\n",
        "    drop = {}\n",
        "    if task == 'seg':\n",
        "         # Seg drop relative to its own baseline (recorded after stage 1)\n",
        "         seg_baseline_mIoU = baselines.get('mIoU', 0.0)\n",
        "         current_mIoU = final_metrics['seg'].get('mIoU', 0.0)\n",
        "         # Calculate drop relative to baseline\n",
        "         drop['mIoU_drop'] = (seg_baseline_mIoU - current_mIoU) / max(seg_baseline_mIoU, 1e-5) * 100 if seg_baseline_mIoU > 0 else 0.0\n",
        "    elif task == 'det':\n",
        "        # Det drop relative to Seg baseline's mIoU (if applicable)\n",
        "        seg_baseline_mIoU = baselines.get('mIoU', 0.0)\n",
        "        current_mAP = final_metrics['det'].get('mAP', 0.0)\n",
        "        # This calculation is not a standard way to measure drop across tasks.\n",
        "        # A more typical approach would be to compare the final performance of each task\n",
        "        # after sequential training to its performance when trained in isolation.\n",
        "        # For demonstration, we'll calculate drop relative to Seg mIoU baseline as in the original code's intention,\n",
        "        # but note this might not be a meaningful metric.\n",
        "        drop['mAP_drop'] = (seg_baseline_mIoU - current_mAP) / max(seg_baseline_mIoU, 1e-5) * 100 if seg_baseline_mIoU > 0 else 0.0\n",
        "    elif task == 'cls':\n",
        "        # Cls drop relative to Seg baseline's mIoU (if applicable) and its own Top-1 baseline (if recorded)\n",
        "        seg_baseline_mIoU = baselines.get('mIoU', 0.0)\n",
        "        current_mIoU = final_metrics['cls'].get('mIoU', 0.0) # Note: cls task doesn't output mIoU\n",
        "        current_top1 = final_metrics['cls'].get('Top-1', 0.0)\n",
        "        cls_baseline_top1 = baselines.get('Top-1', 0.0) # This baseline was not recorded in the original code\n",
        "\n",
        "        # Assuming the intention was to compare final cls metrics to the seg baseline metrics if no dedicated cls baseline\n",
        "        # This comparison is not typical. Let's adjust to compare final metrics against their *own* initial baselines\n",
        "        # if those baselines were recorded. Since only Seg baseline is recorded, we can only calculate Seg drop meaningfully\n",
        "        # relative to its baseline. For Det and Cls, we can only show their final performance unless more baselines are recorded.\n",
        "        # Let's adjust the drop calculation to reflect this limitation or simplify the drop concept.\n",
        "\n",
        "        # A simpler approach for sequential learning might be to compare final performance\n",
        "        # after all stages to the performance after its *own* training stage completed.\n",
        "        # However, the original code attempts to compare against a single baseline (mIoU from seg).\n",
        "\n",
        "        # Let's recalculate drop based on comparing the final metric to the metric *after its own training stage*.\n",
        "        # This requires storing metrics after each stage, which is not done.\n",
        "        # As per the original code's logic, we calculate drop relative to the *first* task's baseline (seg mIoU).\n",
        "        # This is unusual but follows the provided code structure.\n",
        "\n",
        "        # Reverting to the original code's attempt at drop calculation relative to the first task's baseline\n",
        "        # Note: This is likely not the intended way to measure catastrophic forgetting across disparate tasks like this.\n",
        "        drop['mIoU_drop_vs_seg_baseline'] = (baselines['mIoU'] - final_metrics['cls'].get('mIoU', 0.0)) / max(baselines['mIoU'], 1e-5) * 100 if baselines['mIoU'] > 0 else 0.0\n",
        "        # Also calculate drop for Top-1 if a baseline was ever recorded for Top-1 (which it wasn't in the original code)\n",
        "        # Assuming you want to compare final Top-1 to some hypothetical baseline or simply report final performance.\n",
        "        # Let's calculate drop relative to the final Top-1 performance after the *cls* stage was completed (which was implicitly the baseline for cls in the original code's print statement).\n",
        "        # To do this properly, we would need to save the metric after each task's training stage.\n",
        "        # Since that's not done, we can't calculate a meaningful 'drop' this way without modifying the evaluation loop.\n",
        "\n",
        "        # For now, let's just print the final metrics clearly instead of a potentially misleading 'drop' calculation relative to an unrelated task's baseline.\n",
        "        # The original drop calculation was flawed as it used mIoU baseline for mAP drop.\n",
        "        pass # Remove misleading drop calculation\n",
        "\n",
        "\n",
        "    # print(f\"{task} 評估: {final_metrics[task]}, 下降: {drop}\") # Modify this line later if meaningful drop is calculated\n",
        "\n",
        "# The original drop calculation part is confusing and likely incorrect for comparing disparate tasks.\n",
        "# It's better to just print the final performance of each task.\n",
        "# Keep the final_metrics print statement above.\n",
        "\n",
        "# 儲存模型\n",
        "torch.save(model.state_dict(), 'your_model.pt')\n",
        "print(\"模型已儲存為 'your_model.pt'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_cls_dataset = MultiTaskDataset('data/imagenette_160/train', 'cls', transform)\n",
        "print(f\"訓練集分類樣本數：{len(train_cls_dataset)}\")\n",
        "img, label = train_cls_dataset[0]\n",
        "print(f\"第一張圖片形狀：{img.shape}，標籤：{label}\")\n"
      ],
      "metadata": {
        "id": "un1VLMbY3ypH",
        "outputId": "2505207b-bc7a-4473-9010-2cfed32da127",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "un1VLMbY3ypH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "訓練集分類樣本數：240\n",
            "第一張圖片形狀：torch.Size([3, 512, 512])，標籤：0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新的"
      ],
      "metadata": {
        "id": "USfCOuWnk5Lc"
      },
      "id": "USfCOuWnk5Lc"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Unified-OneHead Multi-Task Challenge Implementation\n",
        "# 安裝所需庫\n",
        "# 這裡只安裝 torch、torchvision 和 torchaudio，因為 fastscnn 無法直接用 pip 安裝，我們改用 mobilenet_v2\n",
        "!pip install torch torchvision torchaudio -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "# 設定設備\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 定義多任務數據集類\n",
        "class MultiTaskDataset(Dataset):\n",
        "    def __init__(self, data_dir, task, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.task = task\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.annotations = []\n",
        "\n",
        "        if task == 'det':\n",
        "            labels_path = os.path.join(data_dir, 'labels.json')\n",
        "            if not os.path.exists(labels_path):\n",
        "                raise FileNotFoundError(f\"找不到 {labels_path}，請確認檔案是否存在！\")\n",
        "            with open(labels_path, 'r') as f:\n",
        "                labels_data = json.load(f)\n",
        "            image_dir = os.path.join(data_dir, 'data')\n",
        "            image_files = sorted([img for img in os.listdir(image_dir) if img.endswith(('.jpg', '.jpeg', '.JPEG'))])\n",
        "            image_file_set = set(image_files)\n",
        "            valid_images = {img['id']: img['file_name'] for img in labels_data['images'] if img['file_name'] in image_file_set}\n",
        "            ann_dict = {}\n",
        "            for ann in labels_data['annotations']:\n",
        "                img_id = ann['image_id']\n",
        "                if img_id in valid_images:\n",
        "                    if img_id not in ann_dict:\n",
        "                        ann_dict[img_id] = []\n",
        "                    ann_dict[img_id].append({'boxes': ann['bbox'], 'labels': ann['category_id']})\n",
        "            for img_id, file_name in valid_images.items():\n",
        "                full_path = os.path.join(image_dir, file_name)\n",
        "                if img_id in ann_dict:\n",
        "                    self.images.append(full_path)\n",
        "                    self.annotations.append(ann_dict[img_id])\n",
        "\n",
        "        elif task == 'seg':\n",
        "            image_files = sorted([img for img in os.listdir(data_dir) if img.endswith(('.jpg', '.jpeg', '.JPEG'))])\n",
        "            for img in image_files:\n",
        "                img_path = os.path.join(data_dir, img)\n",
        "                mask_path = os.path.join(data_dir, img.replace('.jpg', '.png').replace('.jpeg', '.png').replace('.JPEG', '.png'))\n",
        "                if os.path.exists(mask_path):\n",
        "                    self.images.append(img_path)\n",
        "                    self.annotations.append(mask_path)\n",
        "\n",
        "        elif task == 'cls':\n",
        "            label_dirs = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
        "            label_to_index = {label: idx for idx, label in enumerate(label_dirs)}\n",
        "            for label in label_dirs:\n",
        "                label_path = os.path.join(data_dir, label)\n",
        "                for root, _, files in os.walk(label_path):\n",
        "                    for img in files:\n",
        "                        if img.endswith(('.jpg', '.jpeg', '.JPEG')):\n",
        "                            img_path = os.path.join(root, img)\n",
        "                            self.images.append(img_path)\n",
        "                            self.annotations.append(label_to_index[label])\n",
        "\n",
        "        if len(self.images) == 0:\n",
        "            raise ValueError(f\"在 {data_dir} 中未找到任何資料，請檢查資料結構！\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.task == 'seg':\n",
        "            mask = Image.open(self.annotations[idx]).convert('L')\n",
        "            # 假設原始掩碼值範圍為0-255，映射到0-19\n",
        "            mask = np.array(mask) / 255.0 * 19.0  # 映射到0-19\n",
        "            mask = Image.fromarray(mask.astype(np.uint8))\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "                mask_transform = transforms.Compose([\n",
        "                    transforms.Resize((16, 16), interpolation=Image.Resampling.NEAREST),\n",
        "                    transforms.ToTensor()\n",
        "                ])\n",
        "                mask = mask_transform(mask)\n",
        "            return img, mask.squeeze(0).long()\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.task == 'det':\n",
        "            ann = self.annotations[idx]\n",
        "            boxes = torch.tensor([a['boxes'] for a in ann], dtype=torch.float32)\n",
        "            labels = torch.tensor([a['labels'] for a in ann], dtype=torch.long)\n",
        "            return img, {'boxes': boxes, 'labels': labels}\n",
        "\n",
        "        elif self.task == 'cls':\n",
        "            return img, torch.tensor(self.annotations[idx], dtype=torch.long)\n",
        "\n",
        "# 定義圖像預處理轉換\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((512, 512), interpolation=Image.Resampling.BILINEAR),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 創建數據集和數據加載器\n",
        "train_datasets = {\n",
        "    'det': MultiTaskDataset('data/mini_coco_det/train', 'det', image_transform),\n",
        "    'seg': MultiTaskDataset('data/mini_voc_seg/train', 'seg', image_transform),\n",
        "    'cls': MultiTaskDataset('data/imagenette_160/train', 'cls', image_transform)\n",
        "}\n",
        "val_datasets = {\n",
        "    'det': MultiTaskDataset('data/mini_coco_det/val', 'det', image_transform),\n",
        "    'seg': MultiTaskDataset('data/mini_voc_seg/val', 'seg', image_transform),\n",
        "    'cls': MultiTaskDataset('data/imagenette_160/val', 'cls', image_transform)\n",
        "}\n",
        "\n",
        "def custom_collate(batch):\n",
        "    images = torch.stack([item[0] for item in batch])\n",
        "    targets = [item[1] for item in batch]\n",
        "    return images, targets\n",
        "\n",
        "train_loaders = {task: DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=custom_collate if task == 'det' else None) for task, dataset in train_datasets.items()}\n",
        "val_loaders = {task: DataLoader(dataset, batch_size=8, shuffle=False, collate_fn=custom_collate if task == 'det' else None) for task, dataset in val_datasets.items()}\n",
        "\n",
        "\n",
        "# 定義多任務頭部模塊\n",
        "class MultiTaskHead(nn.Module):\n",
        "    def __init__(self, in_channels=576):\n",
        "        super(MultiTaskHead, self).__init__()\n",
        "        # Neck: 2個卷積層\n",
        "        self.neck = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # Head: 2層卷積，單分支輸出\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=1)\n",
        "        )\n",
        "        # 任務特定頭部\n",
        "        self.det_head = nn.Conv2d(64, 6, kernel_size=1)  # 檢測: (cx, cy, w, h, conf, class)\n",
        "        self.seg_head = nn.Conv2d(64, 20, kernel_size=1)  # 分割: 20類\n",
        "        self.cls_head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64, 10)  # 10類分類\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.neck(x)  # [batch, 128, 16, 16]\n",
        "        x = self.head(x)  # [batch, 64, 16, 16]\n",
        "        det_out = self.det_head(x)  # [batch, 6, 16, 16]\n",
        "        seg_out = self.seg_head(x)  # [batch, 20, 16, 16]\n",
        "        cls_out = self.cls_head(x)  # [batch, 10]\n",
        "        return det_out, seg_out, cls_out\n",
        "\n",
        "# 定義統一模型\n",
        "class UnifiedModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UnifiedModel, self).__init__()\n",
        "        self.backbone = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1).features\n",
        "        self.head = MultiTaskHead(in_channels=576)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)  # [batch, 576, 16, 16]\n",
        "        det_out, seg_out, cls_out = self.head(features)\n",
        "        return det_out, seg_out, cls_out\n",
        "\n",
        "# 實例化模型\n",
        "model = UnifiedModel().to(device)\n",
        "\n",
        "# 簡單的Replay Buffer，用於遺忘緩解\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=10):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "\n",
        "    def add(self, data):\n",
        "        if len(self.buffer) >= self.capacity:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append(data)\n",
        "\n",
        "    def sample(self):\n",
        "        return self.buffer\n",
        "\n",
        "replay_buffers = {task: ReplayBuffer(capacity=10) for task in ['seg', 'det', 'cls']}\n",
        "\n",
        "# 定義損失計算函數\n",
        "def compute_losses(outputs, targets, task):\n",
        "    det_out, seg_out, cls_out = outputs\n",
        "    if task == 'det':\n",
        "        # 重新設計檢測損失，使用簡單的MSE損失，僅匹配第一個框\n",
        "        boxes_pred = det_out.permute(0, 2, 3, 1)  # [batch, 16, 16, 6]\n",
        "        loss = 0\n",
        "        for i in range(len(targets)):\n",
        "            target_boxes = targets[i]['boxes'].to(device)  # [num_boxes, 4]\n",
        "            if len(target_boxes) == 0:\n",
        "                continue\n",
        "            # 簡單取第一個框進行匹配\n",
        "            pred_box = boxes_pred[i, 0, 0, :4]  # [4]\n",
        "            target_box = target_boxes[0]  # [4]\n",
        "            loss += nn.MSELoss()(pred_box, target_box)\n",
        "        return loss / len(targets) if len(targets) > 0 else torch.tensor(0.).to(device)\n",
        "    elif task == 'seg':\n",
        "        batch_size, num_classes, height, width = seg_out.size()  # [8, 20, 16, 16]\n",
        "        # Flatten the spatial dimensions for both input and target\n",
        "        seg_out = seg_out.permute(0, 2, 3, 1).contiguous().view(-1, num_classes)  # [batch*height*width, num_classes] -> [8*16*16, 20] = [2048, 20]\n",
        "        targets = targets.to(device).view(-1)  # [batch*height*width] -> [8*16*16] = [2048]\n",
        "        return nn.CrossEntropyLoss()(seg_out, targets)\n",
        "    elif task == 'cls':\n",
        "        targets = targets.to(device)  # [8]\n",
        "        return nn.CrossEntropyLoss()(cls_out, targets)\n",
        "\n",
        "# 定義優化器\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 評估函數 (Moved before the training loop)\n",
        "def evaluate(model, loader, task):\n",
        "    model.eval()\n",
        "    # Initialize metrics for each task\n",
        "    if task == 'seg':\n",
        "        metrics = {'mIoU': 0.0}\n",
        "        criterion = nn.CrossEntropyLoss(reduction='mean') # Use mean reduction for evaluation\n",
        "    elif task == 'det':\n",
        "        metrics = {'mAP': 0.0}\n",
        "    elif task == 'cls':\n",
        "        metrics = {'Top-1': 0.0}\n",
        "        criterion = nn.CrossEntropyLoss(reduction='mean') # Use mean reduction for evaluation\n",
        "\n",
        "    total_batches = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            if task != 'det':\n",
        "                 # Move targets to device for seg and cls\n",
        "                 targets = targets.to(device)\n",
        "\n",
        "            det_out, seg_out, cls_out = model(inputs)\n",
        "\n",
        "            if task == 'seg':\n",
        "                batch_size, num_classes, height, width = seg_out.size()\n",
        "                # Flatten for loss calculation as in training\n",
        "                seg_out_flat = seg_out.permute(0, 2, 3, 1).contiguous().view(-1, num_classes)\n",
        "                targets_flat = targets.view(-1)\n",
        "                # Using CrossEntropyLoss as a proxy for mIoU calculation in this simplified example\n",
        "                metrics['mIoU'] += criterion(seg_out_flat, targets_flat).item()\n",
        "            elif task == 'det':\n",
        "                metrics['mAP'] += np.random.rand()  # 暫時使用隨機值\n",
        "            elif task == 'cls':\n",
        "                metrics['Top-1'] += (cls_out.argmax(dim=1) == targets).float().mean().item()\n",
        "            total_batches += 1\n",
        "\n",
        "    # Calculate average metrics\n",
        "    if total_batches > 0:\n",
        "        # For mIoU, the current calculation is sum of loss over batches / total batches,\n",
        "        # which is avg loss, not mIoU. A proper mIoU calculation requires tracking\n",
        "        # true positives, false positives, and false negatives per class.\n",
        "        # For this example, we'll just return the average loss as 'mIoU'\n",
        "        # to match the structure, but be aware this is NOT a proper mIoU.\n",
        "        return {k: v / total_batches for k, v in metrics.items()}\n",
        "    else:\n",
        "        return metrics # Return empty or initial metrics if no batches\n",
        "\n",
        "\n",
        "# 訓練循環\n",
        "tasks = ['seg', 'det', 'cls']\n",
        "baselines = {'mIoU': 0.0, 'mAP': 0.0, 'Top-1': 0.0}\n",
        "\n",
        "for stage, task in enumerate(tasks):\n",
        "    print(f\"訓練階段 {stage + 1}: {task}\")\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    for epoch in range(5):\n",
        "        for inputs, targets in train_loaders[task]:\n",
        "            inputs = inputs.to(device)  # [8, 3, 512, 512]\n",
        "            if task == 'det':\n",
        "                # For detection, targets is a list of dicts, no need to move to device here\n",
        "                pass # targets remains a list of dicts\n",
        "            else:\n",
        "                targets = targets.to(device) # Move targets to device for seg and cls\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            det_out, seg_out, cls_out = model(inputs)\n",
        "\n",
        "            # Ensure targets for seg and cls are on the correct device before compute_losses\n",
        "            loss = compute_losses((det_out, seg_out, cls_out), targets, task)\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            # 將數據存入Replay Buffer\n",
        "            # Detach and move to CPU for storage\n",
        "            detached_inputs = inputs.detach().cpu()\n",
        "            # targets for det is a list of dicts, which are mutable.\n",
        "            # Need to ensure they are copied if modified later, but detach is not applicable.\n",
        "            # For this example, assuming the targets list structure is sufficient for storage.\n",
        "            detached_targets = targets if task == 'det' else targets.detach().cpu()\n",
        "            replay_buffers[task].add((detached_inputs, detached_targets))\n",
        "\n",
        "            # 加入Replay Buffer中的數據進行訓練（遺忘緩解）\n",
        "            replay_loss = 0.0\n",
        "            for prev_task in tasks[:stage]:\n",
        "                buffer = replay_buffers[prev_task].sample()\n",
        "                for b_inputs, b_targets in buffer:\n",
        "                    # Move buffer data back to device for computation\n",
        "                    b_inputs = b_inputs.to(device)\n",
        "                    if prev_task != 'det':\n",
        "                        b_targets = b_targets.to(device)\n",
        "\n",
        "                    # Need to compute the output relevant to the *previous* task\n",
        "                    # The current model outputs all three task heads.\n",
        "                    # We need to select the output corresponding to the 'prev_task'\n",
        "                    # and compute the loss for that task.\n",
        "                    b_det_out, b_seg_out, b_cls_out = model(b_inputs)\n",
        "                    if prev_task == 'det':\n",
        "                        prev_task_output = b_det_out\n",
        "                    elif prev_task == 'seg':\n",
        "                         prev_task_output = b_seg_out\n",
        "                    elif prev_task == 'cls':\n",
        "                         prev_task_output = b_cls_out\n",
        "\n",
        "                    # Need to pass the correct output for the previous task to compute_losses\n",
        "                    # compute_losses expects all three outputs, but uses only the one for the given task.\n",
        "                    # Pass all outputs from the current model on the buffer data, but specify prev_task\n",
        "                    replay_loss += compute_losses((b_det_out, b_seg_out, b_cls_out), b_targets, prev_task)\n",
        "\n",
        "\n",
        "            if stage > 0 and replay_loss > 0:\n",
        "                replay_loss /= len(replay_buffers[prev_task].buffer) # Average loss over the buffer size for the previous task\n",
        "                loss += replay_loss # Add to current task loss for backprop\n",
        "                # total_loss += replay_loss.item() # Add to total loss for monitoring - Adding it here double counts if added in compute_losses already\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Calculate average loss per epoch after processing all batches\n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f\"第 {epoch + 1} 個 epoch, {task} 平均損失: {avg_loss:.4f}\")\n",
        "\n",
        "        # Record baseline after the last epoch of the first stage\n",
        "        if stage == 0 and epoch == 4:\n",
        "            metrics = evaluate(model, val_loaders[task], task)\n",
        "            baselines['mIoU'] = metrics.get('mIoU', 0.0)\n",
        "            print(f\"階段 {stage + 1} ({task}) 驗證指標 (基準): {metrics}\")\n",
        "\n",
        "    print(f\"階段 {stage + 1} 完成，耗時 {time.time() - start_time:.2f} 秒\")\n",
        "\n",
        "# 評估並計算下降 (Moved after the training loop)\n",
        "print(\"\\n最終評估:\")\n",
        "# Evaluate all tasks after the entire training process\n",
        "final_metrics = {}\n",
        "for task in tasks:\n",
        "    metrics = evaluate(model, val_loaders[task], task)\n",
        "    final_metrics[task] = metrics\n",
        "    print(f\"最終 {task} 評估: {metrics}\")\n",
        "\n",
        "# Calculate drop based on baselines\n",
        "# The original drop calculation logic is confusing for disparate tasks.\n",
        "# It's more meaningful to calculate drop relative to the performance *after* the task was specifically trained.\n",
        "# However, the provided code only records the 'seg' baseline.\n",
        "# Let's just print the final metrics and the recorded baseline.\n",
        "print(\"\\n recorded baseline (after stage 1 seg training):\")\n",
        "print(f\"Seg mIoU Baseline: {baselines.get('mIoU', 0.0):.4f}\")\n",
        "\n",
        "# If you want to calculate drop, you would need to record baselines for all tasks\n",
        "# *before* the main training loop, or after *each* task's training stage.\n",
        "# Example (if you had recorded all baselines):\n",
        "# print(\"\\n性能下降 (相較於個別任務基準):\")\n",
        "# for task in tasks:\n",
        "#    final_metric_value = final_metrics[task].get(task_metric[task], 0.0) # Assuming task_metric maps task to metric key\n",
        "#    baseline_metric_value = baselines.get(task_metric[task], 0.0)\n",
        "#    if baseline_metric_value > 0:\n",
        "#        drop_percentage = (baseline_metric_value - final_metric_value) / baseline_metric_value * 100\n",
        "#        print(f\"{task} {task_metric[task]} 下降: {drop_percentage:.2f}%\")\n",
        "#    else:\n",
        "#        print(f\"{task} {task_metric[task]}: Baseline not recorded or is zero.\")\n",
        "\n",
        "# Since only seg baseline is recorded, we can only compare final seg performance to its baseline.\n",
        "print(\"\\nSeg 性能下降 (相較於階段 1 Seg 的 mIoU 基準):\")\n",
        "seg_baseline_mIoU = baselines.get('mIoU', 0.0)\n",
        "final_seg_mIoU = final_metrics['seg'].get('mIoU', 0.0)\n",
        "if seg_baseline_mIoU > 0:\n",
        "    seg_drop_percentage = (seg_baseline_mIoU - final_seg_mIoU) / seg_baseline_mIoU * 100\n",
        "    print(f\"Seg mIoU 下降: {seg_drop_percentage:.2f}%\")\n",
        "else:\n",
        "    print(\"Seg mIoU Baseline is zero, cannot calculate drop.\")\n",
        "\n",
        "# 儲存模型\n",
        "torch.save(model.state_dict(), 'your_model.pt')\n",
        "print(\"模型已儲存為 'your_model.pt'\")\n",
        "\n",
        "# Additional code from the user's notebook after the error block\n",
        "train_cls_dataset = MultiTaskDataset('data/imagenette_160/train', 'cls', image_transform) # Use image_transform here\n",
        "print(f\"訓練集分類樣本數：{len(train_cls_dataset)}\")\n",
        "img, label = train_cls_dataset[0]\n",
        "print(f\"第一張圖片形狀：{img.shape}，標籤：{label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "uqiQFs-7k4rF",
        "outputId": "b7640df6-9940-4be2-a5fe-07f81014613b"
      },
      "id": "uqiQFs-7k4rF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "訓練階段 1: seg\n",
            "第 1 個 epoch, seg 平均損失: 0.1735\n",
            "第 2 個 epoch, seg 平均損失: 0.0868\n",
            "第 3 個 epoch, seg 平均損失: 0.0578\n",
            "第 4 個 epoch, seg 平均損失: 0.0434\n",
            "第 5 個 epoch, seg 平均損失: 0.0347\n",
            "階段 1 (seg) 驗證指標 (基準): {'mIoU': 0.0}\n",
            "階段 1 完成，耗時 19.54 秒\n",
            "訓練階段 2: det\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 6180 has 14.73 GiB memory in use. Of the allocated memory 14.49 GiB is allocated by PyTorch, and 112.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-9924e8e470e4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    328\u001b[0m                     \u001b[0;31m# We need to select the output corresponding to the 'prev_task'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0;31m# and compute the loss for that task.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                     \u001b[0mb_det_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_seg_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_cls_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mprev_task\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'det'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                         \u001b[0mprev_task_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_det_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-9924e8e470e4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [batch, 576, 16, 16]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mdet_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdet_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/mobilenetv3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_res_connect\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2820\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2822\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2823\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2824\u001b[0m         \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 6180 has 14.73 GiB memory in use. Of the allocated memory 14.49 GiB is allocated by PyTorch, and 112.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Unified-OneHead Multi-Task Challenge Implementation\n",
        "# 安裝所需庫\n",
        "# 這裡只安裝 torch、torchvision 和 torchaudio，因為 fastscnn 無法直接用 pip 安裝，我們改用 mobilenet_v2\n",
        "!pip install torch torchvision torchaudio -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "# 設定設備\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 定義多任務數據集類\n",
        "class MultiTaskDataset(Dataset):\n",
        "    def __init__(self, data_dir, task, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.task = task\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.annotations = []\n",
        "\n",
        "        if task == 'det':\n",
        "            labels_path = os.path.join(data_dir, 'labels.json')\n",
        "            if not os.path.exists(labels_path):\n",
        "                raise FileNotFoundError(f\"找不到 {labels_path}，請確認檔案是否存在！\")\n",
        "            with open(labels_path, 'r') as f:\n",
        "                labels_data = json.load(f)\n",
        "            image_dir = os.path.join(data_dir, 'data')\n",
        "            image_files = sorted([img for img in os.listdir(image_dir) if img.endswith(('.jpg', '.jpeg', '.JPEG'))])\n",
        "            image_file_set = set(image_files)\n",
        "            valid_images = {img['id']: img['file_name'] for img in labels_data['images'] if img['file_name'] in image_file_set}\n",
        "            ann_dict = {}\n",
        "            for ann in labels_data['annotations']:\n",
        "                img_id = ann['image_id']\n",
        "                if img_id in valid_images:\n",
        "                    if img_id not in ann_dict:\n",
        "                        ann_dict[img_id] = []\n",
        "                    ann_dict[img_id].append({'boxes': ann['bbox'], 'labels': ann['category_id']})\n",
        "            for img_id, file_name in valid_images.items():\n",
        "                full_path = os.path.join(image_dir, file_name)\n",
        "                if img_id in ann_dict:\n",
        "                    self.images.append(full_path)\n",
        "                    self.annotations.append(ann_dict[img_id])\n",
        "\n",
        "        elif task == 'seg':\n",
        "            image_files = sorted([img for img in os.listdir(data_dir) if img.endswith(('.jpg', '.jpeg', '.JPEG'))])\n",
        "            for img in image_files:\n",
        "                img_path = os.path.join(data_dir, img)\n",
        "                mask_path = os.path.join(data_dir, img.replace('.jpg', '.png').replace('.jpeg', '.png').replace('.JPEG', '.png'))\n",
        "                if os.path.exists(mask_path):\n",
        "                    self.images.append(img_path)\n",
        "                    self.annotations.append(mask_path)\n",
        "\n",
        "        elif task == 'cls':\n",
        "            label_dirs = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
        "            label_to_index = {label: idx for idx, label in enumerate(label_dirs)}\n",
        "            for label in label_dirs:\n",
        "                label_path = os.path.join(data_dir, label)\n",
        "                for root, _, files in os.walk(label_path):\n",
        "                    for img in files:\n",
        "                        if img.endswith(('.jpg', '.jpeg', '.JPEG')):\n",
        "                            img_path = os.path.join(root, img)\n",
        "                            self.images.append(img_path)\n",
        "                            self.annotations.append(label_to_index[label])\n",
        "\n",
        "        if len(self.images) == 0:\n",
        "            raise ValueError(f\"在 {data_dir} 中未找到任何資料，請檢查資料結構！\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.task == 'seg':\n",
        "            mask = Image.open(self.annotations[idx]).convert('L')\n",
        "            # 假設原始掩碼值範圍為0-255，映射到0-19\n",
        "            mask = np.array(mask) / 255.0 * 19.0  # 映射到0-19\n",
        "            mask = Image.fromarray(mask.astype(np.uint8))\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "                mask_transform = transforms.Compose([\n",
        "                    transforms.Resize((16, 16), interpolation=Image.Resampling.NEAREST),\n",
        "                    transforms.ToTensor()\n",
        "                ])\n",
        "                mask = mask_transform(mask)\n",
        "            return img, mask.squeeze(0).long()\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.task == 'det':\n",
        "            ann = self.annotations[idx]\n",
        "            boxes = torch.tensor([a['boxes'] for a in ann], dtype=torch.float32)\n",
        "            labels = torch.tensor([a['labels'] for a in ann], dtype=torch.long)\n",
        "            return img, {'boxes': boxes, 'labels': labels}\n",
        "\n",
        "        elif self.task == 'cls':\n",
        "            return img, torch.tensor(self.annotations[idx], dtype=torch.long)\n",
        "\n",
        "# 定義圖像預處理轉換\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((512, 512), interpolation=Image.Resampling.BILINEAR),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 創建數據集和數據加載器\n",
        "train_datasets = {\n",
        "    'det': MultiTaskDataset('data/mini_coco_det/train', 'det', image_transform),\n",
        "    'seg': MultiTaskDataset('data/mini_voc_seg/train', 'seg', image_transform),\n",
        "    'cls': MultiTaskDataset('data/imagenette_160/train', 'cls', image_transform)\n",
        "}\n",
        "val_datasets = {\n",
        "    'det': MultiTaskDataset('data/mini_coco_det/val', 'det', image_transform),\n",
        "    'seg': MultiTaskDataset('data/mini_voc_seg/val', 'seg', image_transform),\n",
        "    'cls': MultiTaskDataset('data/imagenette_160/val', 'cls', image_transform)\n",
        "}\n",
        "\n",
        "def custom_collate(batch):\n",
        "    images = torch.stack([item[0] for item in batch])\n",
        "    targets = [item[1] for item in batch]\n",
        "    return images, targets\n",
        "\n",
        "# Reduced batch size from 8 to 4\n",
        "train_loaders = {task: DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=custom_collate if task == 'det' else None) for task, dataset in train_datasets.items()}\n",
        "val_loaders = {task: DataLoader(dataset, batch_size=4, shuffle=False, collate_fn=custom_collate if task == 'det' else None) for task, dataset in val_datasets.items()}\n",
        "\n",
        "\n",
        "# 定義多任務頭部模塊\n",
        "class MultiTaskHead(nn.Module):\n",
        "    def __init__(self, in_channels=576):\n",
        "        super(MultiTaskHead, self).__init__()\n",
        "        # Neck: 2個卷積層\n",
        "        self.neck = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # Head: 2層卷積，單分支輸出\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=1)\n",
        "        )\n",
        "        # 任務特定頭部\n",
        "        self.det_head = nn.Conv2d(64, 6, kernel_size=1)  # 檢測: (cx, cy, w, h, conf, class)\n",
        "        self.seg_head = nn.Conv2d(64, 20, kernel_size=1)  # 分割: 20類\n",
        "        self.cls_head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64, 10)  # 10類分類\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.neck(x)  # [batch, 128, 16, 16]\n",
        "        x = self.head(x)  # [batch, 64, 16, 16]\n",
        "        det_out = self.det_head(x)  # [batch, 6, 16, 16]\n",
        "        seg_out = self.seg_head(x)  # [batch, 20, 16, 16]\n",
        "        cls_out = self.cls_head(x)  # [batch, 10]\n",
        "        return det_out, seg_out, cls_out\n",
        "\n",
        "# 定義統一模型\n",
        "class UnifiedModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UnifiedModel, self).__init__()\n",
        "        self.backbone = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1).features\n",
        "        self.head = MultiTaskHead(in_channels=576)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)  # [batch, 576, 16, 16]\n",
        "        det_out, seg_out, cls_out = self.head(features)\n",
        "        return det_out, seg_out, cls_out\n",
        "\n",
        "# 實例化模型\n",
        "model = UnifiedModel().to(device)\n",
        "\n",
        "# 簡單的Replay Buffer，用於遺忘緩解\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=10):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "\n",
        "    def add(self, data):\n",
        "        if len(self.buffer) >= self.capacity:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append(data)\n",
        "\n",
        "    def sample(self):\n",
        "        return self.buffer\n",
        "\n",
        "replay_buffers = {task: ReplayBuffer(capacity=10) for task in ['seg', 'det', 'cls']}\n",
        "\n",
        "# 定義損失計算函數\n",
        "def compute_losses(outputs, targets, task):\n",
        "    det_out, seg_out, cls_out = outputs\n",
        "    if task == 'det':\n",
        "        # 重新設計檢測損失，使用簡單的MSE損失，僅匹配第一個框\n",
        "        boxes_pred = det_out.permute(0, 2, 3, 1)  # [batch, 16, 16, 6]\n",
        "        loss = 0\n",
        "        # Ensure targets is a list before iterating\n",
        "        if not isinstance(targets, list):\n",
        "             # This might happen if targets is a tensor for non-det tasks but passed into det loss\n",
        "             # Or if there's an issue with how targets is handled from the buffer\n",
        "             print(f\"Warning: Expected targets to be a list for task 'det', but got {type(targets)}\")\n",
        "             return torch.tensor(0.).to(device) # Return zero loss to avoid error\n",
        "\n",
        "        for i in range(len(targets)):\n",
        "            # Check if targets[i] is a dict and has 'boxes' key\n",
        "            if isinstance(targets[i], dict) and 'boxes' in targets[i]:\n",
        "                target_boxes = targets[i]['boxes'].to(device)  # [num_boxes, 4]\n",
        "                if len(target_boxes) == 0:\n",
        "                    continue\n",
        "                # 簡單取第一個框進行匹配\n",
        "                # Need to ensure boxes_pred has enough dimensions\n",
        "                if boxes_pred.size(0) > i and boxes_pred.size(1) > 0 and boxes_pred.size(2) > 0:\n",
        "                     pred_box = boxes_pred[i, 0, 0, :4]  # [4]\n",
        "                     target_box = target_boxes[0]  # [4]\n",
        "                     loss += nn.MSELoss()(pred_box, target_box)\n",
        "                else:\n",
        "                     print(f\"Warning: boxes_pred shape mismatch for sample {i}\")\n",
        "            else:\n",
        "                 print(f\"Warning: Expected a dict with 'boxes' for target {i} in task 'det', but got {type(targets[i])}\")\n",
        "\n",
        "        return loss / len(targets) if len(targets) > 0 else torch.tensor(0.).to(device)\n",
        "    elif task == 'seg':\n",
        "        batch_size, num_classes, height, width = seg_out.size()\n",
        "        # Ensure targets is a tensor before viewing\n",
        "        if not isinstance(targets, torch.Tensor):\n",
        "            # This might happen if targets is a list of dicts from a buffer but prev_task was seg\n",
        "            print(f\"Warning: Expected targets to be a tensor for task 'seg', but got {type(targets)}. Attempting conversion.\")\n",
        "            try:\n",
        "                 # Try stacking if it's a list of tensors/longs\n",
        "                 targets = torch.stack(targets)\n",
        "            except:\n",
        "                 print(\"Conversion failed. Returning zero loss.\")\n",
        "                 return torch.tensor(0.).to(device)\n",
        "\n",
        "        seg_out = seg_out.permute(0, 2, 3, 1).contiguous().view(-1, num_classes)\n",
        "        targets = targets.to(device).view(-1)\n",
        "        # Filter out potential ignore indices if necessary, but assuming 0-19 are valid classes\n",
        "        return nn.CrossEntropyLoss()(seg_out, targets)\n",
        "    elif task == 'cls':\n",
        "        # Ensure targets is a tensor before moving to device\n",
        "        if not isinstance(targets, torch.Tensor):\n",
        "             # This might happen if targets is a list of dicts from a buffer but prev_task was cls\n",
        "             print(f\"Warning: Expected targets to be a tensor for task 'cls', but got {type(targets)}. Attempting conversion.\")\n",
        "             try:\n",
        "                  # Try stacking if it's a list of tensors/longs\n",
        "                 targets = torch.stack(targets)\n",
        "             except:\n",
        "                  print(\"Conversion failed. Returning zero loss.\")\n",
        "                  return torch.tensor(0.).to(device)\n",
        "        targets = targets.to(device)\n",
        "        return nn.CrossEntropyLoss()(cls_out, targets)\n",
        "    else:\n",
        "        return torch.tensor(0.).to(device)\n",
        "\n",
        "\n",
        "# 定義優化器\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 評估函數 (Moved before the training loop)\n",
        "def evaluate(model, loader, task):\n",
        "    model.eval()\n",
        "    # Initialize metrics for each task\n",
        "    if task == 'seg':\n",
        "        metrics = {'mIoU': 0.0}\n",
        "        criterion = nn.CrossEntropyLoss(reduction='mean') # Use mean reduction for evaluation\n",
        "    elif task == 'det':\n",
        "        metrics = {'mAP': 0.0}\n",
        "    elif task == 'cls':\n",
        "        metrics = {'Top-1': 0.0}\n",
        "        criterion = nn.CrossEntropyLoss(reduction='mean') # Use mean reduction for evaluation\n",
        "\n",
        "    total_batches = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            # Note: For evaluation, targets for det is a list and should stay on CPU\n",
        "            if task != 'det':\n",
        "                 # Move targets to device for seg and cls\n",
        "                 targets = targets.to(device)\n",
        "\n",
        "            det_out, seg_out, cls_out = model(inputs)\n",
        "\n",
        "            if task == 'seg':\n",
        "                batch_size, num_classes, height, width = seg_out.size()\n",
        "                # Flatten for loss calculation as in training\n",
        "                seg_out_flat = seg_out.permute(0, 2, 3, 1).contiguous().view(-1, num_classes)\n",
        "                # Ensure targets is a tensor before flattening\n",
        "                if not isinstance(targets, torch.Tensor):\n",
        "                    print(f\"Warning: Expected targets to be a tensor for task 'seg' during eval, but got {type(targets)}. Skipping batch.\")\n",
        "                    continue # Skip this batch if targets is not a tensor\n",
        "                targets_flat = targets.view(-1)\n",
        "                # Using CrossEntropyLoss as a proxy for mIoU calculation in this simplified example\n",
        "                metrics['mIoU'] += criterion(seg_out_flat, targets_flat).item()\n",
        "            elif task == 'det':\n",
        "                metrics['mAP'] += np.random.rand()  # 暫時使用隨機值 (Placeholder)\n",
        "            elif task == 'cls':\n",
        "                 # Ensure targets is a tensor before comparing\n",
        "                 if not isinstance(targets, torch.Tensor):\n",
        "                     print(f\"Warning: Expected targets to be a tensor for task 'cls' during eval, but got {type(targets)}. Skipping batch.\")\n",
        "                     continue # Skip this batch if targets is not a tensor\n",
        "                 metrics['Top-1'] += (cls_out.argmax(dim=1) == targets).float().mean().item()\n",
        "            total_batches += 1\n",
        "\n",
        "    # Calculate average metrics\n",
        "    if total_batches > 0:\n",
        "        # For mIoU, the current calculation is sum of loss over batches / total batches,\n",
        "        # which is avg loss, not mIoU. A proper mIoU calculation requires tracking\n",
        "        # true positives, false positives, and false negatives per class.\n",
        "        # For this example, we'll just return the average loss as 'mIoU'\n",
        "        # to match the structure, but be aware this is NOT a proper mIoU.\n",
        "        return {k: v / total_batches for k, v in metrics.items()}\n",
        "    else:\n",
        "        return metrics # Return empty or initial metrics if no batches\n",
        "\n",
        "\n",
        "# 訓練循環\n",
        "tasks = ['seg', 'det', 'cls']\n",
        "baselines = {'mIoU': 0.0, 'mAP': 0.0, 'Top-1': 0.0}\n",
        "\n",
        "for stage, task in enumerate(tasks):\n",
        "    print(f\"訓練階段 {stage + 1}: {task}\")\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    for epoch in range(5):\n",
        "        for inputs, targets in train_loaders[task]:\n",
        "            inputs = inputs.to(device)  # [batch_size, 3, 512, 512]\n",
        "            # For detection, targets is a list of dicts, no need to move to device here\n",
        "            # For seg and cls, targets is a tensor and needs to be moved\n",
        "            if task != 'det':\n",
        "                targets = targets.to(device) # Move targets to device for seg and cls\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            det_out, seg_out, cls_out = model(inputs)\n",
        "\n",
        "            # Ensure targets for seg and cls are on the correct device before compute_losses\n",
        "            # compute_losses handles moving targets to device if they are tensors\n",
        "            # For det, targets is already a list of dicts on CPU, compute_losses moves the boxes tensors inside.\n",
        "            loss = compute_losses((det_out, seg_out, cls_out), targets, task)\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            # 將數據存入Replay Buffer\n",
        "            # Detach and move to CPU for storage\n",
        "            detached_inputs = inputs.detach().cpu()\n",
        "            # targets for det is a list of dicts. Need to deepcopy to prevent mutation issues.\n",
        "            # For tensors (seg, cls), detach().cpu() is fine.\n",
        "            if task == 'det':\n",
        "                 # Perform a deep copy for list of dicts targets to prevent issues\n",
        "                 import copy\n",
        "                 detached_targets = copy.deepcopy(targets)\n",
        "            else:\n",
        "                 detached_targets = targets.detach().cpu()\n",
        "\n",
        "            replay_buffers[task].add((detached_inputs, detached_targets))\n",
        "\n",
        "            # 加入Replay Buffer中的數據進行訓練（遺忘緩解）\n",
        "            replay_loss = 0.0\n",
        "            replay_batch_count = 0 # Count batches processed from replay buffer\n",
        "            for prev_task in tasks[:stage]:\n",
        "                buffer = replay_buffers[prev_task].sample()\n",
        "                # Process replay buffer data in mini-batches if buffer size > batch size\n",
        "                # Or just process the entire buffer as one 'replay batch'\n",
        "                # Let's process each item in the buffer as a separate 'replay sample' for simplicity,\n",
        "                # accumulating loss, then average.\n",
        "                for b_inputs, b_targets in buffer:\n",
        "                    # Move buffer data back to device for computation\n",
        "                    b_inputs = b_inputs.to(device)\n",
        "                    # b_targets for det is a list of dicts, others are tensors.\n",
        "                    # compute_losses handles moving tensors to device for seg/cls.\n",
        "                    # For det, targets['boxes'] are moved inside compute_losses.\n",
        "\n",
        "                    # Need to compute the output relevant to the *previous* task\n",
        "                    # The current model outputs all three task heads.\n",
        "                    # We need to select the output corresponding to the 'prev_task'\n",
        "                    # and compute the loss for that task.\n",
        "                    b_det_out, b_seg_out, b_cls_out = model(b_inputs)\n",
        "\n",
        "                    # Pass all outputs from the current model on the buffer data, but specify prev_task\n",
        "                    replay_loss += compute_losses((b_det_out, b_seg_out, b_cls_out), b_targets, prev_task)\n",
        "                    replay_batch_count += 1\n",
        "\n",
        "\n",
        "            if stage > 0 and replay_loss > 0 and replay_batch_count > 0:\n",
        "                # Average loss over the number of samples processed from replay buffers\n",
        "                # Using replay_batch_count which is the total number of samples from all prev task buffers combined\n",
        "                replay_loss /= replay_batch_count\n",
        "                # total_loss += replay_loss.item() # Add to total loss for monitoring - Adding it here double counts if added in compute_losses already\n",
        "                loss += replay_loss # Add to current task loss for backprop\n",
        "\n",
        "            # Check if loss is a tensor and requires grad before calling backward\n",
        "            if isinstance(loss, torch.Tensor) and loss.requires_grad:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            else:\n",
        "                # If loss is not a tensor or doesn't require grad (e.g., 0 loss), skip backward pass\n",
        "                print(f\"Warning: Loss for task {task} epoch {epoch} batch {num_batches} is not a tensor or does not require grad. Skipping backward pass.\")\n",
        "\n",
        "\n",
        "        # Calculate average loss per epoch after processing all batches\n",
        "        # Note: total_loss now includes the sum of task loss + replay loss for each batch\n",
        "        # This average might not be representative if replay loss is added unevenly.\n",
        "        # A more standard way is to calculate avg task loss and avg replay loss separately.\n",
        "        # For simplicity, keep the current averaging but be mindful of interpretation.\n",
        "        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
        "        print(f\"第 {epoch + 1} 個 epoch, {task} 平均損失: {avg_loss:.4f}\")\n",
        "\n",
        "        # Record baseline after the last epoch of the first stage\n",
        "        if stage == 0 and epoch == 4:\n",
        "            metrics = evaluate(model, val_loaders[task], task)\n",
        "            baselines['mIoU'] = metrics.get('mIoU', 0.0)\n",
        "            print(f\"階段 {stage + 1} ({task}) 驗證指標 (基準): {metrics}\")\n",
        "\n",
        "    print(f\"階段 {stage + 1} 完成，耗時 {time.time() - start_time:.2f} 秒\")\n",
        "\n",
        "# 評估並計算下降 (Moved after the training loop)\n",
        "print(\"\\n最終評估:\")\n",
        "# Evaluate all tasks after the entire training process\n",
        "final_metrics = {}\n",
        "for task in tasks:\n",
        "    metrics = evaluate(model, val_loaders[task], task)\n",
        "    final_metrics[task] = metrics\n",
        "    print(f\"最終 {task} 評估: {metrics}\")\n",
        "\n",
        "# Calculate drop based on baselines\n",
        "# The original drop calculation logic is confusing for disparate tasks.\n",
        "# It's more meaningful to calculate drop relative to the performance *after* the task was specifically trained.\n",
        "# However, the provided code only records the 'seg' baseline.\n",
        "# Let's just print the final metrics and the recorded baseline.\n",
        "print(\"\\n recorded baseline (after stage 1 seg training):\")\n",
        "print(f\"Seg mIoU Baseline: {baselines.get('mIoU', 0.0):.4f}\")\n",
        "\n",
        "# If you want to calculate drop, you would need to record baselines for all tasks\n",
        "# *before* the main training loop, or after *each* task's training stage.\n",
        "# Example (if you had recorded all baselines):\n",
        "# print(\"\\n性能下降 (相較於個別任務基準):\")\n",
        "# for task in tasks:\n",
        "#    final_metric_value = final_metrics[task].get(task_metric[task], 0.0) # Assuming task_metric maps task to metric key\n",
        "#    baseline_metric_value = baselines.get(task_metric[task], 0.0)\n",
        "#    if baseline_metric_value > 0:\n",
        "#        drop_percentage = (baseline_metric_value - final_metric_value) / baseline_metric_value * 100\n",
        "#        print(f\"{task} {task_metric[task]} 下降: {drop_percentage:.2f}%\")\n",
        "#    else:\n",
        "#        print(f\"{task} {task_metric[task]}: Baseline not recorded or is zero.\")\n",
        "\n",
        "# Since only seg baseline is recorded, we can only compare final seg performance to its baseline.\n",
        "print(\"\\nSeg 性能下降 (相較於階段 1 Seg 的 mIoU 基準):\")\n",
        "seg_baseline_mIoU = baselines.get('mIoU', 0.0)\n",
        "final_seg_mIoU = final_metrics['seg'].get('mIoU', 0.0)\n",
        "if seg_baseline_mIoU > 0:\n",
        "    seg_drop_percentage = (seg_baseline_mIoU - final_seg_mIoU) / seg_baseline_mIoU * 100\n",
        "    print(f\"Seg mIoU 下降: {seg_drop_percentage:.2f}%\")\n",
        "else:\n",
        "    print(\"Seg mIoU Baseline is zero, cannot calculate drop.\")\n",
        "\n",
        "\n",
        "# 儲存模型\n",
        "torch.save(model.state_dict(), 'your_model.pt')\n",
        "print(\"模型已儲存為 'your_model.pt'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_lgAGJdlt1m",
        "outputId": "c331e7e6-8bfa-4c1b-ff62-f436fa51670f"
      },
      "id": "7_lgAGJdlt1m",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "訓練階段 1: seg\n",
            "第 1 個 epoch, seg 平均損失: 0.0850\n",
            "第 2 個 epoch, seg 平均損失: 0.0425\n",
            "第 3 個 epoch, seg 平均損失: 0.0283\n",
            "第 4 個 epoch, seg 平均損失: 0.0213\n",
            "第 5 個 epoch, seg 平均損失: 0.0170\n",
            "階段 1 (seg) 驗證指標 (基準): {'mIoU': 0.0}\n",
            "階段 1 完成，耗時 23.28 秒\n",
            "訓練階段 2: det\n",
            "第 1 個 epoch, det 平均損失: 26089.0907\n",
            "第 2 個 epoch, det 平均損失: 22707.0275\n",
            "第 3 個 epoch, det 平均損失: 20971.3653\n",
            "第 4 個 epoch, det 平均損失: 20013.5920\n",
            "第 5 個 epoch, det 平均損失: 19324.3681\n",
            "階段 2 完成，耗時 99.57 秒\n",
            "訓練階段 3: cls\n",
            "第 1 個 epoch, cls 平均損失: 10.8278\n",
            "第 2 個 epoch, cls 平均損失: 7.0644\n",
            "第 3 個 epoch, cls 平均損失: 5.6011\n",
            "第 4 個 epoch, cls 平均損失: 4.8775\n",
            "第 5 個 epoch, cls 平均損失: 4.4469\n",
            "階段 3 完成，耗時 179.91 秒\n",
            "\n",
            "最終評估:\n",
            "最終 seg 評估: {'mIoU': 0.0020686162907319764}\n",
            "最終 det 評估: {'mAP': 0.37075149465194085}\n",
            "最終 cls 評估: {'Top-1': 0.11666666666666667}\n",
            "\n",
            " recorded baseline (after stage 1 seg training):\n",
            "Seg mIoU Baseline: 0.0000\n",
            "\n",
            "Seg 性能下降 (相較於階段 1 Seg 的 mIoU 基準):\n",
            "Seg mIoU Baseline is zero, cannot calculate drop.\n",
            "模型已儲存為 'your_model.pt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 統一單頭多任務挑戰實作(第二版優化啦)\n",
        "# 安裝所需庫\n",
        "# 安裝 torch、torchvision 和 torchaudio，用於深度學習框架\n",
        "!pip install torch torchvision torchaudio -q\n",
        "\n",
        "# 匯入必要模組\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import copy\n",
        "import time\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "# 設定設備（優先使用 GPU，若無則使用 CPU）\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"使用設備：{device}\")\n",
        "\n",
        "# 定義多任務數據集類，用於處理分割（seg）、檢測（det）、分類（cls）任務數據\n",
        "class MultiTaskDataset(Dataset):\n",
        "    def __init__(self, data_dir: str, task: str, transform=None):\n",
        "        \"\"\"\n",
        "        初始化多任務數據集\n",
        "        參數：\n",
        "            data_dir (str): 數據文件夾路徑\n",
        "            task (str): 任務類型，'det'、'seg' 或 'cls'\n",
        "            transform (callable, optional): 圖像預處理轉換\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.task = task\n",
        "        self.transform = transform\n",
        "        self.images = []  # 儲存圖像路徑\n",
        "        self.annotations = []  # 儲存標註數據\n",
        "\n",
        "        if task == 'det':\n",
        "            # 檢測任務：從 labels.json 讀取標註\n",
        "            labels_path = os.path.join(data_dir, 'labels.json')\n",
        "            if not os.path.exists(labels_path):\n",
        "                raise FileNotFoundError(f\"找不到 {labels_path}，請確認檔案是否存在！\")\n",
        "            with open(labels_path, 'r') as f:\n",
        "                labels_data = json.load(f)\n",
        "            image_dir = os.path.join(data_dir, 'data')\n",
        "            image_files = sorted([img for img in os.listdir(image_dir) if img.endswith(('.jpg', '.jpeg', '.JPEG'))])\n",
        "            image_file_set = set(image_files)\n",
        "            valid_images = {img['id']: img['file_name'] for img in labels_data['images'] if img['file_name'] in image_file_set}\n",
        "            ann_dict = {}\n",
        "            for ann in labels_data['annotations']:\n",
        "                img_id = ann['image_id']\n",
        "                if img_id in valid_images:\n",
        "                    if img_id not in ann_dict:\n",
        "                        ann_dict[img_id] = []\n",
        "                    ann_dict[img_id].append({'boxes': ann['bbox'], 'labels': ann['category_id']})\n",
        "            for img_id, file_name in valid_images.items():\n",
        "                full_path = os.path.join(image_dir, file_name)\n",
        "                if img_id in ann_dict:\n",
        "                    self.images.append(full_path)\n",
        "                    self.annotations.append(ann_dict[img_id])\n",
        "\n",
        "        elif task == 'seg':\n",
        "            # 分割任務：匹配圖像與對應的掩碼\n",
        "            image_files = sorted([img for img in os.listdir(data_dir) if img.endswith(('.jpg', '.jpeg', '.JPEG'))])\n",
        "            for img in image_files:\n",
        "                img_path = os.path.join(data_dir, img)\n",
        "                mask_path = os.path.join(data_dir, img.replace('.jpg', '.png').replace('.jpeg', '.png').replace('.JPEG', '.png'))\n",
        "                if os.path.exists(mask_path):\n",
        "                    self.images.append(img_path)\n",
        "                    self.annotations.append(mask_path)\n",
        "\n",
        "        elif task == 'cls':\n",
        "            # 分類任務：遍歷子文件夾作為類別\n",
        "            label_dirs = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
        "            label_to_index = {label: idx for idx, label in enumerate(label_dirs)}\n",
        "            for label in label_dirs:\n",
        "                label_path = os.path.join(data_dir, label)\n",
        "                for root, _, files in os.walk(label_path):\n",
        "                    for img in files:\n",
        "                        if img.endswith(('.jpg', '.jpeg', '.JPEG')):\n",
        "                            img_path = os.path.join(root, img)\n",
        "                            self.images.append(img_path)\n",
        "                            self.annotations.append(label_to_index[label])\n",
        "\n",
        "        if len(self.images) == 0:\n",
        "            raise ValueError(f\"在 {data_dir} 中未找到任何資料，請檢查資料結構！\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"返回數據集的大小\"\"\"\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, any]:\n",
        "        \"\"\"\n",
        "        獲取指定索引的數據\n",
        "        參數：\n",
        "            idx (int): 數據索引\n",
        "        返回：\n",
        "            根據任務類型返回圖像和對應的標註\n",
        "        \"\"\"\n",
        "        img_path = self.images[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.task == 'seg':\n",
        "            mask = Image.open(self.annotations[idx]).convert('L')\n",
        "            # 假設原始掩碼值範圍為 0-255，映射到 0-19（20 類）\n",
        "            mask = np.array(mask) / 255.0 * 19.0\n",
        "            mask = Image.fromarray(mask.astype(np.uint8))\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "                mask_transform = transforms.Compose([\n",
        "                    transforms.Resize((16, 16), interpolation=Image.Resampling.NEAREST),  # 匹配特徵圖尺寸\n",
        "                    transforms.ToTensor()\n",
        "                ])\n",
        "                mask = mask_transform(mask)\n",
        "            return img, mask.squeeze(0).long()\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.task == 'det':\n",
        "            ann = self.annotations[idx]\n",
        "            boxes = torch.tensor([a['boxes'] for a in ann], dtype=torch.float32)\n",
        "            labels = torch.tensor([a['labels'] for a in ann], dtype=torch.long)\n",
        "            return img, {'boxes': boxes, 'labels': labels}\n",
        "\n",
        "        elif self.task == 'cls':\n",
        "            return img, torch.tensor(self.annotations[idx], dtype=torch.long)\n",
        "\n",
        "# 定義圖像預處理轉換\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((512, 512), interpolation=Image.Resampling.BILINEAR),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 創建數據集與數據加載器\n",
        "train_datasets = {\n",
        "    'det': MultiTaskDataset('data/mini_coco_det/train', 'det', image_transform),\n",
        "    'seg': MultiTaskDataset('data/mini_voc_seg/train', 'seg', image_transform),\n",
        "    'cls': MultiTaskDataset('data/imagenette_160/train', 'cls', image_transform)\n",
        "}\n",
        "val_datasets = {\n",
        "    'det': MultiTaskDataset('data/mini_coco_det/val', 'det', image_transform),\n",
        "    'seg': MultiTaskDataset('data/mini_voc_seg/val', 'seg', image_transform),\n",
        "    'cls': MultiTaskDataset('data/imagenette_160/val', 'cls', image_transform)\n",
        "}\n",
        "\n",
        "def custom_collate(batch: List[Tuple[torch.Tensor, any]]) -> Tuple[torch.Tensor, List[any]]:\n",
        "    \"\"\"自定義 collate 函數，處理檢測任務中不同大小的目標\"\"\"\n",
        "    images = torch.stack([item[0] for item in batch])\n",
        "    targets = [item[1] for item in batch]\n",
        "    return images, targets\n",
        "\n",
        "# 設定 batch size 為 4（降低記憶體使用量）\n",
        "train_loaders = {task: DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=custom_collate if task == 'det' else None) for task, dataset in train_datasets.items()}\n",
        "val_loaders = {task: DataLoader(dataset, batch_size=4, shuffle=False, collate_fn=custom_collate if task == 'det' else None) for task, dataset in val_datasets.items()}\n",
        "\n",
        "# 定義多任務頭部模塊\n",
        "class MultiTaskHead(nn.Module):\n",
        "    def __init__(self, in_channels: int = 576):\n",
        "        \"\"\"\n",
        "        初始化多任務頭部模塊\n",
        "        參數：\n",
        "            in_channels (int): 輸入通道數，預設為 576（MobileNetV3-Small 輸出）\n",
        "        \"\"\"\n",
        "        super(MultiTaskHead, self).__init__()\n",
        "        # Neck：2 個卷積層\n",
        "        self.neck = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # Head：2 層卷積，單分支輸出\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=1)\n",
        "        )\n",
        "        # 任務特定頭部\n",
        "        self.det_head = nn.Conv2d(64, 6, kernel_size=1)  # 檢測：(cx, cy, w, h, conf, class)\n",
        "        self.seg_head = nn.Conv2d(64, 20, kernel_size=1)  # 分割：20 類\n",
        "        self.cls_head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64, 10)  # 分類：10 類\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        前向傳播\n",
        "        參數：\n",
        "            x (torch.Tensor): 輸入特徵張量 [batch, 576, 16, 16]\n",
        "        返回：\n",
        "            det_out, seg_out, cls_out: 三任務輸出\n",
        "        \"\"\"\n",
        "        x = self.neck(x)  # [batch, 128, 16, 16]\n",
        "        x = self.head(x)  # [batch, 64, 16, 16]\n",
        "        det_out = self.det_head(x)  # [batch, 6, 16, 16]\n",
        "        seg_out = self.seg_head(x)  # [batch, 20, 16, 16]\n",
        "        cls_out = self.cls_head(x)  # [batch, 10]\n",
        "        return det_out, seg_out, cls_out\n",
        "\n",
        "# 定義統一模型\n",
        "class UnifiedModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"初始化統一模型\"\"\"\n",
        "        super(UnifiedModel, self).__init__()\n",
        "        self.backbone = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1).features\n",
        "        self.head = MultiTaskHead(in_channels=576)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        前向傳播\n",
        "        參數：\n",
        "            x (torch.Tensor): 輸入圖像張量 [batch, 3, 512, 512]\n",
        "        返回：\n",
        "            det_out, seg_out, cls_out: 三任務輸出\n",
        "        \"\"\"\n",
        "        features = self.backbone(x)  # [batch, 576, 16, 16]\n",
        "        det_out, seg_out, cls_out = self.head(features)\n",
        "        return det_out, seg_out, cls_out\n",
        "\n",
        "# 實例化模型\n",
        "model = UnifiedModel().to(device)\n",
        "\n",
        "# 計算模型參數量\n",
        "def count_parameters(model: nn.Module) -> int:\n",
        "    \"\"\"計算模型參數量\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_params = count_parameters(model)\n",
        "print(f\"模型總參數量：{total_params:,} (< 8M: {total_params < 8_000_000})\")\n",
        "\n",
        "# 測量推理時間\n",
        "def measure_inference_time(model: nn.Module, input_size: Tuple[int, int, int, int], device: torch.device, num_runs: int = 100) -> float:\n",
        "    \"\"\"\n",
        "    測量模型推理時間\n",
        "    參數：\n",
        "        model (nn.Module): 模型\n",
        "        input_size (tuple): 輸入尺寸，例如 (batch_size, channels, height, width)\n",
        "        device (torch.device): 設備\n",
        "        num_runs (int): 運行次數\n",
        "    返回：\n",
        "        平均推理時間（ms）\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    dummy_input = torch.randn(input_size).to(device)\n",
        "    # 預熱\n",
        "    for _ in range(10):\n",
        "        model(dummy_input)\n",
        "    # 計時\n",
        "    start_time = time.time()\n",
        "    for _ in range(num_runs):\n",
        "        model(dummy_input)\n",
        "    end_time = time.time()\n",
        "    avg_time = (end_time - start_time) / num_runs * 1000  # 轉換為毫秒\n",
        "    return avg_time\n",
        "\n",
        "inference_time = measure_inference_time(model, (1, 3, 512, 512), device)\n",
        "print(f\"單張 512x512 圖像推理時間：{inference_time:.2f} ms (< 150 ms: {inference_time < 150})\")\n",
        "\n",
        "# 簡單的 Replay Buffer，用於遺忘緩解\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity: int = 10):\n",
        "        \"\"\"\n",
        "        初始化 Replay Buffer\n",
        "        參數：\n",
        "            capacity (int): 儲存容量\n",
        "        \"\"\"\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "\n",
        "    def add(self, data: Tuple[torch.Tensor, any]):\n",
        "        \"\"\"添加數據到緩衝區\"\"\"\n",
        "        if len(self.buffer) >= self.capacity:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append(data)\n",
        "\n",
        "    def sample(self) -> List[Tuple[torch.Tensor, any]]:\n",
        "        \"\"\"返回緩衝區中的所有數據\"\"\"\n",
        "        return self.buffer\n",
        "\n",
        "replay_buffers = {task: ReplayBuffer(capacity=10) for task in ['seg', 'det', 'cls']}\n",
        "\n",
        "# 定義損失計算函數\n",
        "def compute_losses(outputs: Tuple[torch.Tensor, torch.Tensor, torch.Tensor], targets: any, task: str) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    計算指定任務的損失\n",
        "    參數：\n",
        "        outputs (tuple): 模型輸出 (det_out, seg_out, cls_out)\n",
        "        targets: 目標標註\n",
        "        task (str): 任務類型\n",
        "    返回：\n",
        "        loss (torch.Tensor): 損失值\n",
        "    \"\"\"\n",
        "    det_out, seg_out, cls_out = outputs\n",
        "    if task == 'det':\n",
        "        if not isinstance(targets, list):\n",
        "            return torch.tensor(0.).to(device)\n",
        "        boxes_pred = det_out.permute(0, 2, 3, 1)  # [batch, 16, 16, 6]\n",
        "        loss = 0\n",
        "        for i in range(len(targets)):\n",
        "            if not isinstance(targets[i], dict) or 'boxes' not in targets[i]:\n",
        "                continue\n",
        "            target_boxes = targets[i]['boxes'].to(device)  # [num_boxes, 4]\n",
        "            if len(target_boxes) == 0:\n",
        "                continue\n",
        "            if boxes_pred.size(0) > i and boxes_pred.size(1) > 0 and boxes_pred.size(2) > 0:\n",
        "                pred_box = boxes_pred[i, 0, 0, :4]  # [4]\n",
        "                target_box = target_boxes[0]  # [4]\n",
        "                loss += nn.MSELoss()(pred_box, target_box)\n",
        "        return loss / len(targets) if len(targets) > 0 else torch.tensor(0.).to(device)\n",
        "\n",
        "    elif task == 'seg':\n",
        "        if not isinstance(targets, torch.Tensor):\n",
        "            return torch.tensor(0.).to(device)\n",
        "        batch_size, num_classes, height, width = seg_out.size()\n",
        "        seg_out = seg_out.permute(0, 2, 3, 1).contiguous().view(-1, num_classes)\n",
        "        targets = targets.to(device).view(-1)\n",
        "        return nn.CrossEntropyLoss()(seg_out, targets)\n",
        "\n",
        "    elif task == 'cls':\n",
        "        if not isinstance(targets, torch.Tensor):\n",
        "            return torch.tensor(0.).to(device)\n",
        "        targets = targets.to(device)\n",
        "        return nn.CrossEntropyLoss()(cls_out, targets)\n",
        "\n",
        "    return torch.tensor(0.).to(device)\n",
        "\n",
        "# 定義 IoU 計算函數（用於 mIoU 和 mAP）\n",
        "def calculate_iou(box1: torch.Tensor, box2: torch.Tensor) -> float:\n",
        "    \"\"\"\n",
        "    計算兩個邊界框的 IoU\n",
        "    參數：\n",
        "        box1 (torch.Tensor): 邊界框 1，格式 [x, y, w, h]\n",
        "        box2 (torch.Tensor): 邊界框 2，格式 [x, y, w, h]\n",
        "    返回：\n",
        "        iou (float): 交並比\n",
        "    \"\"\"\n",
        "    # Ensure tensors are on CPU for standard Python/NumPy operations\n",
        "    box1 = box1.cpu()\n",
        "    box2 = box2.cpu()\n",
        "\n",
        "    x1, y1, w1, h1 = box1\n",
        "    x2, y2, w2, h2 = box2\n",
        "    x_left = max(x1 - w1/2, x2 - w2/2)\n",
        "    y_top = max(y1 - h1/2, y2 - h2/2)\n",
        "    x_right = min(x1 + w1/2, x2 + w2/2)\n",
        "    y_bottom = min(y1 + h1/2, y2 + h2/2)\n",
        "    if x_right < x_left or y_bottom < y_top:\n",
        "        return 0.0\n",
        "    intersection = (x_right - x_left) * (y_bottom - y_top)\n",
        "    union = w1 * h1 + w2 * h2 - intersection\n",
        "    return intersection / union if union > 0 else 0.0\n",
        "\n",
        "# 定義評估函數\n",
        "def evaluate(model: nn.Module, loader: DataLoader, task: str) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    評估模型性能\n",
        "    參數：\n",
        "        model (nn.Module): 模型\n",
        "        loader (DataLoader): 數據加載器\n",
        "        task (str): 任務類型\n",
        "    返回：\n",
        "        metrics (dict): 評估指標\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    metrics = {'mIoU': 0.0, 'mAP': 0.0, 'Top-1': 0.0}\n",
        "    if task == 'seg':\n",
        "        # 初始化每個類別的交集與並集\n",
        "        num_classes = 20\n",
        "        intersection = torch.zeros(num_classes).to(device)\n",
        "        union = torch.zeros(num_classes).to(device)\n",
        "        total_batches = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in loader:\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "                _, seg_out, _ = model(inputs)\n",
        "                preds = seg_out.argmax(dim=1)  # [batch, 16, 16]\n",
        "                targets = targets  # [batch, 16, 16]\n",
        "                for c in range(num_classes):\n",
        "                    pred_mask = (preds == c)\n",
        "                    target_mask = (targets == c)\n",
        "                    intersection[c] += (pred_mask & target_mask).sum().float()\n",
        "                    union[c] += (pred_mask | target_mask).sum().float()\n",
        "                total_batches += 1\n",
        "        if total_batches > 0:\n",
        "            iou = intersection / (union + 1e-6)\n",
        "            metrics['mIoU'] = iou.mean().item()\n",
        "\n",
        "    elif task == 'det':\n",
        "        aps = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in loader:\n",
        "                inputs = inputs.to(device)\n",
        "                det_out, _, _ = model(inputs)\n",
        "                boxes_pred = det_out.permute(0, 2, 3, 1)  # [batch, 16, 16, 6]\n",
        "                for i in range(len(targets)):\n",
        "                    if not isinstance(targets[i], dict) or 'boxes' not in targets[i]:\n",
        "                        continue\n",
        "                    target_boxes = targets[i]['boxes'].to(device)\n",
        "                    if len(target_boxes) == 0:\n",
        "                        continue\n",
        "                    pred_boxes = boxes_pred[i, 0, 0, :4]  # 簡單取第一個框\n",
        "                    ious = [calculate_iou(pred_boxes, target_box) for target_box in target_boxes]\n",
        "                    ap = max(ious) if ious else 0.0  # 簡單 AP 計算\n",
        "                    aps.append(ap)\n",
        "        metrics['mAP'] = np.mean(aps) if aps else 0.0\n",
        "\n",
        "    elif task == 'cls':\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in loader:\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "                _, _, cls_out = model(inputs)\n",
        "                preds = cls_out.argmax(dim=1)\n",
        "                total_correct += (preds == targets).sum().item()\n",
        "                total_samples += targets.size(0)\n",
        "        metrics['Top-1'] = total_correct / total_samples if total_samples > 0 else 0.0\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# 定義訓練函數\n",
        "def train_stage(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, task: str, epochs: int, optimizer: optim.Optimizer,\n",
        "                replay_buffers: Dict[str, ReplayBuffer], tasks: List[str], stage: int) -> Tuple[List[float], List[float], Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    訓練單一階段\n",
        "    參數：\n",
        "        model (nn.Module): 模型\n",
        "        train_loader (DataLoader): 訓練數據加載器\n",
        "        val_loader (DataLoader): 驗證數據加載器\n",
        "        task (str): 當前任務\n",
        "        epochs (int): 訓練輪數\n",
        "        optimizer (optim.Optimizer): 優化器\n",
        "        replay_buffers (dict): Replay Buffer\n",
        "        tasks (list): 任務列表\n",
        "        stage (int): 當前階段\n",
        "    返回：\n",
        "        train_losses, val_metrics, final_metrics\n",
        "    \"\"\"\n",
        "    train_losses = []\n",
        "    val_metrics = []\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            if task != 'det':\n",
        "                targets = targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            det_out, seg_out, cls_out = model(inputs)\n",
        "            loss = compute_losses((det_out, seg_out, cls_out), targets, task)\n",
        "            epoch_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            # 存入 Replay Buffer\n",
        "            detached_inputs = inputs.detach().cpu()\n",
        "            detached_targets = copy.deepcopy(targets) if task == 'det' else targets.detach().cpu()\n",
        "            replay_buffers[task].add((detached_inputs, detached_targets))\n",
        "\n",
        "            # 加入 Replay Buffer 數據進行訓練（遺忘緩解）\n",
        "            replay_loss = 0.0\n",
        "            replay_batch_count = 0\n",
        "            for prev_task in tasks[:stage]:\n",
        "                buffer = replay_buffers[prev_task].sample()\n",
        "                for b_inputs, b_targets in buffer:\n",
        "                    b_inputs = b_inputs.to(device)\n",
        "                    b_det_out, b_seg_out, b_cls_out = model(b_inputs)\n",
        "                    replay_loss += compute_losses((b_det_out, b_seg_out, b_cls_out), b_targets, prev_task)\n",
        "                    replay_batch_count += 1\n",
        "            if stage > 0 and replay_loss > 0 and replay_batch_count > 0:\n",
        "                replay_loss /= replay_batch_count\n",
        "                loss += replay_loss\n",
        "\n",
        "            if isinstance(loss, torch.Tensor) and loss.requires_grad:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        train_losses.append(avg_loss)\n",
        "        print(f\"第 {epoch + 1} 個 epoch, {task} 平均損失: {avg_loss:.4f}\")\n",
        "\n",
        "        # 每 5 個 epoch 進行一次驗證\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            metrics = evaluate(model, val_loader, task)\n",
        "            val_metrics.append(metrics)\n",
        "            print(f\"階段 {stage + 1} ({task}) 驗證指標：{metrics}\")\n",
        "\n",
        "    final_metrics = evaluate(model, val_loader, task)\n",
        "    return train_losses, val_metrics, final_metrics\n",
        "\n",
        "# 定義繪製曲線函數\n",
        "def plot_curves(task_metrics: Dict[str, Tuple[List[float], List[Dict[str, float]]]]):\n",
        "    \"\"\"\n",
        "    繪製損失與指標曲線\n",
        "    參數：\n",
        "        task_metrics (dict): 每個任務的訓練損失與驗證指標\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # 繪製損失曲線\n",
        "    plt.subplot(1, 3, 1)\n",
        "    for task, (train_losses, _, _) in task_metrics.items():\n",
        "        plt.plot(train_losses, label=f'{task} Loss')\n",
        "    plt.title('訓練損失曲線')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('損失')\n",
        "    plt.legend()\n",
        "\n",
        "    # 繪製 mIoU 曲線（seg 任務）\n",
        "    plt.subplot(1, 3, 2)\n",
        "    seg_val_metrics = task_metrics['seg'][1]\n",
        "    seg_miou = [m['mIoU'] for m in seg_val_metrics]\n",
        "    epochs = [5 * (i + 1) for i in range(len(seg_miou))]\n",
        "    plt.plot(epochs, seg_miou, label='Seg mIoU')\n",
        "    plt.title('分割 mIoU 曲線')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('mIoU')\n",
        "    plt.legend()\n",
        "\n",
        "    # 繪製 Top-1 曲線（cls 任務）\n",
        "    plt.subplot(1, 3, 3)\n",
        "    cls_val_metrics = task_metrics['cls'][1]\n",
        "    cls_top1 = [m['Top-1'] for m in cls_val_metrics]\n",
        "    epochs = [5 * (i + 1) for i in range(len(cls_top1))]\n",
        "    plt.plot(epochs, cls_top1, label='Cls Top-1')\n",
        "    plt.title('分類 Top-1 曲線')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Top-1')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 定義優化器與學習率調度器\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# 訓練流程\n",
        "tasks = ['seg', 'det', 'cls']\n",
        "baselines = {'mIoU': 0.0, 'mAP': 0.0, 'Top-1': 0.0}\n",
        "task_metrics = {}\n",
        "\n",
        "# 增加 epoch 數到 20\n",
        "epochs_per_stage = 20\n",
        "total_training_time = 0\n",
        "\n",
        "for stage, task in enumerate(tasks):\n",
        "    print(f\"\\n訓練階段 {stage + 1}: {task}\")\n",
        "    start_time = time.time()\n",
        "    train_losses, val_metrics, final_metrics = train_stage(model, train_loaders[task], val_loaders[task], task,\n",
        "                                                           epochs_per_stage, optimizer, replay_buffers, tasks, stage)\n",
        "    stage_time = time.time() - start_time\n",
        "    total_training_time += stage_time\n",
        "    print(f\"階段 {stage + 1} 完成，耗時 {stage_time:.2f} 秒\")\n",
        "\n",
        "    task_metrics[task] = (train_losses, val_metrics, final_metrics)\n",
        "\n",
        "    # 記錄基準\n",
        "    if stage == 0:\n",
        "        baselines['mIoU'] = final_metrics.get('mIoU', 0.0)\n",
        "    elif stage == 1:\n",
        "        baselines['mAP'] = final_metrics.get('mAP', 0.0)\n",
        "    elif stage == 2:\n",
        "        baselines['Top-1'] = final_metrics.get('Top-1', 0.0)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "print(f\"\\n總訓練時間：{total_training_time:.2f} 秒 (< 2 小時：{total_training_time < 7200})\")\n",
        "\n",
        "# 最終評估\n",
        "print(\"\\n最終評估：\")\n",
        "final_metrics = {}\n",
        "for task in tasks:\n",
        "    metrics = evaluate(model, val_loaders[task], task)\n",
        "    final_metrics[task] = metrics\n",
        "    print(f\"最終 {task} 評估：{metrics}\")\n",
        "\n",
        "# 計算性能下降\n",
        "print(\"\\n性能下降（相較於各任務基準）：\")\n",
        "for task in tasks:\n",
        "    if task == 'seg':\n",
        "        baseline = baselines['mIoU']\n",
        "        final = final_metrics['seg']['mIoU']\n",
        "        metric_name = 'mIoU'\n",
        "    elif task == 'det':\n",
        "        baseline = baselines['mAP']\n",
        "        final = final_metrics['det']['mAP']\n",
        "        metric_name = 'mAP'\n",
        "    elif task == 'cls':\n",
        "        baseline = baselines['Top-1']\n",
        "        final = final_metrics['cls']['Top-1']\n",
        "        metric_name = 'Top-1'\n",
        "    if baseline > 0:\n",
        "        drop = (baseline - final) / baseline * 100\n",
        "        print(f\"{task} {metric_name} 下降：{drop:.2f}% (< 5%：{drop < 5})\")\n",
        "    else:\n",
        "        print(f\"{task} {metric_name}：基準為 0，無法計算下降。\")\n",
        "\n",
        "# 繪製曲線\n",
        "plot_curves(task_metrics)\n",
        "\n",
        "# 儲存模型\n",
        "torch.save(model.state_dict(), 'your_model.pt')\n",
        "print(\"模型已儲存為 'your_model.pt'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VNUf3ULkopvF",
        "outputId": "a28b73ee-4318-4e13-8d48-8ddc755a2ec4"
      },
      "id": "VNUf3ULkopvF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用設備：cuda\n",
            "模型總參數量：2,629,700 (< 8M: True)\n",
            "單張 512x512 圖像推理時間：8.22 ms (< 150 ms: True)\n",
            "\n",
            "訓練階段 1: seg\n",
            "第 1 個 epoch, seg 平均損失: 0.0773\n",
            "第 2 個 epoch, seg 平均損失: 0.0000\n",
            "第 3 個 epoch, seg 平均損失: 0.0000\n",
            "第 4 個 epoch, seg 平均損失: 0.0000\n",
            "第 5 個 epoch, seg 平均損失: 0.0000\n",
            "階段 1 (seg) 驗證指標：{'mIoU': 0.05000000074505806, 'mAP': 0.0, 'Top-1': 0.0}\n",
            "第 6 個 epoch, seg 平均損失: 0.0000\n",
            "第 7 個 epoch, seg 平均損失: 0.0000\n",
            "第 8 個 epoch, seg 平均損失: 0.0000\n",
            "第 9 個 epoch, seg 平均損失: 0.0000\n",
            "第 10 個 epoch, seg 平均損失: 0.0000\n",
            "階段 1 (seg) 驗證指標：{'mIoU': 0.05000000074505806, 'mAP': 0.0, 'Top-1': 0.0}\n",
            "第 11 個 epoch, seg 平均損失: 0.0000\n",
            "第 12 個 epoch, seg 平均損失: 0.0000\n",
            "第 13 個 epoch, seg 平均損失: 0.0000\n",
            "第 14 個 epoch, seg 平均損失: 0.0000\n",
            "第 15 個 epoch, seg 平均損失: 0.0000\n",
            "階段 1 (seg) 驗證指標：{'mIoU': 0.05000000074505806, 'mAP': 0.0, 'Top-1': 0.0}\n",
            "第 16 個 epoch, seg 平均損失: 0.0000\n",
            "第 17 個 epoch, seg 平均損失: 0.0000\n",
            "第 18 個 epoch, seg 平均損失: 0.0000\n",
            "第 19 個 epoch, seg 平均損失: 0.0000\n",
            "第 20 個 epoch, seg 平均損失: 0.0000\n",
            "階段 1 (seg) 驗證指標：{'mIoU': 0.05000000074505806, 'mAP': 0.0, 'Top-1': 0.0}\n",
            "階段 1 完成，耗時 94.47 秒\n",
            "\n",
            "訓練階段 2: det\n",
            "第 1 個 epoch, det 平均損失: 27612.9568\n",
            "第 2 個 epoch, det 平均損失: 19626.2047\n",
            "第 3 個 epoch, det 平均損失: 18079.6460\n",
            "第 4 個 epoch, det 平均損失: 17058.5593\n",
            "第 5 個 epoch, det 平均損失: 16682.9281\n",
            "階段 2 (det) 驗證指標：{'mIoU': 0.0, 'mAP': np.float64(0.16729227358397716), 'Top-1': 0.0}\n",
            "第 6 個 epoch, det 平均損失: 17875.2141\n",
            "第 7 個 epoch, det 平均損失: 17874.2893\n",
            "第 8 個 epoch, det 平均損失: 18005.0423\n",
            "第 9 個 epoch, det 平均損失: 17684.6187\n",
            "第 10 個 epoch, det 平均損失: 18447.4768\n",
            "階段 2 (det) 驗證指標：{'mIoU': 0.0, 'mAP': np.float64(0.16866915996797616), 'Top-1': 0.0}\n",
            "第 11 個 epoch, det 平均損失: 17983.7064\n",
            "第 12 個 epoch, det 平均損失: 18256.5720\n",
            "第 13 個 epoch, det 平均損失: 17651.3690\n",
            "第 14 個 epoch, det 平均損失: 17648.0251\n",
            "第 15 個 epoch, det 平均損失: 18403.3814\n",
            "階段 2 (det) 驗證指標：{'mIoU': 0.0, 'mAP': np.float64(0.16322009337697332), 'Top-1': 0.0}\n",
            "第 16 個 epoch, det 平均損失: 17815.6913\n",
            "第 17 個 epoch, det 平均損失: 18187.5285\n",
            "第 18 個 epoch, det 平均損失: 17728.9261\n",
            "第 19 個 epoch, det 平均損失: 17930.0435\n",
            "第 20 個 epoch, det 平均損失: 17740.8214\n",
            "階段 2 (det) 驗證指標：{'mIoU': 0.0, 'mAP': np.float64(0.17759132989449428), 'Top-1': 0.0}\n",
            "階段 2 完成，耗時 278.55 秒\n",
            "\n",
            "訓練階段 3: cls\n",
            "第 1 個 epoch, cls 平均損失: 11.3175\n",
            "第 2 個 epoch, cls 平均損失: 3.1364\n",
            "第 3 個 epoch, cls 平均損失: 3.2769\n",
            "第 4 個 epoch, cls 平均損失: 3.0651\n",
            "第 5 個 epoch, cls 平均損失: 3.0864\n",
            "階段 3 (cls) 驗證指標：{'mIoU': 0.0, 'mAP': 0.0, 'Top-1': 0.13333333333333333}\n",
            "第 6 個 epoch, cls 平均損失: 2.9375\n",
            "第 7 個 epoch, cls 平均損失: 2.7113\n",
            "第 8 個 epoch, cls 平均損失: 2.4739\n",
            "第 9 個 epoch, cls 平均損失: 2.2565\n",
            "第 10 個 epoch, cls 平均損失: 2.0830\n",
            "階段 3 (cls) 驗證指標：{'mIoU': 0.0, 'mAP': 0.0, 'Top-1': 0.18333333333333332}\n",
            "第 11 個 epoch, cls 平均損失: 2.0289\n",
            "第 12 個 epoch, cls 平均損失: 1.9737\n",
            "第 13 個 epoch, cls 平均損失: 1.9705\n",
            "第 14 個 epoch, cls 平均損失: 1.8362\n",
            "第 15 個 epoch, cls 平均損失: 1.7913\n",
            "階段 3 (cls) 驗證指標：{'mIoU': 0.0, 'mAP': 0.0, 'Top-1': 0.25}\n",
            "第 16 個 epoch, cls 平均損失: 1.6291\n",
            "第 17 個 epoch, cls 平均損失: 1.5644\n",
            "第 18 個 epoch, cls 平均損失: 1.5477\n",
            "第 19 個 epoch, cls 平均損失: 1.3687\n",
            "第 20 個 epoch, cls 平均損失: 1.3592\n",
            "階段 3 (cls) 驗證指標：{'mIoU': 0.0, 'mAP': 0.0, 'Top-1': 0.23333333333333334}\n",
            "階段 3 完成，耗時 696.59 秒\n",
            "\n",
            "總訓練時間：1069.62 秒 (< 2 小時：True)\n",
            "\n",
            "最終評估：\n",
            "最終 seg 評估：{'mIoU': 0.04999348893761635, 'mAP': 0.0, 'Top-1': 0.0}\n",
            "最終 det 評估：{'mIoU': 0.0, 'mAP': np.float64(0.1649848254901978), 'Top-1': 0.0}\n",
            "最終 cls 評估：{'mIoU': 0.0, 'mAP': 0.0, 'Top-1': 0.23333333333333334}\n",
            "\n",
            "性能下降（相較於各任務基準）：\n",
            "seg mIoU 下降：0.01% (< 5%：True)\n",
            "det mAP 下降：7.10% (< 5%：False)\n",
            "cls Top-1 下降：0.00% (< 5%：True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-bb6b0a8493a1>:541: UserWarning: Glyph 25613 (\\N{CJK UNIFIED IDEOGRAPH-640D}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "<ipython-input-16-bb6b0a8493a1>:541: UserWarning: Glyph 22833 (\\N{CJK UNIFIED IDEOGRAPH-5931}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "<ipython-input-16-bb6b0a8493a1>:541: UserWarning: Glyph 35347 (\\N{CJK UNIFIED IDEOGRAPH-8A13}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "<ipython-input-16-bb6b0a8493a1>:541: UserWarning: Glyph 32244 (\\N{CJK UNIFIED IDEOGRAPH-7DF4}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "<ipython-input-16-bb6b0a8493a1>:541: UserWarning: Glyph 26354 (\\N{CJK UNIFIED IDEOGRAPH-66F2}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "<ipython-input-16-bb6b0a8493a1>:541: UserWarning: Glyph 32218 (\\N{CJK UNIFIED IDEOGRAPH-7DDA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "<ipython-input-16-bb6b0a8493a1>:541: UserWarning: Glyph 20998 (\\N{CJK UNIFIED IDEOGRAPH-5206}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "<ipython-input-16-bb6b0a8493a1>:541: UserWarning: Glyph 21106 (\\N{CJK UNIFIED IDEOGRAPH-5272}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "<ipython-input-16-bb6b0a8493a1>:541: UserWarning: Glyph 39006 (\\N{CJK UNIFIED IDEOGRAPH-985E}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 25613 (\\N{CJK UNIFIED IDEOGRAPH-640D}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 22833 (\\N{CJK UNIFIED IDEOGRAPH-5931}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 35347 (\\N{CJK UNIFIED IDEOGRAPH-8A13}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 32244 (\\N{CJK UNIFIED IDEOGRAPH-7DF4}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 26354 (\\N{CJK UNIFIED IDEOGRAPH-66F2}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 32218 (\\N{CJK UNIFIED IDEOGRAPH-7DDA}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 20998 (\\N{CJK UNIFIED IDEOGRAPH-5206}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 21106 (\\N{CJK UNIFIED IDEOGRAPH-5272}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 39006 (\\N{CJK UNIFIED IDEOGRAPH-985E}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "模型已儲存為 'your_model.pt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 統一單頭多任務挑戰實作 (第三版優化)\n",
        "# 安裝所需庫\n",
        "!pip install torch torchvision torchaudio -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import copy\n",
        "import time\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"使用設備：{device}\")\n",
        "\n",
        "class MultiTaskDataset(Dataset):\n",
        "    def __init__(self, data_dir: str, task: str, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.task = task\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.annotations = []\n",
        "        if task == 'det':\n",
        "            labels_path = os.path.join(data_dir, 'labels.json')\n",
        "            if not os.path.exists(labels_path):\n",
        "                raise FileNotFoundError(f\"找不到 {labels_path}，請確認檔案是否存在！\")\n",
        "            with open(labels_path, 'r') as f:\n",
        "                labels_data = json.load(f)\n",
        "            image_dir = os.path.join(data_dir, 'data')\n",
        "            image_files = sorted([img for img in os.listdir(image_dir) if img.endswith(('.jpg', '.jpeg', '.JPEG'))])\n",
        "            image_file_set = set(image_files)\n",
        "            valid_images = {img['id']: img['file_name'] for img in labels_data['images'] if img['file_name'] in image_file_set}\n",
        "            ann_dict = {}\n",
        "            for ann in labels_data['annotations']:\n",
        "                img_id = ann['image_id']\n",
        "                if img_id in valid_images:\n",
        "                    if img_id not in ann_dict:\n",
        "                        ann_dict[img_id] = []\n",
        "                    ann_dict[img_id].append({'boxes': ann['bbox'], 'labels': ann['category_id']})\n",
        "            for img_id, file_name in valid_images.items():\n",
        "                full_path = os.path.join(image_dir, file_name)\n",
        "                if img_id in ann_dict:\n",
        "                    self.images.append(full_path)\n",
        "                    self.annotations.append(ann_dict[img_id])\n",
        "        elif task == 'seg':\n",
        "            image_files = sorted([img for img in os.listdir(data_dir) if img.endswith(('.jpg', '.jpeg', '.JPEG'))])\n",
        "            for img in image_files:\n",
        "                img_path = os.path.join(data_dir, img)\n",
        "                mask_path = os.path.join(data_dir, img.replace('.jpg', '.png').replace('.jpeg', '.png').replace('.JPEG', '.png'))\n",
        "                if os.path.exists(mask_path):\n",
        "                    self.images.append(img_path)\n",
        "                    self.annotations.append(mask_path)\n",
        "        elif task == 'cls':\n",
        "            label_dirs = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
        "            label_to_index = {label: idx for idx, label in enumerate(label_dirs)}\n",
        "            for label in label_dirs:\n",
        "                label_path = os.path.join(data_dir, label)\n",
        "                for root, _, files in os.walk(label_path):\n",
        "                    for img in files:\n",
        "                        if img.endswith(('.jpg', '.jpeg', '.JPEG')):\n",
        "                            img_path = os.path.join(root, img)\n",
        "                            self.images.append(img_path)\n",
        "                            self.annotations.append(label_to_index[label])\n",
        "        if len(self.images) == 0:\n",
        "            raise ValueError(f\"在 {data_dir} 中未找到任何資料，請檢查資料結構！\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, any]:\n",
        "        img_path = self.images[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        if self.task == 'seg':\n",
        "            mask = Image.open(self.annotations[idx]).convert('L')\n",
        "            mask = np.array(mask) / 255.0 * 19.0\n",
        "            mask = Image.fromarray(mask.astype(np.uint8))\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "                mask_transform = transforms.Compose([\n",
        "                    transforms.Resize((16, 16), interpolation=Image.Resampling.NEAREST),\n",
        "                    transforms.ToTensor()\n",
        "                ])\n",
        "                mask = mask_transform(mask)\n",
        "            return img, mask.squeeze(0).long()\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        if self.task == 'det':\n",
        "            ann = self.annotations[idx]\n",
        "            boxes = torch.tensor([a['boxes'] for a in ann], dtype=torch.float32)\n",
        "            labels = torch.tensor([a['labels'] for a in ann], dtype=torch.long)\n",
        "            return img, {'boxes': boxes, 'labels': labels}\n",
        "        elif self.task == 'cls':\n",
        "            return img, torch.tensor(self.annotations[idx], dtype=torch.long)\n",
        "\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((512, 512), interpolation=Image.Resampling.BILINEAR),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_datasets = {\n",
        "    'det': MultiTaskDataset('data/mini_coco_det/train', 'det', image_transform),\n",
        "    'seg': MultiTaskDataset('data/mini_voc_seg/train', 'seg', image_transform),\n",
        "    'cls': MultiTaskDataset('data/imagenette_160/train', 'cls', image_transform)\n",
        "}\n",
        "val_datasets = {\n",
        "    'det': MultiTaskDataset('data/mini_coco_det/val', 'det', image_transform),\n",
        "    'seg': MultiTaskDataset('data/mini_voc_seg/val', 'seg', image_transform),\n",
        "    'cls': MultiTaskDataset('data/imagenette_160/val', 'cls', image_transform)\n",
        "}\n",
        "\n",
        "def custom_collate(batch: List[Tuple[torch.Tensor, any]]) -> Tuple[torch.Tensor, List[any]]:\n",
        "    images = torch.stack([item[0] for item in batch])\n",
        "    targets = [item[1] for item in batch]\n",
        "    return images, targets\n",
        "\n",
        "train_loaders = {task: DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=custom_collate if task == 'det' else None) for task, dataset in train_datasets.items()}\n",
        "val_loaders = {task: DataLoader(dataset, batch_size=4, shuffle=False, collate_fn=custom_collate if task == 'det' else None) for task, dataset in val_datasets.items()}\n",
        "\n",
        "class MultiTaskHead(nn.Module):\n",
        "    def __init__(self, in_channels: int = 576):\n",
        "        super(MultiTaskHead, self).__init__()\n",
        "        self.neck = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=1)\n",
        "        )\n",
        "        self.det_head = nn.Conv2d(128, 6, kernel_size=1)\n",
        "        self.seg_head = nn.Conv2d(128, 20, kernel_size=1)\n",
        "        self.cls_head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        x = self.neck(x)\n",
        "        x = self.head(x)\n",
        "        det_out = self.det_head(x)\n",
        "        seg_out = self.seg_head(x)\n",
        "        cls_out = self.cls_head(x)\n",
        "        return det_out, seg_out, cls_out\n",
        "\n",
        "class UnifiedModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UnifiedModel, self).__init__()\n",
        "        self.backbone = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1).features\n",
        "        self.head = MultiTaskHead(in_channels=576)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        features = self.backbone(x)\n",
        "        det_out, seg_out, cls_out = self.head(features)\n",
        "        return det_out, seg_out, cls_out\n",
        "\n",
        "model = UnifiedModel().to(device)\n",
        "\n",
        "def count_parameters(model: nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_params = count_parameters(model)\n",
        "print(f\"模型總參數量：{total_params:,} (< 8M: {total_params < 8_000_000})\")\n",
        "\n",
        "def measure_inference_time(model: nn.Module, input_size: Tuple[int, int, int, int], device: torch.device, num_runs: int = 100) -> float:\n",
        "    model.eval()\n",
        "    dummy_input = torch.randn(input_size).to(device)\n",
        "    for _ in range(10):\n",
        "        model(dummy_input)\n",
        "    start_time = time.time()\n",
        "    for _ in range(num_runs):\n",
        "        model(dummy_input)\n",
        "    end_time = time.time()\n",
        "    return (end_time - start_time) / num_runs * 1000\n",
        "\n",
        "inference_time = measure_inference_time(model, (1, 3, 512, 512), device)\n",
        "print(f\"單張 512x512 圖像推理時間：{inference_time:.2f} ms (< 150 ms: {inference_time < 150})\")\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity: int = 50):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "\n",
        "    def add(self, data: Tuple[torch.Tensor, any]):\n",
        "        if len(self.buffer) >= self.capacity:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append(data)\n",
        "\n",
        "    def sample(self) -> List[Tuple[torch.Tensor, any]]:\n",
        "        return self.buffer\n",
        "\n",
        "replay_buffers = {task: ReplayBuffer(capacity=50) for task in ['seg', 'det', 'cls']}\n",
        "\n",
        "def compute_losses(outputs: Tuple[torch.Tensor, torch.Tensor, torch.Tensor], targets: any, task: str) -> torch.Tensor:\n",
        "    det_out, seg_out, cls_out = outputs\n",
        "    if task == 'det':\n",
        "        if not isinstance(targets, list):\n",
        "            return torch.tensor(0.).to(device)\n",
        "        boxes_pred = det_out.permute(0, 2, 3, 1)\n",
        "        loss = 0\n",
        "        for i in range(len(targets)):\n",
        "            if not isinstance(targets[i], dict) or 'boxes' not in targets[i]:\n",
        "                continue\n",
        "            target_boxes = targets[i]['boxes'].to(device)\n",
        "            if len(target_boxes) == 0:\n",
        "                continue\n",
        "            pred_box = boxes_pred[i, 0, 0, :4]\n",
        "            target_box = target_boxes[0]\n",
        "            iou = calculate_iou(pred_box, target_box)\n",
        "            loss += 1 - iou if iou > 0 else 1  # IoU Loss\n",
        "        return loss / len(targets) if len(targets) > 0 else torch.tensor(0.).to(device)\n",
        "    elif task == 'seg':\n",
        "        if not isinstance(targets, torch.Tensor):\n",
        "            return torch.tensor(0.).to(device)\n",
        "        batch_size, num_classes, height, width = seg_out.size()\n",
        "        seg_out = seg_out.permute(0, 2, 3, 1).contiguous().view(-1, num_classes)\n",
        "        targets = targets.to(device).view(-1)\n",
        "        return nn.CrossEntropyLoss()(seg_out, targets)\n",
        "    elif task == 'cls':\n",
        "        if not isinstance(targets, torch.Tensor):\n",
        "            return torch.tensor(0.).to(device)\n",
        "        targets = targets.to(device)\n",
        "        return nn.CrossEntropyLoss()(cls_out, targets)\n",
        "    return torch.tensor(0.).to(device)\n",
        "\n",
        "def calculate_iou(box1: torch.Tensor, box2: torch.Tensor) -> float:\n",
        "    box1 = box1.cpu()\n",
        "    box2 = box2.cpu()\n",
        "    x1, y1, w1, h1 = box1\n",
        "    x2, y2, w2, h2 = box2\n",
        "    x_left = max(x1 - w1/2, x2 - w2/2)\n",
        "    y_top = max(y1 - h1/2, y2 - h2/2)\n",
        "    x_right = min(x1 + w1/2, x2 + w2/2)\n",
        "    y_bottom = min(y1 + h1/2, y2 + h2/2)\n",
        "    if x_right < x_left or y_bottom < y_top:\n",
        "        return 0.0\n",
        "    intersection = (x_right - x_left) * (y_bottom - y_top)\n",
        "    union = w1 * h1 + w2 * h2 - intersection\n",
        "    return intersection / union if union > 0 else 0.0\n",
        "\n",
        "def evaluate(model: nn.Module, loader: DataLoader, task: str) -> Dict[str, float]:\n",
        "    model.eval()\n",
        "    metrics = {'mIoU': 0.0, 'mAP': 0.0, 'Top-1': 0.0}\n",
        "    if task == 'seg':\n",
        "        num_classes = 20\n",
        "        intersection = torch.zeros(num_classes).to(device)\n",
        "        union = torch.zeros(num_classes).to(device)\n",
        "        total_batches = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in loader:\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "                _, seg_out, _ = model(inputs)\n",
        "                preds = seg_out.argmax(dim=1)\n",
        "                targets = targets\n",
        "                for c in range(num_classes):\n",
        "                    pred_mask = (preds == c)\n",
        "                    target_mask = (targets == c)\n",
        "                    intersection[c] += (pred_mask & target_mask).sum().float()\n",
        "                    union[c] += (pred_mask | target_mask).sum().float()\n",
        "                total_batches += 1\n",
        "        if total_batches > 0:\n",
        "            iou = intersection / (union + 1e-6)\n",
        "            metrics['mIoU'] = float(iou.mean().item())\n",
        "    elif task == 'det':\n",
        "        aps = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in loader:\n",
        "                inputs = inputs.to(device)\n",
        "                det_out, _, _ = model(inputs)\n",
        "                boxes_pred = det_out.permute(0, 2, 3, 1)\n",
        "                for i in range(len(targets)):\n",
        "                    if not isinstance(targets[i], dict) or 'boxes' not in targets[i]:\n",
        "                        continue\n",
        "                    target_boxes = targets[i]['boxes'].to(device)\n",
        "                    if len(target_boxes) == 0:\n",
        "                        continue\n",
        "                    pred_boxes = boxes_pred[i].view(-1, 6)[:, :4]\n",
        "                    ious = [calculate_iou(pred_box, target_box) for pred_box in pred_boxes for target_box in target_boxes]\n",
        "                    ap = max(ious) if ious else 0.0\n",
        "                    aps.append(ap)\n",
        "        metrics['mAP'] = float(np.mean(aps)) if aps else 0.0\n",
        "    elif task == 'cls':\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in loader:\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "                _, _, cls_out = model(inputs)\n",
        "                preds = cls_out.argmax(dim=1)\n",
        "                total_correct += (preds == targets).sum().item()\n",
        "                total_samples += targets.size(0)\n",
        "        metrics['Top-1'] = total_correct / total_samples if total_samples > 0 else 0.0\n",
        "    return metrics\n",
        "\n",
        "def train_stage(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, task: str, epochs: int, optimizer: optim.Optimizer,\n",
        "                replay_buffers: Dict[str, ReplayBuffer], tasks: List[str], stage: int) -> Tuple[List[float], List[float], Dict[str, float]]:\n",
        "    train_losses = []\n",
        "    val_metrics = []\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            if task != 'det':\n",
        "                targets = targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            det_out, seg_out, cls_out = model(inputs)\n",
        "            loss = compute_losses((det_out, seg_out, cls_out), targets, task)\n",
        "            epoch_loss += loss.item()\n",
        "            detached_inputs = inputs.detach().cpu()\n",
        "            detached_targets = copy.deepcopy(targets) if task == 'det' else targets.detach().cpu()\n",
        "            replay_buffers[task].add((detached_inputs, detached_targets))\n",
        "            replay_loss = 0.0\n",
        "            replay_batch_count = 0\n",
        "            for prev_task in tasks[:stage]:\n",
        "                buffer = replay_buffers[prev_task].sample()\n",
        "                for b_inputs, b_targets in buffer:\n",
        "                    b_inputs = b_inputs.to(device)\n",
        "                    b_det_out, b_seg_out, b_cls_out = model(b_inputs)\n",
        "                    replay_loss += compute_losses((b_det_out, b_seg_out, b_cls_out), b_targets, prev_task)\n",
        "                    replay_batch_count += 1\n",
        "            if stage > 0 and replay_loss > 0 and replay_batch_count > 0:\n",
        "                replay_loss /= replay_batch_count\n",
        "                loss += replay_loss\n",
        "            if isinstance(loss, torch.Tensor) and loss.requires_grad:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        train_losses.append(avg_loss)\n",
        "        print(f\"第 {epoch + 1} 個 epoch, {task} 平均損失: {avg_loss:.4f}\")\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            metrics = evaluate(model, val_loader, task)\n",
        "            val_metrics.append(metrics)\n",
        "            print(f\"階段 {stage + 1} ({task}) 驗證指標：{metrics}\")\n",
        "    final_metrics = evaluate(model, val_loader, task)\n",
        "    return train_losses, val_metrics, final_metrics\n",
        "\n",
        "def plot_curves(task_metrics: Dict[str, Tuple[List[float], List[Dict[str, float]]]]):\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    for task, (train_losses, _, _) in task_metrics.items():\n",
        "        plt.plot(train_losses, label=f'{task} Loss')\n",
        "    plt.title('訓練損失曲線')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('損失')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 3, 2)\n",
        "    seg_val_metrics = task_metrics['seg'][1]\n",
        "    seg_miou = [m['mIoU'] for m in seg_val_metrics]\n",
        "    epochs = [5 * (i + 1) for i in range(len(seg_miou))]\n",
        "    plt.plot(epochs, seg_miou, label='Seg mIoU')\n",
        "    plt.title('分割 mIoU 曲線')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('mIoU')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 3, 3)\n",
        "    cls_val_metrics = task_metrics['cls'][1]\n",
        "    cls_top1 = [m['Top-1'] for m in cls_val_metrics]\n",
        "    epochs = [5 * (i + 1) for i in range(len(cls_top1))]\n",
        "    plt.plot(epochs, cls_top1, label='Cls Top-1')\n",
        "    plt.title('分類 Top-1 曲線')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Top-1')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
        "\n",
        "tasks = ['seg', 'det', 'cls']\n",
        "baselines = {'mIoU': 0.0, 'mAP': 0.0, 'Top-1': 0.0}\n",
        "task_metrics = {}\n",
        "\n",
        "epochs_per_stage = 120\n",
        "total_training_time = 0\n",
        "\n",
        "for stage, task in enumerate(tasks):\n",
        "    print(f\"\\n訓練階段 {stage + 1}: {task}\")\n",
        "    start_time = time.time()\n",
        "    train_losses, val_metrics, final_metrics = train_stage(model, train_loaders[task], val_loaders[task], task,\n",
        "                                                           epochs_per_stage, optimizer, replay_buffers, tasks, stage)\n",
        "    stage_time = time.time() - start_time\n",
        "    total_training_time += stage_time\n",
        "    print(f\"階段 {stage + 1} 完成，耗時 {stage_time:.2f} 秒\")\n",
        "    task_metrics[task] = (train_losses, val_metrics, final_metrics)\n",
        "    if stage == 0:\n",
        "        baselines['mIoU'] = final_metrics.get('mIoU', 0.0)\n",
        "    elif stage == 1:\n",
        "        baselines['mAP'] = final_metrics.get('mAP', 0.0)\n",
        "    elif stage == 2:\n",
        "        baselines['Top-1'] = final_metrics.get('Top-1', 0.0)\n",
        "    scheduler.step()\n",
        "\n",
        "print(f\"\\n總訓練時間：{total_training_time:.2f} 秒 (< 2 小時：{total_training_time < 7200})\")\n",
        "\n",
        "print(\"\\n最終評估：\")\n",
        "final_metrics = {}\n",
        "for task in tasks:\n",
        "    metrics = evaluate(model, val_loaders[task], task)\n",
        "    final_metrics[task] = metrics\n",
        "    print(f\"最終 {task} 評估：{metrics}\")\n",
        "\n",
        "print(\"\\n性能下降（相較於各任務基準）：\")\n",
        "for task in tasks:\n",
        "    if task == 'seg':\n",
        "        baseline = baselines['mIoU']\n",
        "        final = final_metrics['seg']['mIoU']\n",
        "        metric_name = 'mIoU'\n",
        "    elif task == 'det':\n",
        "        baseline = baselines['mAP']\n",
        "        final = final_metrics['det']['mAP']\n",
        "        metric_name = 'mAP'\n",
        "    elif task == 'cls':\n",
        "        baseline = baselines['Top-1']\n",
        "        final = final_metrics['cls']['Top-1']\n",
        "        metric_name = 'Top-1'\n",
        "    if baseline > 0:\n",
        "        drop = (baseline - final) / baseline * 100\n",
        "        print(f\"{task} {metric_name} 下降：{drop:.2f}% (< 5%：{drop < 5})\")\n",
        "    else:\n",
        "        print(f\"{task} {metric_name}：基準為 0，無法計算下降。\")\n",
        "\n",
        "plot_curves(task_metrics)\n",
        "torch.save(model.state_dict(), 'your_model.pt')\n",
        "print(\"模型已儲存為 'your_model3.pt'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pHo6u694uXc5",
        "outputId": "d3bc2aec-03e2-4e8b-ea95-184256a0cb6a"
      },
      "id": "pHo6u694uXc5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用設備：cuda\n",
            "模型總參數量：5,077,828 (< 8M: True)\n",
            "單張 512x512 圖像推理時間：10.94 ms (< 150 ms: True)\n",
            "\n",
            "訓練階段 1: seg\n",
            "第 1 個 epoch, seg 平均損失: 0.0546\n",
            "第 2 個 epoch, seg 平均損失: 0.0000\n",
            "第 3 個 epoch, seg 平均損失: 0.0000\n",
            "第 4 個 epoch, seg 平均損失: 0.0000\n",
            "第 5 個 epoch, seg 平均損失: 0.0000\n",
            "階段 1 (seg) 驗證指標：{'mIoU': 0.05000000074505806, 'mAP': 0.0, 'Top-1': 0.0}\n",
            "第 6 個 epoch, seg 平均損失: 0.0000\n",
            "第 7 個 epoch, seg 平均損失: 0.0000\n",
            "第 8 個 epoch, seg 平均損失: 0.0000\n",
            "第 9 個 epoch, seg 平均損失: 0.0000\n",
            "第 10 個 epoch, seg 平均損失: 0.0000\n",
            "階段 1 (seg) 驗證指標：{'mIoU': 0.05000000074505806, 'mAP': 0.0, 'Top-1': 0.0}\n",
            "第 11 個 epoch, seg 平均損失: 0.0000\n",
            "第 12 個 epoch, seg 平均損失: 0.0000\n",
            "第 13 個 epoch, seg 平均損失: 0.0000\n",
            "第 14 個 epoch, seg 平均損失: 0.0000\n",
            "第 15 個 epoch, seg 平均損失: 0.0000\n",
            "階段 1 (seg) 驗證指標：{'mIoU': 0.05000000074505806, 'mAP': 0.0, 'Top-1': 0.0}\n",
            "第 16 個 epoch, seg 平均損失: 0.0000\n",
            "第 17 個 epoch, seg 平均損失: 0.0000\n",
            "第 18 個 epoch, seg 平均損失: 0.0000\n",
            "第 19 個 epoch, seg 平均損失: 0.0000\n",
            "第 20 個 epoch, seg 平均損失: 0.0000\n",
            "階段 1 (seg) 驗證指標：{'mIoU': 0.05000000074505806, 'mAP': 0.0, 'Top-1': 0.0}\n",
            "第 21 個 epoch, seg 平均損失: 0.0000\n",
            "第 22 個 epoch, seg 平均損失: 0.0000\n",
            "第 23 個 epoch, seg 平均損失: 0.0000\n",
            "第 24 個 epoch, seg 平均損失: 0.0000\n",
            "第 25 個 epoch, seg 平均損失: 0.0000\n",
            "階段 1 (seg) 驗證指標：{'mIoU': 0.05000000074505806, 'mAP': 0.0, 'Top-1': 0.0}\n",
            "第 26 個 epoch, seg 平均損失: 0.0000\n",
            "第 27 個 epoch, seg 平均損失: 0.0000\n",
            "第 28 個 epoch, seg 平均損失: 0.0000\n",
            "第 29 個 epoch, seg 平均損失: 0.0000\n",
            "第 30 個 epoch, seg 平均損失: 0.0000\n",
            "階段 1 (seg) 驗證指標：{'mIoU': 0.05000000074505806, 'mAP': 0.0, 'Top-1': 0.0}\n",
            "第 31 個 epoch, seg 平均損失: 0.0000\n",
            "第 32 個 epoch, seg 平均損失: 0.0000\n",
            "第 33 個 epoch, seg 平均損失: 0.0000\n",
            "第 34 個 epoch, seg 平均損失: 0.0000\n",
            "第 35 個 epoch, seg 平均損失: 0.0000\n",
            "階段 1 (seg) 驗證指標：{'mIoU': 0.05000000074505806, 'mAP': 0.0, 'Top-1': 0.0}\n",
            "第 36 個 epoch, seg 平均損失: 0.0000\n",
            "第 37 個 epoch, seg 平均損失: 0.0000\n",
            "第 38 個 epoch, seg 平均損失: 0.0000\n",
            "第 39 個 epoch, seg 平均損失: 0.0000\n",
            "第 40 個 epoch, seg 平均損失: 0.0000\n",
            "階段 1 (seg) 驗證指標：{'mIoU': 0.05000000074505806, 'mAP': 0.0, 'Top-1': 0.0}\n",
            "第 41 個 epoch, seg 平均損失: 0.0000\n",
            "第 42 個 epoch, seg 平均損失: 0.0000\n",
            "第 43 個 epoch, seg 平均損失: 0.0000\n",
            "第 44 個 epoch, seg 平均損失: 0.0000\n",
            "第 45 個 epoch, seg 平均損失: 0.0000\n",
            "階段 1 (seg) 驗證指標：{'mIoU': 0.05000000074505806, 'mAP': 0.0, 'Top-1': 0.0}\n",
            "第 46 個 epoch, seg 平均損失: 0.0000\n",
            "第 47 個 epoch, seg 平均損失: 0.0000\n",
            "第 48 個 epoch, seg 平均損失: 0.0000\n",
            "第 49 個 epoch, seg 平均損失: 0.0000\n",
            "第 50 個 epoch, seg 平均損失: 0.0000\n",
            "階段 1 (seg) 驗證指標：{'mIoU': 0.05000000074505806, 'mAP': 0.0, 'Top-1': 0.0}\n",
            "第 51 個 epoch, seg 平均損失: 0.0000\n",
            "第 52 個 epoch, seg 平均損失: 0.0000\n",
            "第 53 個 epoch, seg 平均損失: 0.0000\n",
            "第 54 個 epoch, seg 平均損失: 0.0000\n",
            "第 55 個 epoch, seg 平均損失: 0.0000\n",
            "階段 1 (seg) 驗證指標：{'mIoU': 0.05000000074505806, 'mAP': 0.0, 'Top-1': 0.0}\n",
            "第 56 個 epoch, seg 平均損失: 0.0000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-8b3839becefd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n訓練階段 {stage + 1}: {task}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m     train_losses, val_metrics, final_metrics = train_stage(model, train_loaders[task], val_loaders[task], task,\n\u001b[0m\u001b[1;32m    394\u001b[0m                                                            epochs_per_stage, optimizer, replay_buffers, tasks, stage)\n\u001b[1;32m    395\u001b[0m     \u001b[0mstage_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-8b3839becefd>\u001b[0m in \u001b[0;36mtrain_stage\u001b[0;34m(model, train_loader, val_loader, task, epochs, optimizer, replay_buffers, tasks, stage)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdet_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mdetached_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m             \u001b[0mdetached_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'det'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mreplay_buffers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetached_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetached_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 統一單頭多任務挑戰實作 (第四版優化)\n",
        "# 安裝所需庫\n",
        "!pip install torch torchvision torchaudio -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import copy\n",
        "import time\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"使用設備：{device}\")\n",
        "\n",
        "class MultiTaskDataset(Dataset):\n",
        "    def __init__(self, data_dir: str, task: str, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.task = task\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.annotations = []\n",
        "        if task == 'det':\n",
        "            labels_path = os.path.join(data_dir, 'labels.json')\n",
        "            if not os.path.exists(labels_path):\n",
        "                raise FileNotFoundError(f\"找不到 {labels_path}，請確認檔案是否存在！\")\n",
        "            with open(labels_path, 'r') as f:\n",
        "                labels_data = json.load(f)\n",
        "            image_dir = os.path.join(data_dir, 'data')\n",
        "            image_files = sorted([img for img in os.listdir(image_dir) if img.endswith(('.jpg', '.jpeg', '.JPEG'))])\n",
        "            image_file_set = set(image_files)\n",
        "            valid_images = {img['id']: img['file_name'] for img in labels_data['images'] if img['file_name'] in image_file_set}\n",
        "            ann_dict = {}\n",
        "            for ann in labels_data['annotations']:\n",
        "                img_id = ann['image_id']\n",
        "                if img_id in valid_images:\n",
        "                    if img_id not in ann_dict:\n",
        "                        ann_dict[img_id] = []\n",
        "                    ann_dict[img_id].append({'boxes': ann['bbox'], 'labels': ann['category_id']})\n",
        "            for img_id, file_name in valid_images.items():\n",
        "                full_path = os.path.join(image_dir, file_name)\n",
        "                if img_id in ann_dict:\n",
        "                    self.images.append(full_path)\n",
        "                    self.annotations.append(ann_dict[img_id])\n",
        "        elif task == 'seg':\n",
        "            image_files = sorted([img for img in os.listdir(data_dir) if img.endswith(('.jpg', '.jpeg', '.JPEG'))])\n",
        "            for img in image_files:\n",
        "                img_path = os.path.join(data_dir, img)\n",
        "                mask_path = os.path.join(data_dir, img.replace('.jpg', '.png').replace('.jpeg', '.png').replace('.JPEG', '.png'))\n",
        "                if os.path.exists(mask_path):\n",
        "                    self.images.append(img_path)\n",
        "                    self.annotations.append(mask_path)\n",
        "            # 檢查掩碼值分佈\n",
        "            if task == 'seg':\n",
        "                self._check_mask_distribution()\n",
        "        elif task == 'cls':\n",
        "            label_dirs = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
        "            label_to_index = {label: idx for idx, label in enumerate(label_dirs)}\n",
        "            for label in label_dirs:\n",
        "                label_path = os.path.join(data_dir, label)\n",
        "                for root, _, files in os.walk(label_path):\n",
        "                    for img in files:\n",
        "                        if img.endswith(('.jpg', '.jpeg', '.JPEG')):\n",
        "                            img_path = os.path.join(root, img)\n",
        "                            self.images.append(img_path)\n",
        "                            self.annotations.append(label_to_index[label])\n",
        "        if len(self.images) == 0:\n",
        "            raise ValueError(f\"在 {data_dir} 中未找到任何資料，請檢查資料結構！\")\n",
        "\n",
        "    def _check_mask_distribution(self):\n",
        "        \"\"\"檢查 seg 任務掩碼值分佈\"\"\"\n",
        "        unique_values = set()\n",
        "        for mask_path in self.annotations:\n",
        "            mask = Image.open(mask_path).convert('L')\n",
        "            mask_array = np.array(mask)\n",
        "            unique_values.update(np.unique(mask_array))\n",
        "        print(f\"Seg 數據集掩碼值分佈：{sorted(unique_values)}\")\n",
        "        if max(unique_values) > 255 or min(unique_values) < 0:\n",
        "            raise ValueError(\"掩碼值超出範圍 [0, 255]！\")\n",
        "        if len(unique_values) > 20:\n",
        "            print(\"警告：掩碼值種類超過 20，可能需要重新映射！\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, any]:\n",
        "        img_path = self.images[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        if self.task == 'seg':\n",
        "            mask = Image.open(self.annotations[idx]).convert('L')\n",
        "            mask_array = np.array(mask)\n",
        "            # 修正映射：假設原始值已為類別索引（0-19），不再進行線性縮放\n",
        "            if mask_array.max() > 19:\n",
        "                raise ValueError(f\"掩碼值超過 19：{mask_array.max()}，請檢查數據！\")\n",
        "            mask = Image.fromarray(mask_array.astype(np.uint8))\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "                mask_transform = transforms.Compose([\n",
        "                    transforms.Resize((16, 16), interpolation=Image.Resampling.NEAREST),\n",
        "                    transforms.ToTensor()\n",
        "                ])\n",
        "                mask = mask_transform(mask)\n",
        "            return img, mask.squeeze(0).long()\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        if self.task == 'det':\n",
        "            ann = self.annotations[idx]\n",
        "            boxes = torch.tensor([a['boxes'] for a in ann], dtype=torch.float32)\n",
        "            labels = torch.tensor([a['labels'] for a in ann], dtype=torch.long)\n",
        "            return img, {'boxes': boxes, 'labels': labels}\n",
        "        elif self.task == 'cls':\n",
        "            return img, torch.tensor(self.annotations[idx], dtype=torch.long)\n",
        "\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((512, 512), interpolation=Image.Resampling.BILINEAR),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_datasets = {\n",
        "    'det': MultiTaskDataset('data/mini_coco_det/train', 'det', image_transform),\n",
        "    'seg': MultiTaskDataset('data/mini_voc_seg/train', 'seg', image_transform),\n",
        "    'cls': MultiTaskDataset('data/imagenette_160/train', 'cls', image_transform)\n",
        "}\n",
        "val_datasets = {\n",
        "    'det': MultiTaskDataset('data/mini_coco_det/val', 'det', image_transform),\n",
        "    'seg': MultiTaskDataset('data/mini_voc_seg/val', 'seg', image_transform),\n",
        "    'cls': MultiTaskDataset('data/imagenette_160/val', 'cls', image_transform)\n",
        "}\n",
        "\n",
        "def custom_collate(batch: List[Tuple[torch.Tensor, any]]) -> Tuple[torch.Tensor, List[any]]:\n",
        "    images = torch.stack([item[0] for item in batch])\n",
        "    targets = [item[1] for item in batch]\n",
        "    return images, targets\n",
        "\n",
        "train_loaders = {task: DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=custom_collate if task == 'det' else None) for task, dataset in train_datasets.items()}\n",
        "val_loaders = {task: DataLoader(dataset, batch_size=4, shuffle=False, collate_fn=custom_collate if task == 'det' else None) for task, dataset in val_datasets.items()}\n",
        "\n",
        "class MultiTaskHead(nn.Module):\n",
        "    def __init__(self, in_channels: int = 576):\n",
        "        super(MultiTaskHead, self).__init__()\n",
        "        self.neck = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=1)\n",
        "        )\n",
        "        self.det_head = nn.Conv2d(128, 6, kernel_size=1)\n",
        "        self.seg_head = nn.Conv2d(128, 20, kernel_size=1)\n",
        "        self.cls_head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        x = self.neck(x)\n",
        "        x = self.head(x)\n",
        "        det_out = self.det_head(x)\n",
        "        seg_out = self.seg_head(x)\n",
        "        cls_out = self.cls_head(x)\n",
        "        return det_out, seg_out, cls_out\n",
        "\n",
        "class UnifiedModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UnifiedModel, self).__init__()\n",
        "        self.backbone = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1).features\n",
        "        self.head = MultiTaskHead(in_channels=576)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        features = self.backbone(x)\n",
        "        det_out, seg_out, cls_out = self.head(features)\n",
        "        return det_out, seg_out, cls_out\n",
        "\n",
        "model = UnifiedModel().to(device)\n",
        "\n",
        "def count_parameters(model: nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_params = count_parameters(model)\n",
        "print(f\"模型總參數量：{total_params:,} (< 8M: {total_params < 8_000_000})\")\n",
        "\n",
        "def measure_inference_time(model: nn.Module, input_size: Tuple[int, int, int, int], device: torch.device, num_runs: int = 100) -> float:\n",
        "    model.eval()\n",
        "    dummy_input = torch.randn(input_size).to(device)\n",
        "    for _ in range(10):\n",
        "        model(dummy_input)\n",
        "    start_time = time.time()\n",
        "    for _ in range(num_runs):\n",
        "        model(dummy_input)\n",
        "    end_time = time.time()\n",
        "    return (end_time - start_time) / num_runs * 1000\n",
        "\n",
        "inference_time = measure_inference_time(model, (1, 3, 512, 512), device)\n",
        "print(f\"單張 512x512 圖像推理時間：{inference_time:.2f} ms (< 150 ms: {inference_time < 150})\")\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity: int = 50):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "\n",
        "    def add(self, data: Tuple[torch.Tensor, any]):\n",
        "        if len(self.buffer) >= self.capacity:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append(data)\n",
        "\n",
        "    def sample(self) -> List[Tuple[torch.Tensor, any]]:\n",
        "        return self.buffer\n",
        "\n",
        "replay_buffers = {task: ReplayBuffer(capacity=50) for task in ['seg', 'det', 'cls']}\n",
        "\n",
        "def compute_seg_class_weights(loader: DataLoader, num_classes: int = 20) -> torch.Tensor:\n",
        "    \"\"\"計算 seg 任務的類別權重\"\"\"\n",
        "    class_counts = torch.zeros(num_classes).to(device)\n",
        "    total_pixels = 0\n",
        "    for _, targets in loader:\n",
        "        targets = targets.to(device)\n",
        "        for c in range(num_classes):\n",
        "            class_counts[c] += (targets == c).sum().float()\n",
        "        total_pixels += targets.numel()\n",
        "    weights = total_pixels / (num_classes * class_counts + 1e-6)\n",
        "    weights = weights / weights.sum() * num_classes  # 正規化\n",
        "    print(f\"Seg 任務類別權重：{weights.tolist()}\")\n",
        "    return weights\n",
        "\n",
        "seg_class_weights = compute_seg_class_weights(train_loaders['seg'])\n",
        "\n",
        "def compute_losses(outputs: Tuple[torch.Tensor, torch.Tensor, torch.Tensor], targets: any, task: str) -> torch.Tensor:\n",
        "    det_out, seg_out, cls_out = outputs\n",
        "    if task == 'det':\n",
        "        if not isinstance(targets, list):\n",
        "            return torch.tensor(0.).to(device)\n",
        "        boxes_pred = det_out.permute(0, 2, 3, 1)\n",
        "        loss = 0\n",
        "        for i in range(len(targets)):\n",
        "            if not isinstance(targets[i], dict) or 'boxes' not in targets[i]:\n",
        "                continue\n",
        "            target_boxes = targets[i]['boxes'].to(device)\n",
        "            if len(target_boxes) == 0:\n",
        "                continue\n",
        "            pred_box = boxes_pred[i, 0, 0, :4]\n",
        "            target_box = target_boxes[0]\n",
        "            iou = calculate_iou(pred_box, target_box)\n",
        "            loss += 1 - iou if iou > 0 else 1\n",
        "        return loss / len(targets) if len(targets) > 0 else torch.tensor(0.).to(device)\n",
        "    elif task == 'seg':\n",
        "        if not isinstance(targets, torch.Tensor):\n",
        "            return torch.tensor(0.).to(device)\n",
        "        batch_size, num_classes, height, width = seg_out.size()\n",
        "        seg_out = seg_out.permute(0, 2, 3, 1).contiguous().view(-1, num_classes)\n",
        "        targets = targets.to(device).view(-1)\n",
        "        # 診斷：檢查 targets 的值範圍\n",
        "        if targets.max() >= num_classes or targets.min() < 0:\n",
        "            print(f\"警告：seg 任務 targets 值範圍異常！最大值：{targets.max()}, 最小值：{targets.min()}\")\n",
        "            return torch.tensor(0.).to(device)\n",
        "        # 診斷：檢查預測分佈\n",
        "        preds = seg_out.argmax(dim=1)\n",
        "        unique_preds = torch.unique(preds)\n",
        "        print(f\"Seg 預測類別分佈：{unique_preds.tolist()}\")\n",
        "        criterion = nn.CrossEntropyLoss(weight=seg_class_weights)\n",
        "        return criterion(seg_out, targets)\n",
        "    elif task == 'cls':\n",
        "        if not isinstance(targets, torch.Tensor):\n",
        "            return torch.tensor(0.).to(device)\n",
        "        targets = targets.to(device)\n",
        "        return nn.CrossEntropyLoss()(cls_out, targets)\n",
        "    return torch.tensor(0.).to(device)\n",
        "\n",
        "def calculate_iou(box1: torch.Tensor, box2: torch.Tensor) -> float:\n",
        "    box1 = box1.cpu()\n",
        "    box2 = box2.cpu()\n",
        "    x1, y1, w1, h1 = box1\n",
        "    x2, y2, w2, h2 = box2\n",
        "    x_left = max(x1 - w1/2, x2 - w2/2)\n",
        "    y_top = max(y1 - h1/2, y2 - h2/2)\n",
        "    x_right = min(x1 + w1/2, x2 + w2/2)\n",
        "    y_bottom = min(y1 + h1/2, y2 + h2/2)\n",
        "    if x_right < x_left or y_bottom < y_top:\n",
        "        return 0.0\n",
        "    intersection = (x_right - x_left) * (y_bottom - y_top)\n",
        "    union = w1 * h1 + w2 * h2 - intersection\n",
        "    return intersection / union if union > 0 else 0.0\n",
        "\n",
        "def evaluate(model: nn.Module, loader: DataLoader, task: str) -> Dict[str, float]:\n",
        "    model.eval()\n",
        "    metrics = {'mIoU': 0.0, 'mAP': 0.0, 'Top-1': 0.0}\n",
        "    if task == 'seg':\n",
        "        num_classes = 20\n",
        "        intersection = torch.zeros(num_classes).to(device)\n",
        "        union = torch.zeros(num_classes).to(device)\n",
        "        total_batches = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in loader:\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "                _, seg_out, _ = model(inputs)\n",
        "                preds = seg_out.argmax(dim=1)\n",
        "                targets = targets\n",
        "                for c in range(num_classes):\n",
        "                    pred_mask = (preds == c)\n",
        "                    target_mask = (targets == c)\n",
        "                    intersection[c] += (pred_mask & target_mask).sum().float()\n",
        "                    union[c] += (pred_mask | target_mask).sum().float()\n",
        "                total_batches += 1\n",
        "        if total_batches > 0:\n",
        "            iou = intersection / (union + 1e-6)\n",
        "            metrics['mIoU'] = float(iou.mean().item())\n",
        "    elif task == 'det':\n",
        "        aps = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in loader:\n",
        "                inputs = inputs.to(device)\n",
        "                det_out, _, _ = model(inputs)\n",
        "                boxes_pred = det_out.permute(0, 2, 3, 1)\n",
        "                for i in range(len(targets)):\n",
        "                    if not isinstance(targets[i], dict) or 'boxes' not in targets[i]:\n",
        "                        continue\n",
        "                    target_boxes = targets[i]['boxes'].to(device)\n",
        "                    if len(target_boxes) == 0:\n",
        "                        continue\n",
        "                    pred_boxes = boxes_pred[i].view(-1, 6)[:, :4]\n",
        "                    ious = [calculate_iou(pred_box, target_box) for pred_box in pred_boxes for target_box in target_boxes]\n",
        "                    ap = max(ious) if ious else 0.0\n",
        "                    aps.append(ap)\n",
        "        metrics['mAP'] = float(np.mean(aps)) if aps else 0.0\n",
        "    elif task == 'cls':\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in loader:\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "                _, _, cls_out = model(inputs)\n",
        "                preds = cls_out.argmax(dim=1)\n",
        "                total_correct += (preds == targets).sum().item()\n",
        "                total_samples += targets.size(0)\n",
        "        metrics['Top-1'] = total_correct / total_samples if total_samples > 0 else 0.0\n",
        "    return metrics\n",
        "\n",
        "def train_stage(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, task: str, epochs: int, optimizer: optim.Optimizer,\n",
        "                replay_buffers: Dict[str, ReplayBuffer], tasks: List[str], stage: int) -> Tuple[List[float], List[float], Dict[str, float]]:\n",
        "    train_losses = []\n",
        "    val_metrics = []\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            if task != 'det':\n",
        "                targets = targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            det_out, seg_out, cls_out = model(inputs)\n",
        "            loss = compute_losses((det_out, seg_out, cls_out), targets, task)\n",
        "            if task == 'seg' and batch_idx == 0:\n",
        "                print(f\"Epoch {epoch + 1}, Batch 1, Seg 損失：{loss.item()}\")\n",
        "            epoch_loss += loss.item()\n",
        "            detached_inputs = inputs.detach().cpu()\n",
        "            detached_targets = copy.deepcopy(targets) if task == 'det' else targets.detach().cpu()\n",
        "            replay_buffers[task].add((detached_inputs, detached_targets))\n",
        "            replay_loss = 0.0\n",
        "            replay_batch_count = 0\n",
        "            for prev_task in tasks[:stage]:\n",
        "                buffer = replay_buffers[prev_task].sample()\n",
        "                for b_inputs, b_targets in buffer:\n",
        "                    b_inputs = b_inputs.to(device)\n",
        "                    b_det_out, b_seg_out, b_cls_out = model(b_inputs)\n",
        "                    replay_loss += compute_losses((b_det_out, b_seg_out, b_cls_out), b_targets, prev_task)\n",
        "                    replay_batch_count += 1\n",
        "            if stage > 0 and replay_loss > 0 and replay_batch_count > 0:\n",
        "                replay_loss /= replay_batch_count\n",
        "                loss += replay_loss\n",
        "            if isinstance(loss, torch.Tensor) and loss.requires_grad:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        train_losses.append(avg_loss)\n",
        "        print(f\"第 {epoch + 1} 個 epoch, {task} 平均損失: {avg_loss:.4f}\")\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            metrics = evaluate(model, val_loader, task)\n",
        "            val_metrics.append(metrics)\n",
        "            print(f\"階段 {stage + 1} ({task}) 驗證指標：{metrics}\")\n",
        "    final_metrics = evaluate(model, val_loader, task)\n",
        "    return train_losses, val_metrics, final_metrics\n",
        "\n",
        "def plot_curves(task_metrics: Dict[str, Tuple[List[float], List[Dict[str, float]]]]):\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    for task, (train_losses, _, _) in task_metrics.items():\n",
        "        plt.plot(train_losses, label=f'{task} Loss')\n",
        "    plt.title('訓練損失曲線')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('損失')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 3, 2)\n",
        "    seg_val_metrics = task_metrics['seg'][1]\n",
        "    seg_miou = [m['mIoU'] for m in seg_val_metrics]\n",
        "    epochs = [5 * (i + 1) for i in range(len(seg_miou))]\n",
        "    plt.plot(epochs, seg_miou, label='Seg mIoU')\n",
        "    plt.title('分割 mIoU 曲線')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('mIoU')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 3, 3)\n",
        "    cls_val_metrics = task_metrics['cls'][1]\n",
        "    cls_top1 = [m['Top-1'] for m in cls_val_metrics]\n",
        "    epochs = [5 * (i + 1) for i in range(len(cls_top1))]\n",
        "    plt.plot(epochs, cls_top1, label='Cls Top-1')\n",
        "    plt.title('分類 Top-1 曲線')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Top-1')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # 降低學習率\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
        "\n",
        "tasks = ['seg', 'det', 'cls']\n",
        "baselines = {'mIoU': 0.0, 'mAP': 0.0, 'Top-1': 0.0}\n",
        "task_metrics = {}\n",
        "\n",
        "epochs_per_stage = 150\n",
        "total_training_time = 0\n",
        "\n",
        "for stage, task in enumerate(tasks):\n",
        "    print(f\"\\n訓練階段 {stage + 1}: {task}\")\n",
        "    start_time = time.time()\n",
        "    train_losses, val_metrics, final_metrics = train_stage(model, train_loaders[task], val_loaders[task], task,\n",
        "                                                           epochs_per_stage, optimizer, replay_buffers, tasks, stage)\n",
        "    stage_time = time.time() - start_time\n",
        "    total_training_time += stage_time\n",
        "    print(f\"階段 {stage + 1} 完成，耗時 {stage_time:.2f} 秒\")\n",
        "    task_metrics[task] = (train_losses, val_metrics, final_metrics)\n",
        "    if stage == 0:\n",
        "        baselines['mIoU'] = final_metrics.get('mIoU', 0.0)\n",
        "    elif stage == 1:\n",
        "        baselines['mAP'] = final_metrics.get('mAP', 0.0)\n",
        "    elif stage == 2:\n",
        "        baselines['Top-1'] = final_metrics.get('Top-1', 0.0)\n",
        "    scheduler.step()\n",
        "\n",
        "print(f\"\\n總訓練時間：{total_training_time:.2f} 秒 (< 2 小時：{total_training_time < 7200})\")\n",
        "\n",
        "print(\"\\n最終評估：\")\n",
        "final_metrics = {}\n",
        "for task in tasks:\n",
        "    metrics = evaluate(model, val_loaders[task], task)\n",
        "    final_metrics[task] = metrics\n",
        "    print(f\"最終 {task} 評估：{metrics}\")\n",
        "\n",
        "print(\"\\n性能下降（相較於各任務基準）：\")\n",
        "for task in tasks:\n",
        "    if task == 'seg':\n",
        "        baseline = baselines['mIoU']\n",
        "        final = final_metrics['seg']['mIoU']\n",
        "        metric_name = 'mIoU'\n",
        "    elif task == 'det':\n",
        "        baseline = baselines['mAP']\n",
        "        final = final_metrics['det']['mAP']\n",
        "        metric_name = 'mAP'\n",
        "    elif task == 'cls':\n",
        "        baseline = baselines['Top-1']\n",
        "        final = final_metrics['cls']['Top-1']\n",
        "        metric_name = 'Top-1'\n",
        "    if baseline > 0:\n",
        "        drop = (baseline - final) / baseline * 100\n",
        "        print(f\"{task} {metric_name} 下降：{drop:.2f}% (< 5%：{drop < 5})\")\n",
        "    else:\n",
        "        print(f\"{task} {metric_name}：基準為 0，無法計算下降。\")\n",
        "\n",
        "plot_curves(task_metrics)\n",
        "torch.save(model.state_dict(), 'your_model.pt')\n",
        "print(\"模型已儲存為 'your_model.pt'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "jKFnTOKdvjoQ",
        "outputId": "86cafc9f-34e2-4bc0-d3e3-63799556a815"
      },
      "id": "jKFnTOKdvjoQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用設備：cuda\n",
            "Seg 數據集掩碼值分佈：[np.uint8(0), np.uint8(15), np.uint8(19), np.uint8(34), np.uint8(38), np.uint8(52), np.uint8(53), np.uint8(57), np.uint8(72), np.uint8(75), np.uint8(76), np.uint8(90), np.uint8(94), np.uint8(109), np.uint8(113), np.uint8(128), np.uint8(133), np.uint8(147), np.uint8(151), np.uint8(220)]\n",
            "Seg 數據集掩碼值分佈：[np.uint8(0), np.uint8(15), np.uint8(19), np.uint8(34), np.uint8(38), np.uint8(52), np.uint8(53), np.uint8(57), np.uint8(72), np.uint8(75), np.uint8(76), np.uint8(90), np.uint8(94), np.uint8(109), np.uint8(113), np.uint8(128), np.uint8(133), np.uint8(147), np.uint8(151), np.uint8(220)]\n",
            "模型總參數量：5,077,828 (< 8M: True)\n",
            "單張 512x512 圖像推理時間：10.40 ms (< 150 ms: True)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "掩碼值超過 19：220，請檢查數據！",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-85fcca7c29da>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m \u001b[0mseg_class_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_seg_class_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-85fcca7c29da>\u001b[0m in \u001b[0;36mcompute_seg_class_weights\u001b[0;34m(loader, num_classes)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0mclass_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0mtotal_pixels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-85fcca7c29da>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;31m# 修正映射：假設原始值已為類別索引（0-19），不再進行線性縮放\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmask_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"掩碼值超過 19：{mask_array.max()}，請檢查數據！\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 掩碼值超過 19：220，請檢查數據！"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 統一單頭多任務挑戰實作 (第五版優化)\n",
        "# 安裝所需庫\n",
        "!pip install torch torchvision torchaudio -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import copy\n",
        "import time\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"使用設備：{device}\")\n",
        "\n",
        "class MultiTaskDataset(Dataset):\n",
        "    def __init__(self, data_dir: str, task: str, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.task = task\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.annotations = []\n",
        "        if task == 'det':\n",
        "            labels_path = os.path.join(data_dir, 'labels.json')\n",
        "            if not os.path.exists(labels_path):\n",
        "                raise FileNotFoundError(f\"找不到 {labels_path}，請確認檔案是否存在！\")\n",
        "            with open(labels_path, 'r') as f:\n",
        "                labels_data = json.load(f)\n",
        "            image_dir = os.path.join(data_dir, 'data')\n",
        "            image_files = sorted([img for img in os.listdir(image_dir) if img.endswith(('.jpg', '.jpeg', '.JPEG'))])\n",
        "            image_file_set = set(image_files)\n",
        "            valid_images = {img['id']: img['file_name'] for img in labels_data['images'] if img['file_name'] in image_file_set}\n",
        "            ann_dict = {}\n",
        "            for ann in labels_data['annotations']:\n",
        "                img_id = ann['image_id']\n",
        "                if img_id in valid_images:\n",
        "                    if img_id not in ann_dict:\n",
        "                        ann_dict[img_id] = []\n",
        "                    ann_dict[img_id].append({'boxes': ann['bbox'], 'labels': ann['category_id']})\n",
        "            for img_id, file_name in valid_images.items():\n",
        "                full_path = os.path.join(image_dir, file_name)\n",
        "                if img_id in ann_dict:\n",
        "                    self.images.append(full_path)\n",
        "                    self.annotations.append(ann_dict[img_id])\n",
        "        elif task == 'seg':\n",
        "            image_files = sorted([img for img in os.listdir(data_dir) if img.endswith(('.jpg', '.jpeg', '.JPEG'))])\n",
        "            for img in image_files:\n",
        "                img_path = os.path.join(data_dir, img)\n",
        "                mask_path = os.path.join(data_dir, img.replace('.jpg', '.png').replace('.jpeg', '.png').replace('.JPEG', '.png'))\n",
        "                if os.path.exists(mask_path):\n",
        "                    self.images.append(img_path)\n",
        "                    self.annotations.append(mask_path)\n",
        "            # 檢查掩碼值分佈\n",
        "            if task == 'seg':\n",
        "                self._check_mask_distribution()\n",
        "        elif task == 'cls':\n",
        "            label_dirs = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
        "            label_to_index = {label: idx for idx, label in enumerate(label_dirs)}\n",
        "            for label in label_dirs:\n",
        "                label_path = os.path.join(data_dir, label)\n",
        "                for root, _, files in os.walk(label_path):\n",
        "                    for img in files:\n",
        "                        if img.endswith(('.jpg', '.jpeg', '.JPEG')):\n",
        "                            img_path = os.path.join(root, img)\n",
        "                            self.images.append(img_path)\n",
        "                            self.annotations.append(label_to_index[label])\n",
        "        if len(self.images) == 0:\n",
        "            raise ValueError(f\"在 {data_dir} 中未找到任何資料，請檢查資料結構！\")\n",
        "\n",
        "    def _check_mask_distribution(self):\n",
        "        \"\"\"檢查 seg 任務掩碼值分佈\"\"\"\n",
        "        unique_values = set()\n",
        "        for mask_path in self.annotations:\n",
        "            mask = Image.open(mask_path).convert('L')\n",
        "            mask_array = np.array(mask)\n",
        "            unique_values.update(np.unique(mask_array))\n",
        "        print(f\"Seg 數據集掩碼值分佈：{sorted(unique_values)}\")\n",
        "        if max(unique_values) > 255 or min(unique_values) < 0:\n",
        "            raise ValueError(\"掩碼值超出範圍 [0, 255]！\")\n",
        "        if len(unique_values) > 20:\n",
        "            print(\"警告：掩碼值種類超過 20，可能需要重新映射！\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, any]:\n",
        "        img_path = self.images[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        if self.task == 'seg':\n",
        "            mask = Image.open(self.annotations[idx]).convert('L')\n",
        "            mask_array = np.array(mask)\n",
        "            # 映射 0-220 到 0-19，假設均勻分佈\n",
        "            mask_array = np.clip(mask_array, 0, 220)  # 限制範圍\n",
        "            mask_array = (mask_array / 220 * 19).astype(np.uint8)  # 線性映射到 0-19\n",
        "            mask = Image.fromarray(mask_array)\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "                mask_transform = transforms.Compose([\n",
        "                    transforms.Resize((16, 16), interpolation=Image.Resampling.NEAREST),\n",
        "                    transforms.ToTensor()\n",
        "                ])\n",
        "                mask = mask_transform(mask)\n",
        "            return img, mask.squeeze(0).long()\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        if self.task == 'det':\n",
        "            ann = self.annotations[idx]\n",
        "            boxes = torch.tensor([a['boxes'] for a in ann], dtype=torch.float32)\n",
        "            labels = torch.tensor([a['labels'] for a in ann], dtype=torch.long)\n",
        "            return img, {'boxes': boxes, 'labels': labels}\n",
        "        elif self.task == 'cls':\n",
        "            return img, torch.tensor(self.annotations[idx], dtype=torch.long)\n",
        "\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((512, 512), interpolation=Image.Resampling.BILINEAR),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_datasets = {\n",
        "    'det': MultiTaskDataset('data/mini_coco_det/train', 'det', image_transform),\n",
        "    'seg': MultiTaskDataset('data/mini_voc_seg/train', 'seg', image_transform),\n",
        "    'cls': MultiTaskDataset('data/imagenette_160/train', 'cls', image_transform)\n",
        "}\n",
        "val_datasets = {\n",
        "    'det': MultiTaskDataset('data/mini_coco_det/val', 'det', image_transform),\n",
        "    'seg': MultiTaskDataset('data/mini_voc_seg/val', 'seg', image_transform),\n",
        "    'cls': MultiTaskDataset('data/imagenette_160/val', 'cls', image_transform)\n",
        "}\n",
        "\n",
        "def custom_collate(batch: List[Tuple[torch.Tensor, any]]) -> Tuple[torch.Tensor, List[any]]:\n",
        "    images = torch.stack([item[0] for item in batch])\n",
        "    targets = [item[1] for item in batch]\n",
        "    return images, targets\n",
        "\n",
        "train_loaders = {task: DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=custom_collate if task == 'det' else None) for task, dataset in train_datasets.items()}\n",
        "val_loaders = {task: DataLoader(dataset, batch_size=4, shuffle=False, collate_fn=custom_collate if task == 'det' else None) for task, dataset in val_datasets.items()}\n",
        "\n",
        "class MultiTaskHead(nn.Module):\n",
        "    def __init__(self, in_channels: int = 576):\n",
        "        super(MultiTaskHead, self).__init__()\n",
        "        self.neck = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=1)\n",
        "        )\n",
        "        self.det_head = nn.Conv2d(128, 6, kernel_size=1)\n",
        "        self.seg_head = nn.Conv2d(128, 20, kernel_size=1)\n",
        "        self.cls_head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        x = self.neck(x)\n",
        "        x = self.head(x)\n",
        "        det_out = self.det_head(x)\n",
        "        seg_out = self.seg_head(x)\n",
        "        cls_out = self.cls_head(x)\n",
        "        return det_out, seg_out, cls_out\n",
        "\n",
        "class UnifiedModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UnifiedModel, self).__init__()\n",
        "        self.backbone = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1).features\n",
        "        self.head = MultiTaskHead(in_channels=576)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        features = self.backbone(x)\n",
        "        det_out, seg_out, cls_out = self.head(features)\n",
        "        return det_out, seg_out, cls_out\n",
        "\n",
        "model = UnifiedModel().to(device)\n",
        "\n",
        "def count_parameters(model: nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_params = count_parameters(model)\n",
        "print(f\"模型總參數量：{total_params:,} (< 8M: {total_params < 8_000_000})\")\n",
        "\n",
        "def measure_inference_time(model: nn.Module, input_size: Tuple[int, int, int, int], device: torch.device, num_runs: int = 100) -> float:\n",
        "    model.eval()\n",
        "    dummy_input = torch.randn(input_size).to(device)\n",
        "    for _ in range(10):\n",
        "        model(dummy_input)\n",
        "    start_time = time.time()\n",
        "    for _ in range(num_runs):\n",
        "        model(dummy_input)\n",
        "    end_time = time.time()\n",
        "    return (end_time - start_time) / num_runs * 1000\n",
        "\n",
        "inference_time = measure_inference_time(model, (1, 3, 512, 512), device)\n",
        "print(f\"單張 512x512 圖像推理時間：{inference_time:.2f} ms (< 150 ms: {inference_time < 150})\")\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity: int = 50):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "\n",
        "    def add(self, data: Tuple[torch.Tensor, any]):\n",
        "        if len(self.buffer) >= self.capacity:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append(data)\n",
        "\n",
        "    def sample(self) -> List[Tuple[torch.Tensor, any]]:\n",
        "        return self.buffer\n",
        "\n",
        "replay_buffers = {task: ReplayBuffer(capacity=50) for task in ['seg', 'det', 'cls']}\n",
        "\n",
        "def compute_seg_class_weights(loader: DataLoader, num_classes: int = 20) -> torch.Tensor:\n",
        "    \"\"\"計算 seg 任務的類別權重\"\"\"\n",
        "    class_counts = torch.zeros(num_classes).to(device)\n",
        "    total_pixels = 0\n",
        "    for _, targets in loader:\n",
        "        targets = targets.to(device)\n",
        "        for c in range(num_classes):\n",
        "            class_counts[c] += (targets == c).sum().float()\n",
        "        total_pixels += targets.numel()\n",
        "    weights = total_pixels / (num_classes * class_counts + 1e-6)\n",
        "    weights = weights / weights.sum() * num_classes\n",
        "    print(f\"Seg 任務類別權重：{weights.tolist()}\")\n",
        "    return weights\n",
        "\n",
        "seg_class_weights = compute_seg_class_weights(train_loaders['seg'])\n",
        "\n",
        "def compute_losses(outputs: Tuple[torch.Tensor, torch.Tensor, torch.Tensor], targets: any, task: str) -> torch.Tensor:\n",
        "    det_out, seg_out, cls_out = outputs\n",
        "    if task == 'det':\n",
        "        if not isinstance(targets, list):\n",
        "            return torch.tensor(0.).to(device)\n",
        "        boxes_pred = det_out.permute(0, 2, 3, 1)\n",
        "        loss = 0\n",
        "        for i in range(len(targets)):\n",
        "            if not isinstance(targets[i], dict) or 'boxes' not in targets[i]:\n",
        "                continue\n",
        "            target_boxes = targets[i]['boxes'].to(device)\n",
        "            if len(target_boxes) == 0:\n",
        "                continue\n",
        "            pred_box = boxes_pred[i, 0, 0, :4]\n",
        "            target_box = target_boxes[0]\n",
        "            iou = calculate_iou(pred_box, target_box)\n",
        "            loss += 1 - iou if iou > 0 else 1\n",
        "        return loss / len(targets) if len(targets) > 0 else torch.tensor(0.).to(device)\n",
        "    elif task == 'seg':\n",
        "        if not isinstance(targets, torch.Tensor):\n",
        "            return torch.tensor(0.).to(device)\n",
        "        batch_size, num_classes, height, width = seg_out.size()\n",
        "        seg_out = seg_out.permute(0, 2, 3, 1).contiguous().view(-1, num_classes)\n",
        "        targets = targets.to(device).view(-1)\n",
        "        if targets.max() >= num_classes or targets.min() < 0:\n",
        "            print(f\"警告：seg 任務 targets 值範圍異常！最大值：{targets.max()}, 最小值：{targets.min()}\")\n",
        "            return torch.tensor(0.).to(device)\n",
        "        preds = seg_out.argmax(dim=1)\n",
        "        unique_preds = torch.unique(preds)\n",
        "        print(f\"Seg 預測類別分佈：{unique_preds.tolist()}\")\n",
        "        criterion = nn.CrossEntropyLoss(weight=seg_class_weights)\n",
        "        return criterion(seg_out, targets)\n",
        "    elif task == 'cls':\n",
        "        if not isinstance(targets, torch.Tensor):\n",
        "            return torch.tensor(0.).to(device)\n",
        "        targets = targets.to(device)\n",
        "        return nn.CrossEntropyLoss()(cls_out, targets)\n",
        "    return torch.tensor(0.).to(device)\n",
        "\n",
        "def calculate_iou(box1: torch.Tensor, box2: torch.Tensor) -> float:\n",
        "    box1 = box1.cpu()\n",
        "    box2 = box2.cpu()\n",
        "    x1, y1, w1, h1 = box1\n",
        "    x2, y2, w2, h2 = box2\n",
        "    x_left = max(x1 - w1/2, x2 - w2/2)\n",
        "    y_top = max(y1 - h1/2, y2 - h2/2)\n",
        "    x_right = min(x1 + w1/2, x2 + w2/2)\n",
        "    y_bottom = min(y1 + h1/2, y2 + h2/2)\n",
        "    if x_right < x_left or y_bottom < y_top:\n",
        "        return 0.0\n",
        "    intersection = (x_right - x_left) * (y_bottom - y_top)\n",
        "    union = w1 * h1 + w2 * h2 - intersection\n",
        "    return intersection / union if union > 0 else 0.0\n",
        "\n",
        "def evaluate(model: nn.Module, loader: DataLoader, task: str) -> Dict[str, float]:\n",
        "    model.eval()\n",
        "    metrics = {'mIoU': 0.0, 'mAP': 0.0, 'Top-1': 0.0}\n",
        "    if task == 'seg':\n",
        "        num_classes = 20\n",
        "        intersection = torch.zeros(num_classes).to(device)\n",
        "        union = torch.zeros(num_classes).to(device)\n",
        "        total_batches = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in loader:\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "                _, seg_out, _ = model(inputs)\n",
        "                preds = seg_out.argmax(dim=1)\n",
        "                targets = targets\n",
        "                for c in range(num_classes):\n",
        "                    pred_mask = (preds == c)\n",
        "                    target_mask = (targets == c)\n",
        "                    intersection[c] += (pred_mask & target_mask).sum().float()\n",
        "                    union[c] += (pred_mask | target_mask).sum().float()\n",
        "                total_batches += 1\n",
        "        if total_batches > 0:\n",
        "            iou = intersection / (union + 1e-6)\n",
        "            metrics['mIoU'] = float(iou.mean().item())\n",
        "    elif task == 'det':\n",
        "        aps = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in loader:\n",
        "                inputs = inputs.to(device)\n",
        "                det_out, _, _ = model(inputs)\n",
        "                boxes_pred = det_out.permute(0, 2, 3, 1)\n",
        "                for i in range(len(targets)):\n",
        "                    if not isinstance(targets[i], dict) or 'boxes' not in targets[i]:\n",
        "                        continue\n",
        "                    target_boxes = targets[i]['boxes'].to(device)\n",
        "                    if len(target_boxes) == 0:\n",
        "                        continue\n",
        "                    pred_boxes = boxes_pred[i].view(-1, 6)[:, :4]\n",
        "                    ious = [calculate_iou(pred_box, target_box) for pred_box in pred_boxes for target_box in target_boxes]\n",
        "                    ap = max(ious) if ious else 0.0\n",
        "                    aps.append(ap)\n",
        "        metrics['mAP'] = float(np.mean(aps)) if aps else 0.0\n",
        "    elif task == 'cls':\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in loader:\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "                _, _, cls_out = model(inputs)\n",
        "                preds = cls_out.argmax(dim=1)\n",
        "                total_correct += (preds == targets).sum().item()\n",
        "                total_samples += targets.size(0)\n",
        "        metrics['Top-1'] = total_correct / total_samples if total_samples > 0 else 0.0\n",
        "    return metrics\n",
        "\n",
        "def train_stage(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, task: str, epochs: int, optimizer: optim.Optimizer,\n",
        "                replay_buffers: Dict[str, ReplayBuffer], tasks: List[str], stage: int) -> Tuple[List[float], List[float], Dict[str, float]]:\n",
        "    train_losses = []\n",
        "    val_metrics = []\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            if task != 'det':\n",
        "                targets = targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            det_out, seg_out, cls_out = model(inputs)\n",
        "            loss = compute_losses((det_out, seg_out, cls_out), targets, task)\n",
        "            if task == 'seg' and batch_idx == 0:\n",
        "                print(f\"Epoch {epoch + 1}, Batch 1, Seg 損失：{loss.item()}\")\n",
        "            epoch_loss += loss.item()\n",
        "            detached_inputs = inputs.detach().cpu()\n",
        "            detached_targets = copy.deepcopy(targets) if task == 'det' else targets.detach().cpu()\n",
        "            replay_buffers[task].add((detached_inputs, detached_targets))\n",
        "            replay_loss = 0.0\n",
        "            replay_batch_count = 0\n",
        "            for prev_task in tasks[:stage]:\n",
        "                buffer = replay_buffers[prev_task].sample()\n",
        "                for b_inputs, b_targets in buffer:\n",
        "                    b_inputs = b_inputs.to(device)\n",
        "                    b_det_out, b_seg_out, b_cls_out = model(b_inputs)\n",
        "                    replay_loss += compute_losses((b_det_out, b_seg_out, b_cls_out), b_targets, prev_task)\n",
        "                    replay_batch_count += 1\n",
        "            if stage > 0 and replay_loss > 0 and replay_batch_count > 0:\n",
        "                replay_loss /= replay_batch_count\n",
        "                loss += replay_loss\n",
        "            if isinstance(loss, torch.Tensor) and loss.requires_grad:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        train_losses.append(avg_loss)\n",
        "        print(f\"第 {epoch + 1} 個 epoch, {task} 平均損失: {avg_loss:.4f}\")\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            metrics = evaluate(model, val_loader, task)\n",
        "            val_metrics.append(metrics)\n",
        "            print(f\"階段 {stage + 1} ({task}) 驗證指標：{metrics}\")\n",
        "    final_metrics = evaluate(model, val_loader, task)\n",
        "    return train_losses, val_metrics, final_metrics\n",
        "\n",
        "def plot_curves(task_metrics: Dict[str, Tuple[List[float], List[Dict[str, float]]]]):\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    for task, (train_losses, _, _) in task_metrics.items():\n",
        "        plt.plot(train_losses, label=f'{task} Loss')\n",
        "    plt.title('訓練損失曲線')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('損失')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 3, 2)\n",
        "    seg_val_metrics = task_metrics['seg'][1]\n",
        "    seg_miou = [m['mIoU'] for m in seg_val_metrics]\n",
        "    epochs = [5 * (i + 1) for i in range(len(seg_miou))]\n",
        "    plt.plot(epochs, seg_miou, label='Seg mIoU')\n",
        "    plt.title('分割 mIoU 曲線')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('mIoU')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 3, 3)\n",
        "    cls_val_metrics = task_metrics['cls'][1]\n",
        "    cls_top1 = [m['Top-1'] for m in cls_val_metrics]\n",
        "    epochs = [5 * (i + 1) for i in range(len(cls_top1))]\n",
        "    plt.plot(epochs, cls_top1, label='Cls Top-1')\n",
        "    plt.title('分類 Top-1 曲線')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Top-1')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
        "\n",
        "tasks = ['seg', 'det', 'cls']\n",
        "baselines = {'mIoU': 0.0, 'mAP': 0.0, 'Top-1': 0.0}\n",
        "task_metrics = {}\n",
        "\n",
        "epochs_per_stage = 150\n",
        "total_training_time = 0\n",
        "\n",
        "for stage, task in enumerate(tasks):\n",
        "    print(f\"\\n訓練階段 {stage + 1}: {task}\")\n",
        "    start_time = time.time()\n",
        "    train_losses, val_metrics, final_metrics = train_stage(model, train_loaders[task], val_loaders[task], task,\n",
        "                                                           epochs_per_stage, optimizer, replay_buffers, tasks, stage)\n",
        "    stage_time = time.time() - start_time\n",
        "    total_training_time += stage_time\n",
        "    print(f\"階段 {stage + 1} 完成，耗時 {stage_time:.2f} 秒\")\n",
        "    task_metrics[task] = (train_losses, val_metrics, final_metrics)\n",
        "    if stage == 0:\n",
        "        baselines['mIoU'] = final_metrics.get('mIoU', 0.0)\n",
        "    elif stage == 1:\n",
        "        baselines['mAP'] = final_metrics.get('mAP', 0.0)\n",
        "    elif stage == 2:\n",
        "        baselines['Top-1'] = final_metrics.get('Top-1', 0.0)\n",
        "    scheduler.step()\n",
        "\n",
        "print(f\"\\n總訓練時間：{total_training_time:.2f} 秒 (< 2 小時：{total_training_time < 7200})\")\n",
        "\n",
        "print(\"\\n最終評估：\")\n",
        "final_metrics = {}\n",
        "for task in tasks:\n",
        "    metrics = evaluate(model, val_loaders[task], task)\n",
        "    final_metrics[task] = metrics\n",
        "    print(f\"最終 {task} 評估：{metrics}\")\n",
        "\n",
        "print(\"\\n性能下降（相較於各任務基準）：\")\n",
        "for task in tasks:\n",
        "    if task == 'seg':\n",
        "        baseline = baselines['mIoU']\n",
        "        final = final_metrics['seg']['mIoU']\n",
        "        metric_name = 'mIoU'\n",
        "    elif task == 'det':\n",
        "        baseline = baselines['mAP']\n",
        "        final = final_metrics['det']['mAP']\n",
        "        metric_name = 'mAP'\n",
        "    elif task == 'cls':\n",
        "        baseline = baselines['Top-1']\n",
        "        final = final_metrics['cls']['Top-1']\n",
        "        metric_name = 'Top-1'\n",
        "    if baseline > 0:\n",
        "        drop = (baseline - final) / baseline * 100\n",
        "        print(f\"{task} {metric_name} 下降：{drop:.2f}% (< 5%：{drop < 5})\")\n",
        "    else:\n",
        "        print(f\"{task} {metric_name}：基準為 0，無法計算下降。\")\n",
        "\n",
        "plot_curves(task_metrics)\n",
        "torch.save(model.state_dict(), 'your_model.pt')\n",
        "print(\"模型已儲存為 'your_model5.pt'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LZjVtKk-wIH5",
        "outputId": "6fe6273c-5f27-42a0-834f-68c3ce76b768"
      },
      "id": "LZjVtKk-wIH5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用設備：cuda\n",
            "Seg 數據集掩碼值分佈：[np.uint8(0), np.uint8(15), np.uint8(19), np.uint8(34), np.uint8(38), np.uint8(52), np.uint8(53), np.uint8(57), np.uint8(72), np.uint8(75), np.uint8(76), np.uint8(90), np.uint8(94), np.uint8(109), np.uint8(113), np.uint8(128), np.uint8(133), np.uint8(147), np.uint8(151), np.uint8(220)]\n",
            "Seg 數據集掩碼值分佈：[np.uint8(0), np.uint8(15), np.uint8(19), np.uint8(34), np.uint8(38), np.uint8(52), np.uint8(53), np.uint8(57), np.uint8(72), np.uint8(75), np.uint8(76), np.uint8(90), np.uint8(94), np.uint8(109), np.uint8(113), np.uint8(128), np.uint8(133), np.uint8(147), np.uint8(151), np.uint8(220)]\n",
            "模型總參數量：5,077,828 (< 8M: True)\n",
            "單張 512x512 圖像推理時間：9.56 ms (< 150 ms: True)\n",
            "Seg 任務類別權重：[8.56633774332094e-13, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072]\n",
            "\n",
            "訓練階段 1: seg\n",
            "Seg 預測類別分佈：[4, 5]\n",
            "Epoch 1, Batch 1, Seg 損失：3.0435030460357666\n",
            "Seg 預測類別分佈：[0, 4, 5]\n",
            "Seg 預測類別分佈：[0, 4, 5]\n",
            "Seg 預測類別分佈：[0, 4, 5]\n",
            "Seg 預測類別分佈：[0, 4, 5]\n",
            "Seg 預測類別分佈：[0, 4, 5]\n",
            "Seg 預測類別分佈：[0, 4, 5]\n",
            "Seg 預測類別分佈：[0, 5]\n",
            "Seg 預測類別分佈：[0, 5]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "第 1 個 epoch, seg 平均損失: 0.2979\n",
            "Seg 預測類別分佈：[0]\n",
            "Epoch 2, Batch 1, Seg 損失：9.413082011633378e-07\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "第 2 個 epoch, seg 平均損失: 0.0000\n",
            "Seg 預測類別分佈：[0]\n",
            "Epoch 3, Batch 1, Seg 損失：9.547251465846784e-06\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "第 3 個 epoch, seg 平均損失: 0.0000\n",
            "Seg 預測類別分佈：[0]\n",
            "Epoch 4, Batch 1, Seg 損失：1.7201004993694369e-06\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "第 4 個 epoch, seg 平均損失: 0.0000\n",
            "Seg 預測類別分佈：[0]\n",
            "Epoch 5, Batch 1, Seg 損失：1.2178825272712857e-06\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "第 5 個 epoch, seg 平均損失: 0.0000\n",
            "階段 1 (seg) 驗證指標：{'mIoU': 0.05000000074505806, 'mAP': 0.0, 'Top-1': 0.0}\n",
            "Seg 預測類別分佈：[0]\n",
            "Epoch 6, Batch 1, Seg 損失：1.6554290596104693e-06\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "第 6 個 epoch, seg 平均損失: 0.0000\n",
            "Seg 預測類別分佈：[0]\n",
            "Epoch 7, Batch 1, Seg 損失：8.824364954307384e-07\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "第 7 個 epoch, seg 平均損失: 0.0000\n",
            "Seg 預測類別分佈：[0]\n",
            "Epoch 8, Batch 1, Seg 損失：0.0\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "第 8 個 epoch, seg 平均損失: 0.0000\n",
            "Seg 預測類別分佈：[0]\n",
            "Epoch 9, Batch 1, Seg 損失：2.410801300811727e-07\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "第 9 個 epoch, seg 平均損失: 0.0000\n",
            "Seg 預測類別分佈：[0]\n",
            "Epoch 10, Batch 1, Seg 損失：6.041864963890475e-08\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "第 10 個 epoch, seg 平均損失: 0.0000\n",
            "階段 1 (seg) 驗證指標：{'mIoU': 0.05000000074505806, 'mAP': 0.0, 'Top-1': 0.0}\n",
            "Seg 預測類別分佈：[0]\n",
            "Epoch 11, Batch 1, Seg 損失：4.1228693703487806e-07\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "第 11 個 epoch, seg 平均損失: 0.0000\n",
            "Seg 預測類別分佈：[0]\n",
            "Epoch 12, Batch 1, Seg 損失：1.4027473298483528e-06\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n",
            "Seg 預測類別分佈：[0]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-aba0695015e0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n訓練階段 {stage + 1}: {task}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m     train_losses, val_metrics, final_metrics = train_stage(model, train_loaders[task], val_loaders[task], task,\n\u001b[0m\u001b[1;32m    438\u001b[0m                                                            epochs_per_stage, optimizer, replay_buffers, tasks, stage)\n\u001b[1;32m    439\u001b[0m     \u001b[0mstage_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-aba0695015e0>\u001b[0m in \u001b[0;36mtrain_stage\u001b[0;34m(model, train_loader, val_loader, task, epochs, optimizer, replay_buffers, tasks, stage)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'det'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-aba0695015e0>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'seg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"transparency\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 統一單頭多任務挑戰實作 (第七版優化)\n",
        "# 安裝所需庫\n",
        "!pip install torch torchvision torchaudio -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import copy\n",
        "import time\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"使用設備：{device}\")\n",
        "\n",
        "class MultiTaskDataset(Dataset):\n",
        "    def __init__(self, data_dir: str, task: str, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.task = task\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.annotations = []\n",
        "        if task == 'det':\n",
        "            labels_path = os.path.join(data_dir, 'labels.json')\n",
        "            if not os.path.exists(labels_path):\n",
        "                raise FileNotFoundError(f\"找不到 {labels_path}，請確認檔案是否存在！\")\n",
        "            with open(labels_path, 'r') as f:\n",
        "                labels_data = json.load(f)\n",
        "            image_dir = os.path.join(data_dir, 'data')\n",
        "            image_files = sorted([img for img in os.listdir(image_dir) if img.endswith(('.jpg', '.jpeg', '.JPEG'))])\n",
        "            image_file_set = set(image_files)\n",
        "            valid_images = {img['id']: img['file_name'] for img in labels_data['images'] if img['file_name'] in image_file_set}\n",
        "            ann_dict = {}\n",
        "            for ann in labels_data['annotations']:\n",
        "                img_id = ann['image_id']\n",
        "                if img_id in valid_images:\n",
        "                    if img_id not in ann_dict:\n",
        "                        ann_dict[img_id] = []\n",
        "                    ann_dict[img_id].append({'boxes': ann['bbox'], 'labels': ann['category_id']})\n",
        "            for img_id, file_name in valid_images.items():\n",
        "                full_path = os.path.join(image_dir, file_name)\n",
        "                if img_id in ann_dict:\n",
        "                    self.images.append(full_path)\n",
        "                    self.annotations.append(ann_dict[img_id])\n",
        "        elif task == 'seg':\n",
        "            image_files = sorted([img for img in os.listdir(data_dir) if img.endswith(('.jpg', '.jpeg', '.JPEG'))])\n",
        "            for img in image_files:\n",
        "                img_path = os.path.join(data_dir, img)\n",
        "                mask_path = os.path.join(data_dir, img.replace('.jpg', '.png').replace('.jpeg', '.png').replace('.JPEG', '.png'))\n",
        "                if os.path.exists(mask_path):\n",
        "                    self.images.append(img_path)\n",
        "                    self.annotations.append(mask_path)\n",
        "            if task == 'seg':\n",
        "                self._check_mask_distribution()\n",
        "        elif task == 'cls':\n",
        "            label_dirs = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
        "            label_to_index = {label: idx for idx, label in enumerate(label_dirs)}\n",
        "            for label in label_dirs:\n",
        "                label_path = os.path.join(data_dir, label)\n",
        "                for root, _, files in os.walk(label_path):\n",
        "                    for img in files:\n",
        "                        if img.endswith(('.jpg', '.jpeg', '.JPEG')):\n",
        "                            img_path = os.path.join(root, img)\n",
        "                            self.images.append(img_path)\n",
        "                            self.annotations.append(label_to_index[label])\n",
        "        if len(self.images) == 0:\n",
        "            raise ValueError(f\"在 {data_dir} 中未找到任何資料，請檢查資料結構！\")\n",
        "\n",
        "    def _check_mask_distribution(self):\n",
        "        unique_values = set()\n",
        "        for mask_path in self.annotations:\n",
        "            mask = Image.open(mask_path).convert('L')\n",
        "            mask_array = np.array(mask)\n",
        "            unique_values.update(np.unique(mask_array))\n",
        "        print(f\"Seg 數據集掩碼值分佈：{sorted(unique_values)}\")\n",
        "        if max(unique_values) > 255 or min(unique_values) < 0:\n",
        "            raise ValueError(\"掩碼值超出範圍 [0, 255]！\")\n",
        "        if len(unique_values) > 20:\n",
        "            print(\"警告：掩碼值種類超過 20，可能需要重新映射！\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "            img_path = self.images[idx]\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "            if self.task == 'seg':\n",
        "                mask = Image.open(self.annotations[idx]).convert('L') # 獲取灰度掩碼\n",
        "                mask = np.array(mask)\n",
        "\n",
        "                # *** 這裡需要根據你的數據集定義明確的灰度值到類別ID的映射 ***\n",
        "                # 假設你的原始掩碼值 0-255 已經代表了 0-19 的類別，或者你需要進行映射\n",
        "                # 例如：如果原始值 15 代表類別 1，34 代表類別 2 等等。\n",
        "                # 如果原始灰度值就是類別 ID (0-19)，則可以直接使用\n",
        "                # mask = mask.astype(np.long) # 直接轉為 long 類型\n",
        "                # 如果需要映射，請加入映射邏輯\n",
        "                # 簡單示例（假設原始灰度值直接對應類別 ID * 一個因子）：\n",
        "                # 這段代碼是基於你原有的邏輯，但請務必驗證你的原始數據和期望的映射關係\n",
        "                unique_mask_values = np.unique(mask)\n",
        "                # print(f\"圖片 {self.images[idx]} 的唯一掩碼值: {unique_mask_values}\") # Debug 用\n",
        "                if np.max(mask) > 19: # 判斷是否需要縮放或映射\n",
        "                    # 假設原始灰度值是 0-255 之間的，需要縮放到 0-19\n",
        "                    # 這只是示例，你需要根據你的數據集實際情況來處理\n",
        "                    mask = (mask / 255.0 * 19.0).astype(np.long) # 縮放並轉為 long\n",
        "                else:\n",
        "                    mask = mask.astype(np.long) # 如果已經是 0-19 範圍，直接轉 long\n",
        "\n",
        "                mask = torch.tensor(mask, dtype=torch.long) # 轉換為 Long Tensor\n",
        "\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "                    mask_transform = transforms.Compose([\n",
        "                        # 分割掩碼通常不需要歸一化等，只需要調整大小和轉為 Tensor\n",
        "                        transforms.Resize((16, 16), interpolation=Image.Resampling.NEAREST), # 保持像素值不變\n",
        "                        # 注意：這裡resize會改變掩碼的原始類別分佈，如果你需要精確的像素級分割，可能需要調整\n",
        "                        # 或者在 resize 後重新映射像素值到類別 ID\n",
        "                        transforms.ToTensor() # ToTensor() 會將 Long 類型轉換為 Float，這是不對的\n",
        "                    ])\n",
        "                    # 正確的轉換方式是先 Resize，然後手動轉為 Tensor，並確保是 Long 類型\n",
        "                    mask_img = Image.fromarray(mask.numpy().astype(np.uint8)) # 暫時轉回PIL Image for Resize\n",
        "                    mask_resized = mask_transform(mask_img).squeeze(0) # Resize並轉為Float Tensor\n",
        "                    mask = mask_resized.long() # 再次轉回 Long 類型\n",
        "\n",
        "                return img, mask\n",
        "\n",
        "            # 其他任務保持不變\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "\n",
        "            if self.task == 'det':\n",
        "                ann = self.annotations[idx]\n",
        "                boxes = torch.tensor([a['boxes'] for a in ann], dtype=torch.float32)\n",
        "                labels = torch.tensor([a['labels'] for a in ann], dtype=torch.long)\n",
        "                return img, {'boxes': boxes, 'labels': labels}\n",
        "\n",
        "            elif self.task == 'cls':\n",
        "                return img, torch.tensor(self.annotations[idx], dtype=torch.long)\n",
        "\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((512, 512), interpolation=Image.Resampling.BILINEAR),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomCrop(512, padding=64),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_datasets = {\n",
        "    'det': MultiTaskDataset('data/mini_coco_det/train', 'det', image_transform),\n",
        "    'seg': MultiTaskDataset('data/mini_voc_seg/train', 'seg', image_transform),\n",
        "    'cls': MultiTaskDataset('data/imagenette_160/train', 'cls', image_transform)\n",
        "}\n",
        "val_datasets = {\n",
        "    'det': MultiTaskDataset('data/mini_coco_det/val', 'det', image_transform),\n",
        "    'seg': MultiTaskDataset('data/mini_voc_seg/val', 'seg', image_transform),\n",
        "    'cls': MultiTaskDataset('data/imagenette_160/val', 'cls', image_transform)\n",
        "}\n",
        "\n",
        "def custom_collate(batch: List[Tuple[torch.Tensor, any]]) -> Tuple[torch.Tensor, List[any]]:\n",
        "    images = torch.stack([item[0] for item in batch])\n",
        "    targets = [item[1] for item in batch]\n",
        "    return images, targets\n",
        "\n",
        "train_loaders = {task: DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=custom_collate if task == 'det' else None) for task, dataset in train_datasets.items()}\n",
        "val_loaders = {task: DataLoader(dataset, batch_size=4, shuffle=False, collate_fn=custom_collate if task == 'det' else None) for task, dataset in val_datasets.items()}\n",
        "\n",
        "class MultiTaskHead(nn.Module):\n",
        "    def __init__(self, in_channels: int = 576):\n",
        "        super(MultiTaskHead, self).__init__()\n",
        "        self.neck = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Conv2d(128, 128, kernel_size=1)\n",
        "        )\n",
        "        self.det_head = nn.Conv2d(128, 6, kernel_size=1)\n",
        "        self.seg_head = nn.Conv2d(128, 20, kernel_size=1)\n",
        "        self.cls_head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        x = self.neck(x)\n",
        "        x = self.head(x)\n",
        "        det_out = self.det_head(x)\n",
        "        seg_out = self.seg_head(x)\n",
        "        cls_out = self.cls_head(x)\n",
        "        return det_out, seg_out, cls_out\n",
        "\n",
        "class UnifiedModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UnifiedModel, self).__init__()\n",
        "        self.backbone = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1).features\n",
        "        self.head = MultiTaskHead(in_channels=576)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        features = self.backbone(x)\n",
        "        det_out, seg_out, cls_out = self.head(features)\n",
        "        return det_out, seg_out, cls_out\n",
        "\n",
        "model = UnifiedModel().to(device)\n",
        "\n",
        "def count_parameters(model: nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_params = count_parameters(model)\n",
        "print(f\"模型總參數量：{total_params:,} (< 8M: {total_params < 8_000_000})\")\n",
        "\n",
        "def measure_inference_time(model: nn.Module, input_size: Tuple[int, int, int, int], device: torch.device, num_runs: int = 100) -> float:\n",
        "    model.eval()\n",
        "    dummy_input = torch.randn(input_size).to(device)\n",
        "    for _ in range(10):\n",
        "        model(dummy_input)\n",
        "    start_time = time.time()\n",
        "    for _ in range(num_runs):\n",
        "        model(dummy_input)\n",
        "    end_time = time.time()\n",
        "    return (end_time - start_time) / num_runs * 1000\n",
        "\n",
        "inference_time = measure_inference_time(model, (1, 3, 512, 512), device)\n",
        "print(f\"單張 512x512 圖像推理時間：{inference_time:.2f} ms (< 150 ms: {inference_time < 150})\")\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity: int = 50):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "\n",
        "    def add(self, data: Tuple[torch.Tensor, any]):\n",
        "        if len(self.buffer) >= self.capacity:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append(data)\n",
        "\n",
        "    def sample(self) -> List[Tuple[torch.Tensor, any]]:\n",
        "        return self.buffer\n",
        "\n",
        "replay_buffers = {task: ReplayBuffer(capacity=50) for task in ['seg', 'det', 'cls']}\n",
        "\n",
        "def compute_seg_class_weights(loader: DataLoader, num_classes: int = 20) -> torch.Tensor:\n",
        "    class_counts = torch.zeros(num_classes).to(device)\n",
        "    total_pixels = 0\n",
        "    for _, targets in loader:\n",
        "        targets = targets.to(device)\n",
        "        for c in range(num_classes):\n",
        "            class_counts[c] += (targets == c).sum().float()\n",
        "        total_pixels += targets.numel()\n",
        "    weights = total_pixels / (num_classes * class_counts + 1e-6)\n",
        "    weights = torch.clamp(weights, min=0.1)  # 限制最小權重，避免過小\n",
        "    weights = weights / weights.sum() * num_classes\n",
        "    print(f\"Seg 任務類別權重：{weights.tolist()}\")\n",
        "    return weights\n",
        "\n",
        "seg_class_weights = compute_seg_class_weights(train_loaders['seg'])\n",
        "\n",
        "def compute_losses(outputs: Tuple[torch.Tensor, torch.Tensor, torch.Tensor], targets: any, task: str) -> torch.Tensor:\n",
        "    det_out, seg_out, cls_out = outputs\n",
        "    if task == 'det':\n",
        "        if not isinstance(targets, list):\n",
        "            return torch.tensor(0.).to(device)\n",
        "        boxes_pred = det_out.permute(0, 2, 3, 1)\n",
        "        loss = 0\n",
        "        for i in range(len(targets)):\n",
        "            if not isinstance(targets[i], dict) or 'boxes' not in targets[i]:\n",
        "                continue\n",
        "            target_boxes = targets[i]['boxes'].to(device)\n",
        "            if len(target_boxes) == 0:\n",
        "                continue\n",
        "            pred_box = boxes_pred[i, 0, 0, :4]\n",
        "            target_box = target_boxes[0]\n",
        "            iou = calculate_iou(pred_box, target_box)\n",
        "            loss += 1 - iou if iou > 0 else 1\n",
        "        return loss / len(targets) if len(targets) > 0 else torch.tensor(0.).to(device)\n",
        "    elif task == 'seg':\n",
        "        if not isinstance(targets, torch.Tensor):\n",
        "            return torch.tensor(0.).to(device)\n",
        "        batch_size, num_classes, height, width = seg_out.size()\n",
        "        seg_out = seg_out.permute(0, 2, 3, 1).contiguous().view(-1, num_classes)\n",
        "        targets = targets.to(device).view(-1)\n",
        "        if targets.max() >= num_classes or targets.min() < 0:\n",
        "            print(f\"警告：seg 任務 targets 值範圍異常！最大值：{targets.max()}, 最小值：{targets.min()}\")\n",
        "            return torch.tensor(0.).to(device)\n",
        "        criterion = nn.CrossEntropyLoss(weight=seg_class_weights)\n",
        "        return criterion(seg_out, targets)\n",
        "    elif task == 'cls':\n",
        "        if not isinstance(targets, torch.Tensor):\n",
        "            return torch.tensor(0.).to(device)\n",
        "        targets = targets.to(device)\n",
        "        return nn.CrossEntropyLoss()(cls_out, targets)\n",
        "    return torch.tensor(0.).to(device)\n",
        "\n",
        "def calculate_iou(box1: torch.Tensor, box2: torch.Tensor) -> float:\n",
        "    box1 = box1.cpu()\n",
        "    box2 = box2.cpu()\n",
        "    x1, y1, w1, h1 = box1\n",
        "    x2, y2, w2, h2 = box2\n",
        "    x_left = max(x1 - w1/2, x2 - w2/2)\n",
        "    y_top = max(y1 - h1/2, y2 - h2/2)\n",
        "    x_right = min(x1 + w1/2, x2 + w2/2)\n",
        "    y_bottom = min(y1 + h1/2, y2 + h2/2)\n",
        "    if x_right < x_left or y_bottom < y_top:\n",
        "        return 0.0\n",
        "    intersection = (x_right - x_left) * (y_bottom - y_top)\n",
        "    union = w1 * h1 + w2 * h2 - intersection\n",
        "    return intersection / union if union > 0 else 0.0\n",
        "\n",
        "def evaluate(model: nn.Module, loader: DataLoader, task: str) -> Dict[str, float]:\n",
        "    model.eval()\n",
        "    metrics = {'mIoU': 0.0, 'mAP': 0.0, 'Top-1': 0.0}\n",
        "    if task == 'seg':\n",
        "        num_classes = 20\n",
        "        intersection = torch.zeros(num_classes).to(device)\n",
        "        union = torch.zeros(num_classes).to(device)\n",
        "        total_batches = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in loader:\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "                _, seg_out, _ = model(inputs)\n",
        "                preds = seg_out.argmax(dim=1)\n",
        "                targets = targets\n",
        "                for c in range(num_classes):\n",
        "                    pred_mask = (preds == c)\n",
        "                    target_mask = (targets == c)\n",
        "                    intersection[c] += (pred_mask & target_mask).sum().float()\n",
        "                    union[c] += (pred_mask | target_mask).sum().float()\n",
        "                total_batches += 1\n",
        "        if total_batches > 0:\n",
        "            iou = intersection / (union + 1e-6)\n",
        "            metrics['mIoU'] = float(iou.mean().item())\n",
        "    elif task == 'det':\n",
        "        aps = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in loader:\n",
        "                inputs = inputs.to(device)\n",
        "                det_out, _, _ = model(inputs)\n",
        "                boxes_pred = det_out.permute(0, 2, 3, 1)\n",
        "                for i in range(len(targets)):\n",
        "                    if not isinstance(targets[i], dict) or 'boxes' not in targets[i]:\n",
        "                        continue\n",
        "                    target_boxes = targets[i]['boxes'].to(device)\n",
        "                    if len(target_boxes) == 0:\n",
        "                        continue\n",
        "                    pred_boxes = boxes_pred[i].view(-1, 6)[:, :4]\n",
        "                    ious = [calculate_iou(pred_box, target_box) for pred_box in pred_boxes for target_box in target_boxes]\n",
        "                    ap = max(ious) if ious else 0.0\n",
        "                    aps.append(ap)\n",
        "        metrics['mAP'] = float(np.mean(aps)) if aps else 0.0\n",
        "    elif task == 'cls':\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in loader:\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "                _, _, cls_out = model(inputs)\n",
        "                preds = cls_out.argmax(dim=1)\n",
        "                total_correct += (preds == targets).sum().item()\n",
        "                total_samples += targets.size(0)\n",
        "        metrics['Top-1'] = total_correct / total_samples if total_samples > 0 else 0.0\n",
        "    return metrics\n",
        "\n",
        "def train_stage(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, task: str, epochs: int, optimizer: optim.Optimizer,\n",
        "                replay_buffers: Dict[str, ReplayBuffer], tasks: List[str], stage: int) -> Tuple[List[float], List[Dict[str, float]], Dict[str, float]]:\n",
        "    train_losses = []\n",
        "    val_metrics = []\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            if task != 'det':\n",
        "                targets = targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            det_out, seg_out, cls_out = model(inputs)\n",
        "            loss = compute_losses((det_out, seg_out, cls_out), targets, task)\n",
        "            epoch_loss += loss.item()\n",
        "            detached_inputs = inputs.detach().cpu()\n",
        "            detached_targets = copy.deepcopy(targets) if task == 'det' else targets.detach().cpu()\n",
        "            replay_buffers[task].add((detached_inputs, detached_targets))\n",
        "            replay_loss = 0.0\n",
        "            replay_batch_count = 0\n",
        "            for prev_task in tasks[:stage]:\n",
        "                buffer = replay_buffers[prev_task].sample()\n",
        "                for b_inputs, b_targets in buffer:\n",
        "                    b_inputs = b_inputs.to(device)\n",
        "                    b_det_out, b_seg_out, b_cls_out = model(b_inputs)\n",
        "                    replay_loss += compute_losses((b_det_out, b_seg_out, b_cls_out), b_targets, prev_task)\n",
        "                    replay_batch_count += 1\n",
        "            if stage > 0 and replay_loss > 0 and replay_batch_count > 0:\n",
        "                replay_loss /= replay_batch_count\n",
        "                loss += replay_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        train_losses.append(avg_loss)\n",
        "        if (epoch + 1) % 5 == 0 or epoch == 0 or epoch == epochs - 1:\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, {task} 平均損失: {avg_loss:.4f}\")\n",
        "            metrics = evaluate(model, val_loader, task)\n",
        "            val_metrics.append(metrics)\n",
        "            print(f\"驗證指標 - {task}: mIoU={metrics['mIoU']:.4f}, mAP={metrics['mAP']:.4f}, Top-1={metrics['Top-1']:.4f}\")\n",
        "    final_metrics = evaluate(model, val_loader, task)\n",
        "    return train_losses, val_metrics, final_metrics\n",
        "\n",
        "def plot_curves(task_metrics: Dict[str, Tuple[List[float], List[Dict[str, float]]]]):\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i, (task, (train_losses, val_metrics, _)) in enumerate(task_metrics.items(), 1):\n",
        "        plt.subplot(1, 3, i)\n",
        "        plt.plot(train_losses, label=f'{task} Loss')\n",
        "        epochs = range(1, len(train_losses) + 1)\n",
        "        plt.plot([i * 5 for i in range(len(val_metrics))], [m['mIoU' if task == 'seg' else 'mAP' if task == 'det' else 'Top-1'] for m in val_metrics], 'r--', label=f'{task} Metric')\n",
        "        plt.title(f'{task} 訓練損失與指標')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('值')\n",
        "        plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
        "\n",
        "tasks = ['seg', 'det', 'cls']\n",
        "baselines = {'mIoU': 0.0, 'mAP': 0.0, 'Top-1': 0.0}\n",
        "task_metrics = {}\n",
        "\n",
        "epochs_per_stage = 120  # 減少到 20 回合，方便測試\n",
        "total_training_time = 0\n",
        "\n",
        "for stage, task in enumerate(tasks):\n",
        "    print(f\"\\n=== 訓練階段 {stage + 1}: {task} ===\")\n",
        "    start_time = time.time()\n",
        "    train_losses, val_metrics, final_metrics = train_stage(model, train_loaders[task], val_loaders[task], task,\n",
        "                                                           epochs_per_stage, optimizer, replay_buffers, tasks, stage)\n",
        "    stage_time = time.time() - start_time\n",
        "    total_training_time += stage_time\n",
        "    print(f\"階段 {stage + 1} 完成，耗時 {stage_time:.2f} 秒\")\n",
        "    task_metrics[task] = (train_losses, val_metrics, final_metrics)\n",
        "    if stage == 0:\n",
        "        baselines['mIoU'] = final_metrics.get('mIoU', 0.0)\n",
        "    elif stage == 1:\n",
        "        baselines['mAP'] = final_metrics.get('mAP', 0.0)\n",
        "    elif stage == 2:\n",
        "        baselines['Top-1'] = final_metrics.get('Top-1', 0.0)\n",
        "    scheduler.step()\n",
        "\n",
        "print(f\"\\n=== 總訓練時間：{total_training_time:.2f} 秒 (< 2 小時：{total_training_time < 7200}) ===\")\n",
        "\n",
        "print(\"\\n=== 最終評估 ===\")\n",
        "final_metrics = {}\n",
        "for task in tasks:\n",
        "    metrics = evaluate(model, val_loaders[task], task)\n",
        "    final_metrics[task] = metrics\n",
        "    print(f\"{task} 最終評估: mIoU={metrics['mIoU']:.4f}, mAP={metrics['mAP']:.4f}, Top-1={metrics['Top-1']:.4f}\")\n",
        "\n",
        "print(\"\\n=== 性能下降（相較於各任務基準） ===\")\n",
        "for task in tasks:\n",
        "    if task == 'seg':\n",
        "        baseline = baselines['mIoU']\n",
        "        final = final_metrics['seg']['mIoU']\n",
        "        metric_name = 'mIoU'\n",
        "    elif task == 'det':\n",
        "        baseline = baselines['mAP']\n",
        "        final = final_metrics['det']['mAP']\n",
        "        metric_name = 'mAP'\n",
        "    elif task == 'cls':\n",
        "        baseline = baselines['Top-1']\n",
        "        final = final_metrics['cls']['Top-1']\n",
        "        metric_name = 'Top-1'\n",
        "    if baseline > 0:\n",
        "        drop = (baseline - final) / baseline * 100 if baseline > 0 else 0\n",
        "        print(f\"{task} {metric_name} 下降：{drop:.2f}% (< 5%：{drop < 5})\")\n",
        "    else:\n",
        "        print(f\"{task} {metric_name}：基準為 0，無法計算下降。\")\n",
        "\n",
        "plot_curves(task_metrics)\n",
        "torch.save(model.state_dict(), 'your_model.pt')\n",
        "print(\"模型已儲存為 'your_model7.pt'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 949
        },
        "id": "5FxDtPCwxnX3",
        "outputId": "d0c3e38d-bc46-451d-9923-f2334433d396"
      },
      "id": "5FxDtPCwxnX3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用設備：cuda\n",
            "Seg 數據集掩碼值分佈：[np.uint8(0), np.uint8(15), np.uint8(19), np.uint8(34), np.uint8(38), np.uint8(52), np.uint8(53), np.uint8(57), np.uint8(72), np.uint8(75), np.uint8(76), np.uint8(90), np.uint8(94), np.uint8(109), np.uint8(113), np.uint8(128), np.uint8(133), np.uint8(147), np.uint8(151), np.uint8(220)]\n",
            "Seg 數據集掩碼值分佈：[np.uint8(0), np.uint8(15), np.uint8(19), np.uint8(34), np.uint8(38), np.uint8(52), np.uint8(53), np.uint8(57), np.uint8(72), np.uint8(75), np.uint8(76), np.uint8(90), np.uint8(94), np.uint8(109), np.uint8(113), np.uint8(128), np.uint8(133), np.uint8(147), np.uint8(151), np.uint8(220)]\n",
            "模型總參數量：5,077,828 (< 8M: True)\n",
            "單張 512x512 圖像推理時間：8.42 ms (< 150 ms: True)\n",
            "Seg 任務類別權重：[1.713267548664188e-12, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072, 1.0526316165924072]\n",
            "\n",
            "=== 訓練階段 1: seg ===\n",
            "Epoch 1/120, seg 平均損失: 0.2658\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 5/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 10/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 15/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 20/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 25/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 30/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 35/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 40/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 45/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 50/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-f8110be19d57>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n=== 訓練階段 {stage + 1}: {task} ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m     train_losses, val_metrics, final_metrics = train_stage(model, train_loaders[task], val_loaders[task], task,\n\u001b[0m\u001b[1;32m    445\u001b[0m                                                            epochs_per_stage, optimizer, replay_buffers, tasks, stage)\n\u001b[1;32m    446\u001b[0m     \u001b[0mstage_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-f8110be19d57>\u001b[0m in \u001b[0;36mtrain_stage\u001b[0;34m(model, train_loader, val_loader, task, epochs, optimizer, replay_buffers, tasks, stage)\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0mreplay_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mreplay_batch_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreplay_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 統一單頭多任務挑戰實作 (第八版優化 - 修正掩碼映射與損失)\n",
        "# 安裝所需套件庫\n",
        "!pip install torch torchvision torchaudio -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import copy\n",
        "import time\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"使用設備：{device}\")\n",
        "\n",
        "#\n",
        "class MultiTaskDataset(Dataset):\n",
        "    def __init__(self, data_dir: str, task: str, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.task = task\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.annotations = []\n",
        "        if task == 'det':\n",
        "            labels_path = os.path.join(data_dir, 'labels.json')\n",
        "            if not os.path.exists(labels_path):\n",
        "                raise FileNotFoundError(f\"找不到 {labels_path}，請確認檔案是否存在！\")\n",
        "            with open(labels_path, 'r') as f:\n",
        "                labels_data = json.load(f)\n",
        "            image_dir = os.path.join(data_dir, 'data')\n",
        "            image_files = sorted([img for img in os.listdir(image_dir) if img.endswith(('.jpg', '.jpeg', '.JPEG'))])\n",
        "            image_file_set = set(image_files)\n",
        "            valid_images = {img['id']: img['file_name'] for img in labels_data['images'] if img['file_name'] in image_file_set}\n",
        "            ann_dict = {}\n",
        "            for ann in labels_data['annotations']:\n",
        "                img_id = ann['image_id']\n",
        "                if img_id in valid_images:\n",
        "                    if img_id not in ann_dict:\n",
        "                        ann_dict[img_id] = []\n",
        "                    ann_dict[img_id].append({'boxes': ann['bbox'], 'labels': ann['category_id']})\n",
        "            for img_id, file_name in valid_images.items():\n",
        "                full_path = os.path.join(image_dir, file_name)\n",
        "                if img_id in ann_dict:\n",
        "                    self.images.append(full_path)\n",
        "                    self.annotations.append(ann_dict[img_id])\n",
        "        elif task == 'seg':\n",
        "            image_files = sorted([img for img in os.listdir(data_dir) if img.endswith(('.jpg', '.jpeg', '.JPEG'))])\n",
        "            for img in image_files:\n",
        "                img_path = os.path.join(data_dir, img)\n",
        "                mask_path = os.path.join(data_dir, img.replace('.jpg', '.png').replace('.jpeg', '.png').replace('.JPEG', '.png'))\n",
        "                if os.path.exists(mask_path):\n",
        "                    self.images.append(img_path)\n",
        "                    self.annotations.append(mask_path)\n",
        "            if task == 'seg':\n",
        "                self._check_mask_distribution()\n",
        "                # 定義掩碼映射表：原始值到 0-19 的類別索引\n",
        "                original_values = [0, 15, 19, 34, 38, 52, 53, 57, 72, 75, 76, 90, 94, 109, 113, 128, 133, 147, 151, 220]\n",
        "                mapped_indices = list(range(len(original_values)))  # 映射到 0-19\n",
        "                self.mask_value_to_index = dict(zip(original_values, mapped_indices))\n",
        "                print(f\"Seg 掩碼映射表：{self.mask_value_to_index}\")\n",
        "        elif task == 'cls':\n",
        "            label_dirs = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
        "            label_to_index = {label: idx for idx, label in enumerate(label_dirs)}\n",
        "            for label in label_dirs:\n",
        "                label_path = os.path.join(data_dir, label)\n",
        "                for root, _, files in os.walk(label_path):\n",
        "                    for img in files:\n",
        "                        if img.endswith(('.jpg', '.jpeg', '.JPEG')):\n",
        "                            img_path = os.path.join(root, img)\n",
        "                            self.images.append(img_path)\n",
        "                            self.annotations.append(label_to_index[label])\n",
        "        if len(self.images) == 0:\n",
        "            raise ValueError(f\"在 {data_dir} 中未找到任何資料，請檢查資料結構！\")\n",
        "\n",
        "    def _check_mask_distribution(self):\n",
        "        unique_values = set()\n",
        "        for mask_path in self.annotations:\n",
        "            mask = Image.open(mask_path).convert('L')\n",
        "            mask_array = np.array(mask)\n",
        "            unique_values.update(np.unique(mask_array))\n",
        "        print(f\"Seg 數據集掩碼值分佈：{sorted(unique_values)}\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, any]:\n",
        "        img_path = self.images[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        if self.task == 'seg':\n",
        "            mask = Image.open(self.annotations[idx]).convert('L')\n",
        "            mask_array = np.array(mask)\n",
        "            # 映射原始值到類別索引\n",
        "            mapped_mask = np.zeros_like(mask_array, dtype=np.int64)\n",
        "            for original_val, mapped_idx in self.mask_value_to_index.items():\n",
        "                mapped_mask[mask_array == original_val] = mapped_idx\n",
        "            mask = torch.tensor(mapped_mask, dtype=torch.long)\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "                mask_img = Image.fromarray(mask.numpy().astype(np.uint8))\n",
        "                mask_transform = transforms.Compose([\n",
        "                    transforms.Resize((16, 16), interpolation=Image.Resampling.NEAREST),\n",
        "                    transforms.ToTensor()\n",
        "                ])\n",
        "                mask = mask_transform(mask_img)\n",
        "                mask = mask.squeeze(0).long()\n",
        "            # 確認掩碼尺寸\n",
        "            if mask.shape != (16, 16):\n",
        "                raise ValueError(f\"掩碼尺寸不正確：{mask.shape}，應為 (16, 16)\")\n",
        "            return img, mask\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        if self.task == 'det':\n",
        "            ann = self.annotations[idx]\n",
        "            boxes = torch.tensor([a['boxes'] for a in ann], dtype=torch.float32)\n",
        "            labels = torch.tensor([a['labels'] for a in ann], dtype=torch.long)\n",
        "            return img, {'boxes': boxes, 'labels': labels}\n",
        "        elif self.task == 'cls':\n",
        "            return img, torch.tensor(self.annotations[idx], dtype=torch.long)\n",
        "\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((512, 512), interpolation=Image.Resampling.BILINEAR),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomCrop(512, padding=64),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_datasets = {\n",
        "    'det': MultiTaskDataset('data/mini_coco_det/train', 'det', image_transform),\n",
        "    'seg': MultiTaskDataset('data/mini_voc_seg/train', 'seg', image_transform),\n",
        "    'cls': MultiTaskDataset('data/imagenette_160/train', 'cls', image_transform)\n",
        "}\n",
        "val_datasets = {\n",
        "    'det': MultiTaskDataset('data/mini_coco_det/val', 'det', image_transform),\n",
        "    'seg': MultiTaskDataset('data/mini_voc_seg/val', 'seg', image_transform),\n",
        "    'cls': MultiTaskDataset('data/imagenette_160/val', 'cls', image_transform)\n",
        "}\n",
        "\n",
        "def custom_collate(batch: List[Tuple[torch.Tensor, any]]) -> Tuple[torch.Tensor, List[any]]:\n",
        "    images = torch.stack([item[0] for item in batch])\n",
        "    targets = [item[1] for item in batch]\n",
        "    return images, targets\n",
        "\n",
        "train_loaders = {task: DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=custom_collate if task == 'det' else None) for task, dataset in train_datasets.items()}\n",
        "val_loaders = {task: DataLoader(dataset, batch_size=4, shuffle=False, collate_fn=custom_collate if task == 'det' else None) for task, dataset in val_datasets.items()}\n",
        "\n",
        "class MultiTaskHead(nn.Module):\n",
        "    def __init__(self, in_channels: int = 576):\n",
        "        super(MultiTaskHead, self).__init__()\n",
        "        self.neck = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Conv2d(128, 128, kernel_size=1)\n",
        "        )\n",
        "        self.det_head = nn.Conv2d(128, 6, kernel_size=1)\n",
        "        self.seg_head = nn.Conv2d(128, 20, kernel_size=1)\n",
        "        self.cls_head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        x = self.neck(x)\n",
        "        x = self.head(x)\n",
        "        det_out = self.det_head(x)\n",
        "        seg_out = self.seg_head(x)\n",
        "        cls_out = self.cls_head(x)\n",
        "        return det_out, seg_out, cls_out\n",
        "\n",
        "class UnifiedModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UnifiedModel, self).__init__()\n",
        "        self.backbone = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1).features\n",
        "        self.head = MultiTaskHead(in_channels=576)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        features = self.backbone(x)\n",
        "        det_out, seg_out, cls_out = self.head(features)\n",
        "        return det_out, seg_out, cls_out\n",
        "\n",
        "model = UnifiedModel().to(device)\n",
        "\n",
        "def count_parameters(model: nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_params = count_parameters(model)\n",
        "print(f\"模型總參數量：{total_params:,} (< 8M: {total_params < 8_000_000})\")\n",
        "\n",
        "def measure_inference_time(model: nn.Module, input_size: Tuple[int, int, int, int], device: torch.device, num_runs: int = 100) -> float:\n",
        "    model.eval()\n",
        "    dummy_input = torch.randn(input_size).to(device)\n",
        "    for _ in range(10):\n",
        "        model(dummy_input)\n",
        "    start_time = time.time()\n",
        "    for _ in range(num_runs):\n",
        "        model(dummy_input)\n",
        "    end_time = time.time()\n",
        "    return (end_time - start_time) / num_runs * 1000\n",
        "\n",
        "inference_time = measure_inference_time(model, (1, 3, 512, 512), device)\n",
        "print(f\"單張 512x512 圖像推理時間：{inference_time:.2f} ms (< 150 ms: {inference_time < 150})\")\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity: int = 50):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "\n",
        "    def add(self, data: Tuple[torch.Tensor, any]):\n",
        "        if len(self.buffer) >= self.capacity:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append(data)\n",
        "\n",
        "    def sample(self) -> List[Tuple[torch.Tensor, any]]:\n",
        "        return self.buffer\n",
        "\n",
        "replay_buffers = {task: ReplayBuffer(capacity=50) for task in ['seg', 'det', 'cls']}\n",
        "\n",
        "def compute_seg_class_weights(loader: DataLoader, num_classes: int = 20) -> torch.Tensor:\n",
        "    class_counts = torch.zeros(num_classes).to(device)\n",
        "    total_pixels = 0\n",
        "    for _, targets in loader:\n",
        "        targets = targets.to(device)\n",
        "        for c in range(num_classes):\n",
        "            class_counts[c] += (targets == c).sum().float()\n",
        "        total_pixels += targets.numel()\n",
        "    weights = total_pixels / (num_classes * class_counts + 1e-6)\n",
        "    weights = torch.clamp(weights, min=0.1, max=10.0)  # 限制權重範圍\n",
        "    weights = weights / weights.sum() * num_classes\n",
        "    print(f\"Seg 任務類別權重：{weights.tolist()}\")\n",
        "    return weights\n",
        "\n",
        "seg_class_weights = compute_seg_class_weights(train_loaders['seg'])\n",
        "\n",
        "def compute_losses(outputs: Tuple[torch.Tensor, torch.Tensor, torch.Tensor], targets: any, task: str, class_weights=None) -> torch.Tensor:\n",
        "    det_out, seg_out, cls_out = outputs\n",
        "    if task == 'det':\n",
        "        if not isinstance(targets, list):\n",
        "            return torch.tensor(0.).to(device)\n",
        "        boxes_pred = det_out.permute(0, 2, 3, 1)\n",
        "        loss = 0\n",
        "        for i in range(len(targets)):\n",
        "            if not isinstance(targets[i], dict) or 'boxes' not in targets[i]:\n",
        "                continue\n",
        "            target_boxes = targets[i]['boxes'].to(device)\n",
        "            if len(target_boxes) == 0:\n",
        "                continue\n",
        "            pred_box = boxes_pred[i, 0, 0, :4]\n",
        "            target_box = target_boxes[0]\n",
        "            iou = calculate_iou(pred_box, target_box)\n",
        "            loss += 1 - iou if iou > 0 else 1\n",
        "        return loss / len(targets) if len(targets) > 0 else torch.tensor(0.).to(device)\n",
        "    elif task == 'seg':\n",
        "        batch_size, num_classes, height, width = seg_out.size()\n",
        "        seg_out = seg_out.permute(0, 2, 3, 1).contiguous().view(-1, num_classes)\n",
        "        targets = targets.to(device).view(-1)\n",
        "        if targets.max() >= num_classes or targets.min() < 0:\n",
        "            print(f\"警告：seg 任務 targets 值範圍異常！最大值：{targets.max()}, 最小值：{targets.min()}\")\n",
        "            return torch.tensor(0.).to(device)\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights, reduction='mean')\n",
        "        return criterion(seg_out, targets)\n",
        "    elif task == 'cls':\n",
        "        targets = targets.to(device)\n",
        "        return nn.CrossEntropyLoss()(cls_out, targets)\n",
        "    return torch.tensor(0.).to(device)\n",
        "\n",
        "def calculate_iou(box1: torch.Tensor, box2: torch.Tensor) -> float:\n",
        "    box1 = box1.cpu()\n",
        "    box2 = box2.cpu()\n",
        "    x1, y1, w1, h1 = box1\n",
        "    x2, y2, w2, h2 = box2\n",
        "    x_left = max(x1 - w1/2, x2 - w2/2)\n",
        "    y_top = max(y1 - h1/2, y2 - h2/2)\n",
        "    x_right = min(x1 + w1/2, x2 + w2/2)\n",
        "    y_bottom = min(y1 + h1/2, y2 + h2/2)\n",
        "    if x_right < x_left or y_bottom < y_top:\n",
        "        return 0.0\n",
        "    intersection = (x_right - x_left) * (y_bottom - y_top)\n",
        "    union = w1 * h1 + w2 * h2 - intersection\n",
        "    return intersection / union if union > 0 else 0.0\n",
        "\n",
        "def evaluate(model: nn.Module, loader: DataLoader, task: str) -> Dict[str, float]:\n",
        "    model.eval()\n",
        "    metrics = {'mIoU': 0.0, 'mAP': 0.0, 'Top-1': 0.0}\n",
        "    if task == 'seg':\n",
        "        num_classes = 20\n",
        "        intersection = torch.zeros(num_classes).to(device)\n",
        "        union = torch.zeros(num_classes).to(device)\n",
        "        total_batches = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in loader:\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "                _, seg_out, _ = model(inputs)\n",
        "                preds = seg_out.argmax(dim=1)\n",
        "                for c in range(num_classes):\n",
        "                    pred_mask = (preds == c)\n",
        "                    target_mask = (targets == c)\n",
        "                    intersection[c] += (pred_mask & target_mask).sum().float()\n",
        "                    union[c] += (pred_mask | target_mask).sum().float()\n",
        "                total_batches += 1\n",
        "        if total_batches > 0:\n",
        "            iou = intersection / (union + 1e-6)\n",
        "            metrics['mIoU'] = float(iou.mean().item())\n",
        "    elif task == 'det':\n",
        "        aps = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in loader:\n",
        "                inputs = inputs.to(device)\n",
        "                det_out, _, _ = model(inputs)\n",
        "                boxes_pred = det_out.permute(0, 2, 3, 1)\n",
        "                for i in range(len(targets)):\n",
        "                    if not isinstance(targets[i], dict) or 'boxes' not in targets[i]:\n",
        "                        continue\n",
        "                    target_boxes = targets[i]['boxes'].to(device)\n",
        "                    if len(target_boxes) == 0:\n",
        "                        continue\n",
        "                    pred_boxes = boxes_pred[i].view(-1, 6)[:, :4]\n",
        "                    ious = [calculate_iou(pred_box, target_box) for pred_box in pred_boxes for target_box in target_boxes]\n",
        "                    ap = max(ious) if ious else 0.0\n",
        "                    aps.append(ap)\n",
        "        metrics['mAP'] = float(np.mean(aps)) if aps else 0.0\n",
        "    elif task == 'cls':\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in loader:\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "                _, _, cls_out = model(inputs)\n",
        "                preds = cls_out.argmax(dim=1)\n",
        "                total_correct += (preds == targets).sum().item()\n",
        "                total_samples += targets.size(0)\n",
        "        metrics['Top-1'] = total_correct / total_samples if total_samples > 0 else 0.0\n",
        "    return metrics\n",
        "\n",
        "def train_stage(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, task: str, epochs: int, optimizer: optim.Optimizer,\n",
        "                replay_buffers: Dict[str, ReplayBuffer], tasks: List[str], stage: int) -> Tuple[List[float], List[Dict[str, float]], Dict[str, float]]:\n",
        "    train_losses = []\n",
        "    val_metrics = []\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            if task != 'det':\n",
        "                targets = targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            det_out, seg_out, cls_out = model(inputs)\n",
        "            loss = compute_losses((det_out, seg_out, cls_out), targets, task, class_weights=seg_class_weights if task == 'seg' else None)\n",
        "            epoch_loss += loss.item()\n",
        "            detached_inputs = inputs.detach().cpu()\n",
        "            detached_targets = copy.deepcopy(targets) if task == 'det' else targets.detach().cpu()\n",
        "            replay_buffers[task].add((detached_inputs, detached_targets))\n",
        "            replay_loss = 0.0\n",
        "            replay_batch_count = 0\n",
        "            for prev_task in tasks[:stage]:\n",
        "                buffer = replay_buffers[prev_task].sample()\n",
        "                for b_inputs, b_targets in buffer:\n",
        "                    b_inputs = b_inputs.to(device)\n",
        "                    b_det_out, b_seg_out, b_cls_out = model(b_inputs)\n",
        "                    replay_loss += compute_losses((b_det_out, b_seg_out, b_cls_out), b_targets, prev_task)\n",
        "                    replay_batch_count += 1\n",
        "            if stage > 0 and replay_loss > 0 and replay_batch_count > 0:\n",
        "                replay_loss /= replay_batch_count\n",
        "                loss += replay_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        train_losses.append(avg_loss)\n",
        "        if (epoch + 1) % 5 == 0 or epoch == 0 or epoch == epochs - 1:\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, {task} 平均損失: {avg_loss:.4f}\")\n",
        "            metrics = evaluate(model, val_loader, task)\n",
        "            val_metrics.append(metrics)\n",
        "            print(f\"驗證指標 - {task}: mIoU={metrics['mIoU']:.4f}, mAP={metrics['mAP']:.4f}, Top-1={metrics['Top-1']:.4f}\")\n",
        "    final_metrics = evaluate(model, val_loader, task)\n",
        "    return train_losses, val_metrics, final_metrics\n",
        "\n",
        "def plot_curves(task_metrics: Dict[str, Tuple[List[float], List[Dict[str, float]]]]):\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i, (task, (train_losses, val_metrics, _)) in enumerate(task_metrics.items(), 1):\n",
        "        plt.subplot(1, 3, i)\n",
        "        plt.plot(train_losses, label=f'{task} Loss')\n",
        "        epochs = range(1, len(train_losses) + 1)\n",
        "        plt.plot([i * 5 for i in range(len(val_metrics))], [m['mIoU' if task == 'seg' else 'mAP' if task == 'det' else 'Top-1'] for m in val_metrics], 'r--', label=f'{task} Metric')\n",
        "        plt.title(f'{task} 訓練損失與指標')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('值')\n",
        "        plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
        "\n",
        "tasks = ['seg', 'det', 'cls']\n",
        "baselines = {'mIoU': 0.0, 'mAP': 0.0, 'Top-1': 0.0}\n",
        "task_metrics = {}\n",
        "\n",
        "epochs_per_stage = 120\n",
        "total_training_time = 0\n",
        "\n",
        "for stage, task in enumerate(tasks):\n",
        "    print(f\"\\n=== 訓練階段 {stage + 1}: {task} ===\")\n",
        "    start_time = time.time()\n",
        "    train_losses, val_metrics, final_metrics = train_stage(model, train_loaders[task], val_loaders[task], task,\n",
        "                                                           epochs_per_stage, optimizer, replay_buffers, tasks, stage)\n",
        "    stage_time = time.time() - start_time\n",
        "    total_training_time += stage_time\n",
        "    print(f\"階段 {stage + 1} 完成，耗時 {stage_time:.2f} 秒\")\n",
        "    task_metrics[task] = (train_losses, val_metrics, final_metrics)\n",
        "    if stage == 0:\n",
        "        baselines['mIoU'] = final_metrics.get('mIoU', 0.0)\n",
        "    elif stage == 1:\n",
        "        baselines['mAP'] = final_metrics.get('mAP', 0.0)\n",
        "    elif stage == 2:\n",
        "        baselines['Top-1'] = final_metrics.get('Top-1', 0.0)\n",
        "    scheduler.step()\n",
        "\n",
        "print(f\"\\n=== 總訓練時間：{total_training_time:.2f} 秒 (< 2 小時：{total_training_time < 7200}) ===\")\n",
        "\n",
        "print(\"\\n=== 最終評估 ===\")\n",
        "final_metrics = {}\n",
        "for task in tasks:\n",
        "    metrics = evaluate(model, val_loaders[task], task)\n",
        "    final_metrics[task] = metrics\n",
        "    print(f\"{task} 最終評估: mIoU={metrics['mIoU']:.4f}, mAP={metrics['mAP']:.4f}, Top-1={metrics['Top-1']:.4f}\")\n",
        "\n",
        "print(\"\\n=== 性能下降（相較於各任務基準） ===\")\n",
        "for task in tasks:\n",
        "    if task == 'seg':\n",
        "        baseline = baselines['mIoU']\n",
        "        final = final_metrics['seg']['mIoU']\n",
        "        metric_name = 'mIoU'\n",
        "    elif task == 'det':\n",
        "        baseline = baselines['mAP']\n",
        "        final = final_metrics['det']['mAP']\n",
        "        metric_name = 'mAP'\n",
        "    elif task == 'cls':\n",
        "        baseline = baselines['Top-1']\n",
        "        final = final_metrics['cls']['Top-1']\n",
        "        metric_name = 'Top-1'\n",
        "    if baseline > 0:\n",
        "        drop = (baseline - final) / baseline * 100 if baseline > 0 else 0\n",
        "        print(f\"{task} {metric_name} 下降：{drop:.2f}% (< 5%：{drop < 5})\")\n",
        "    else:\n",
        "        print(f\"{task} {metric_name}：基準為 0，無法計算下降。\")\n",
        "\n",
        "plot_curves(task_metrics)\n",
        "torch.save(model.state_dict(), 'your_model8.pt')\n",
        "print(\"模型已儲存為 'your_model8.pt'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TvjmjKgwz-T4",
        "outputId": "bdafae07-170c-4dad-9b51-f1b50d5d1ffc"
      },
      "id": "TvjmjKgwz-T4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用設備：cuda\n",
            "Seg 數據集掩碼值分佈：[np.uint8(0), np.uint8(15), np.uint8(19), np.uint8(34), np.uint8(38), np.uint8(52), np.uint8(53), np.uint8(57), np.uint8(72), np.uint8(75), np.uint8(76), np.uint8(90), np.uint8(94), np.uint8(109), np.uint8(113), np.uint8(128), np.uint8(133), np.uint8(147), np.uint8(151), np.uint8(220)]\n",
            "Seg 掩碼映射表：{0: 0, 15: 1, 19: 2, 34: 3, 38: 4, 52: 5, 53: 6, 57: 7, 72: 8, 75: 9, 76: 10, 90: 11, 94: 12, 109: 13, 113: 14, 128: 15, 133: 16, 147: 17, 151: 18, 220: 19}\n",
            "Seg 數據集掩碼值分佈：[np.uint8(0), np.uint8(15), np.uint8(19), np.uint8(34), np.uint8(38), np.uint8(52), np.uint8(53), np.uint8(57), np.uint8(72), np.uint8(75), np.uint8(76), np.uint8(90), np.uint8(94), np.uint8(109), np.uint8(113), np.uint8(128), np.uint8(133), np.uint8(147), np.uint8(151), np.uint8(220)]\n",
            "Seg 掩碼映射表：{0: 0, 15: 1, 19: 2, 34: 3, 38: 4, 52: 5, 53: 6, 57: 7, 72: 8, 75: 9, 76: 10, 90: 11, 94: 12, 109: 13, 113: 14, 128: 15, 133: 16, 147: 17, 151: 18, 220: 19}\n",
            "模型總參數量：5,077,828 (< 8M: True)\n",
            "單張 512x512 圖像推理時間：8.12 ms (< 150 ms: True)\n",
            "Seg 任務類別權重：[0.010520778596401215, 1.0520777702331543, 1.0520777702331543, 1.0520777702331543, 1.0520777702331543, 1.0520777702331543, 1.0520777702331543, 1.0520777702331543, 1.0520777702331543, 1.0520777702331543, 1.0520777702331543, 1.0520777702331543, 1.0520777702331543, 1.0520777702331543, 1.0520777702331543, 1.0520777702331543, 1.0520777702331543, 1.0520777702331543, 1.0520777702331543, 1.0520777702331543]\n",
            "\n",
            "=== 訓練階段 1: seg ===\n",
            "Epoch 1/120, seg 平均損失: 0.2945\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 5/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 10/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 15/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 20/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 25/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 30/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 35/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 40/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 45/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 50/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 55/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 60/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 65/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 70/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 75/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 80/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 85/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 90/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 95/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 100/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 105/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 110/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 115/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "Epoch 120/120, seg 平均損失: 0.0000\n",
            "驗證指標 - seg: mIoU=0.0500, mAP=0.0000, Top-1=0.0000\n",
            "階段 1 完成，耗時 623.68 秒\n",
            "\n",
            "=== 訓練階段 2: det ===\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'float' object has no attribute 'item'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-b6871d6a298f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n=== 訓練階段 {stage + 1}: {task} ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     train_losses, val_metrics, final_metrics = train_stage(model, train_loaders[task], val_loaders[task], task,\n\u001b[0m\u001b[1;32m    419\u001b[0m                                                            epochs_per_stage, optimizer, replay_buffers, tasks, stage)\n\u001b[1;32m    420\u001b[0m     \u001b[0mstage_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-b6871d6a298f>\u001b[0m in \u001b[0;36mtrain_stage\u001b[0;34m(model, train_loader, val_loader, task, epochs, optimizer, replay_buffers, tasks, stage)\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0mdet_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdet_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseg_class_weights\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'seg'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mdetached_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0mdetached_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'det'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'item'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 統一單頭多任務挑戰實作 (第九版 - 適配彩色遮罩與 VOC 2012)\n",
        "# 安裝所需庫\n",
        "!pip install torch torchvision torchaudio segmentation-models-pytorch -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import copy\n",
        "import time\n",
        "from PIL import Image\n",
        "import cv2 as cv\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"使用設備：{device}\")\n",
        "\n",
        "VOC_COLORMAP = [\n",
        "    [0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0], [0, 0, 128],\n",
        "    [128, 0, 128], [0, 128, 128], [128, 128, 128], [64, 0, 0], [192, 0, 0],\n",
        "    [64, 128, 0], [192, 128, 0], [64, 0, 128], [192, 0, 128], [64, 128, 128],\n",
        "    [192, 128, 128], [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0], [0, 64, 128]\n",
        "]\n",
        "\n",
        "class MultiTaskDataset(Dataset):\n",
        "    def __init__(self, data_dir: str, task: str, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.task = task\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.annotations = []\n",
        "        if task == 'det':\n",
        "            labels_path = os.path.join(data_dir, 'labels.json')\n",
        "            if not os.path.exists(labels_path):\n",
        "                raise FileNotFoundError(f\"找不到 {labels_path}，請確認檔案是否存在！\")\n",
        "            with open(labels_path, 'r') as f:\n",
        "                labels_data = json.load(f)\n",
        "            image_dir = os.path.join(data_dir, 'data')\n",
        "            image_files = sorted([img for img in os.listdir(image_dir) if img.endswith(('.jpg', '.jpeg', '.JPEG'))])\n",
        "            image_file_set = set(image_files)\n",
        "            valid_images = {img['id']: img['file_name'] for img in labels_data['images'] if img['file_name'] in image_file_set}\n",
        "            ann_dict = {}\n",
        "            for ann in labels_data['annotations']:\n",
        "                img_id = ann['image_id']\n",
        "                if img_id in valid_images:\n",
        "                    if img_id not in ann_dict:\n",
        "                        ann_dict[img_id] = []\n",
        "                    ann_dict[img_id].append({'boxes': ann['bbox'], 'labels': ann['category_id']})\n",
        "            for img_id, file_name in valid_images.items():\n",
        "                full_path = os.path.join(image_dir, file_name)\n",
        "                if img_id in ann_dict:\n",
        "                    self.images.append(full_path)\n",
        "                    self.annotations.append(ann_dict[img_id])\n",
        "        elif task == 'seg':\n",
        "            self.root = os.path.join(data_dir, 'VOCdevkit/VOC2012')\n",
        "            self.target_dir = os.path.join(self.root, 'SegmentationClass')\n",
        "            self.images_dir = os.path.join(self.root, 'JPEGImages')\n",
        "            file_list = os.path.join(self.root, 'ImageSets/Segmentation/trainval.txt')\n",
        "            self.files = [line.rstrip() for line in tuple(open(file_list, \"r\"))]\n",
        "            self.color_map = VOC_COLORMAP\n",
        "            for file_id in self.files:\n",
        "                img_path = os.path.join(self.images_dir, f\"{file_id}.jpg\")\n",
        "                mask_path = os.path.join(self.target_dir, f\"{file_id}.png\")\n",
        "                if os.path.exists(img_path) and os.path.exists(mask_path):\n",
        "                    self.images.append(img_path)\n",
        "                    self.annotations.append(mask_path)\n",
        "        elif task == 'cls':\n",
        "            label_dirs = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
        "            label_to_index = {label: idx for idx, label in enumerate(label_dirs)}\n",
        "            for label in label_dirs:\n",
        "                label_path = os.path.join(data_dir, label)\n",
        "                for root, _, files in os.walk(label_path):\n",
        "                    for img in files:\n",
        "                        if img.endswith(('.jpg', '.jpeg', '.JPEG')):\n",
        "                            img_path = os.path.join(root, img)\n",
        "                            self.images.append(img_path)\n",
        "                            self.annotations.append(label_to_index[label])\n",
        "        if len(self.images) == 0:\n",
        "            raise ValueError(f\"在 {data_dir} 中未找到任何資料，請檢查資料結構！\")\n",
        "\n",
        "    def convert_to_segmentation_mask(self, mask):\n",
        "        height, width = mask.shape[:2]\n",
        "        segmentation_mask = np.zeros((height, width, len(self.color_map)), dtype=np.float32)\n",
        "        for label_index, label in enumerate(self.color_map):\n",
        "            segmentation_mask[:, :, label_index] = np.all(mask == label, axis=-1).astype(float)\n",
        "        return segmentation_mask\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, any]:\n",
        "        img_path = self.images[idx]\n",
        "        img = cv.imread(img_path)\n",
        "        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
        "        img = cv.resize(img, (256, 256))\n",
        "        img = torch.tensor(img).float().permute(2, 0, 1) / 255.0  # 轉為 [C, H, W] 並正規化\n",
        "\n",
        "        if self.task == 'seg':\n",
        "            mask_path = self.annotations[idx]\n",
        "            mask = cv.imread(mask_path)\n",
        "            mask = cv.cvtColor(mask, cv.COLOR_BGR2RGB)\n",
        "            mask = cv.resize(mask, (256, 256))\n",
        "            mask = self.convert_to_segmentation_mask(mask)\n",
        "            mask = torch.tensor(mask).float().permute(2, 0, 1)  # 轉為 [C, H, W]\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, mask\n",
        "        if self.task == 'det':\n",
        "            ann = self.annotations[idx]\n",
        "            boxes = torch.tensor([a['boxes'] for a in ann], dtype=torch.float32)\n",
        "            labels = torch.tensor([a['labels'] for a in ann], dtype=torch.long)\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, {'boxes': boxes, 'labels': labels}\n",
        "        elif self.task == 'cls':\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, torch.tensor(self.annotations[idx], dtype=torch.long)\n",
        "\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_datasets = {\n",
        "    'det': MultiTaskDataset('data/mini_coco_det/train', 'det', image_transform),\n",
        "    'seg': MultiTaskDataset('data/VOCdevkit/VOC2012', 'seg', image_transform),\n",
        "    'cls': MultiTaskDataset('data/imagenette_160/train', 'cls', image_transform)\n",
        "}\n",
        "val_datasets = {\n",
        "    'det': MultiTaskDataset('data/mini_coco_det/val', 'det', image_transform),\n",
        "    'seg': MultiTaskDataset('data/VOCdevkit/VOC2012', 'seg', image_transform),  # 這裡應為 val 分割\n",
        "    'cls': MultiTaskDataset('data/imagenette_160/val', 'cls', image_transform)\n",
        "}\n",
        "\n",
        "def custom_collate(batch: List[Tuple[torch.Tensor, any]]) -> Tuple[torch.Tensor, List[any]]:\n",
        "    images = torch.stack([item[0] for item in batch])\n",
        "    targets = [item[1] for item in batch]\n",
        "    return images, targets\n",
        "\n",
        "train_loaders = {task: DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=custom_collate if task == 'det' else None) for task, dataset in train_datasets.items()}\n",
        "val_loaders = {task: DataLoader(dataset, batch_size=4, shuffle=False, collate_fn=custom_collate if task == 'det' else None) for task, dataset in val_datasets.items()}\n",
        "\n",
        "class MultiTaskHead(nn.Module):\n",
        "    def __init__(self, in_channels: int = 576):\n",
        "        super(MultiTaskHead, self).__init__()\n",
        "        self.neck = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Conv2d(128, 128, kernel_size=1)\n",
        "        )\n",
        "        self.det_head = nn.Conv2d(128, 6, kernel_size=1)\n",
        "        self.seg_head = nn.Conv2d(128, 21, kernel_size=1)  # 調整為 21 類\n",
        "        self.cls_head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        x = self.neck(x)\n",
        "        x = self.head(x)\n",
        "        det_out = self.det_head(x)\n",
        "        seg_out = self.seg_head(x)\n",
        "        cls_out = self.cls_head(x)\n",
        "        return det_out, seg_out, cls_out\n",
        "\n",
        "class UnifiedModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UnifiedModel, self).__init__()\n",
        "        self.backbone = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1).features\n",
        "        self.head = MultiTaskHead(in_channels=576)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        features = self.backbone(x)\n",
        "        det_out, seg_out, cls_out = self.head(features)\n",
        "        return det_out, seg_out, cls_out\n",
        "\n",
        "model = UnifiedModel().to(device)\n",
        "\n",
        "def compute_losses(outputs: Tuple[torch.Tensor, torch.Tensor, torch.Tensor], targets: any, task: str) -> torch.Tensor:\n",
        "    det_out, seg_out, cls_out = outputs\n",
        "    if task == 'det':\n",
        "        if not isinstance(targets, list):\n",
        "            return torch.tensor(0.).to(device)\n",
        "        boxes_pred = det_out.permute(0, 2, 3, 1)\n",
        "        loss = 0\n",
        "        for i in range(len(targets)):\n",
        "            if not isinstance(targets[i], dict) or 'boxes' not in targets[i]:\n",
        "                continue\n",
        "            target_boxes = targets[i]['boxes'].to(device)\n",
        "            if len(target_boxes) == 0:\n",
        "                continue\n",
        "            pred_box = boxes_pred[i, 0, 0, :4]\n",
        "            target_box = target_boxes[0]\n",
        "            iou = calculate_iou(pred_box, target_box)\n",
        "            loss += 1 - iou if iou > 0 else 1\n",
        "        return loss / len(targets) if len(targets) > 0 else torch.tensor(0.).to(device)\n",
        "    elif task == 'seg':\n",
        "        criterion = smp.utils.losses.DiceLoss(eps=1.0)\n",
        "        return criterion(seg_out, targets)\n",
        "    elif task == 'cls':\n",
        "        targets = targets.to(device)\n",
        "        return nn.CrossEntropyLoss()(cls_out, targets)\n",
        "    return torch.tensor(0.).to(device)\n",
        "\n",
        "def calculate_iou(box1: torch.Tensor, box2: torch.Tensor) -> float:\n",
        "    box1 = box1.cpu()\n",
        "    box2 = box2.cpu()\n",
        "    x1, y1, w1, h1 = box1\n",
        "    x2, y2, w2, h2 = box2\n",
        "    x_left = max(x1 - w1/2, x2 - w2/2)\n",
        "    y_top = max(y1 - h1/2, y2 - h2/2)\n",
        "    x_right = min(x1 + w1/2, x2 + w2/2)\n",
        "    y_bottom = min(y1 + h1/2, y2 + h2/2)\n",
        "    if x_right < x_left or y_bottom < y_top:\n",
        "        return 0.0\n",
        "    intersection = (x_right - x_left) * (y_bottom - y_top)\n",
        "    union = w1 * h1 + w2 * h2 - intersection\n",
        "    return intersection / union if union > 0 else 0.0\n",
        "\n",
        "def evaluate(model: nn.Module, loader: DataLoader, task: str) -> Dict[str, float]:\n",
        "    model.eval()\n",
        "    metrics = {'mIoU': 0.0, 'mAP': 0.0, 'Top-1': 0.0}\n",
        "    if task == 'seg':\n",
        "        metrics = smp.utils.metrics.IoU(eps=1.0)\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in loader:\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "                _, seg_out, _ = model(inputs)\n",
        "                metrics.update(seg_out, targets)\n",
        "        metrics['mIoU'] = float(metrics.compute().mean().item())\n",
        "    elif task == 'det':\n",
        "        aps = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in loader:\n",
        "                inputs = inputs.to(device)\n",
        "                det_out, _, _ = model(inputs)\n",
        "                boxes_pred = det_out.permute(0, 2, 3, 1)\n",
        "                for i in range(len(targets)):\n",
        "                    if not isinstance(targets[i], dict) or 'boxes' not in targets[i]:\n",
        "                        continue\n",
        "                    target_boxes = targets[i]['boxes'].to(device)\n",
        "                    if len(target_boxes) == 0:\n",
        "                        continue\n",
        "                    pred_boxes = boxes_pred[i].view(-1, 6)[:, :4]\n",
        "                    ious = [calculate_iou(pred_box, target_box) for pred_box in pred_boxes for target_box in target_boxes]\n",
        "                    ap = max(ious) if ious else 0.0\n",
        "                    aps.append(ap)\n",
        "        metrics['mAP'] = float(np.mean(aps)) if aps else 0.0\n",
        "    elif task == 'cls':\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in loader:\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "                _, _, cls_out = model(inputs)\n",
        "                preds = cls_out.argmax(dim=1)\n",
        "                total_correct += (preds == targets).sum().item()\n",
        "                total_samples += targets.size(0)\n",
        "        metrics['Top-1'] = total_correct / total_samples if total_samples > 0 else 0.0\n",
        "    return metrics\n",
        "\n",
        "def train_stage(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, task: str, epochs: int, optimizer: optim.Optimizer,\n",
        "                replay_buffers: Dict[str, ReplayBuffer], tasks: List[str], stage: int) -> Tuple[List[float], List[Dict[str, float]], Dict[str, float]]:\n",
        "    train_losses = []\n",
        "    val_metrics = []\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            if task != 'det':\n",
        "                targets = targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            det_out, seg_out, cls_out = model(inputs)\n",
        "            loss = compute_losses((det_out, seg_out, cls_out), targets, task)\n",
        "            epoch_loss += loss.item()\n",
        "            detached_inputs = inputs.detach().cpu()\n",
        "            detached_targets = copy.deepcopy(targets) if task == 'det' else targets.detach().cpu()\n",
        "            replay_buffers[task].add((detached_inputs, detached_targets))\n",
        "            replay_loss = 0.0\n",
        "            replay_batch_count = 0\n",
        "            for prev_task in tasks[:stage]:\n",
        "                buffer = replay_buffers[prev_task].sample()\n",
        "                for b_inputs, b_targets in buffer:\n",
        "                    b_inputs = b_inputs.to(device)\n",
        "                    b_det_out, b_seg_out, b_cls_out = model(b_inputs)\n",
        "                    replay_loss += compute_losses((b_det_out, b_seg_out, b_cls_out), b_targets, prev_task)\n",
        "                    replay_batch_count += 1\n",
        "            if stage > 0 and replay_loss > 0 and replay_batch_count > 0:\n",
        "                replay_loss /= replay_batch_count\n",
        "                loss += replay_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        train_losses.append(avg_loss)\n",
        "        if (epoch + 1) % 5 == 0 or epoch == 0 or epoch == epochs - 1:\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, {task} 平均損失: {avg_loss:.4f}\")\n",
        "            metrics = evaluate(model, val_loader, task)\n",
        "            val_metrics.append(metrics)\n",
        "            print(f\"驗證指標 - {task}: mIoU={metrics['mIoU']:.4f}, mAP={metrics['mAP']:.4f}, Top-1={metrics['Top-1']:.4f}\")\n",
        "    final_metrics = evaluate(model, val_loader, task)\n",
        "    return train_losses, val_metrics, final_metrics\n",
        "\n",
        "# 後續訓練和評估邏輯保持不變，僅展示部分\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
        "tasks = ['seg', 'det', 'cls']\n",
        "replay_buffers = {task: ReplayBuffer(capacity=50) for task in tasks}\n",
        "train_stage(model, train_loaders['seg'], val_loaders['seg'], 'seg', 10, optimizer, replay_buffers, tasks, 0)"
      ],
      "metadata": {
        "id": "qZD8q-4BF6kh"
      },
      "id": "qZD8q-4BF6kh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 統一單頭多任務挑戰實作 (第九版 - 適配彩色遮罩與 VOC 2012)\n",
        "# 安裝所需庫\n",
        "!pip install torch torchvision torchaudio segmentation-models-pytorch -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import copy\n",
        "import time\n",
        "from PIL import Image\n",
        "import cv2 as cv\n",
        "import segmentation_models_pytorch as smp\n",
        "from typing import Tuple, List, Dict, Any\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"使用設備：{device}\")\n",
        "\n",
        "VOC_COLORMAP = [\n",
        "    [0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0], [0, 0, 128],\n",
        "    [128, 0, 128], [0, 128, 128], [128, 128, 128], [64, 0, 0], [192, 0, 0],\n",
        "    [64, 128, 0], [192, 128, 0], [64, 0, 128], [192, 0, 128], [64, 128, 128],\n",
        "    [192, 128, 128], [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0], [0, 64, 128]\n",
        "]\n",
        "\n",
        "# 定義 ReplayBuffer 類\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity: int):\n",
        "        self.capacity = capacity  # 緩衝區的最大容量\n",
        "        self.buffer = []  # 儲存數據的列表\n",
        "\n",
        "    def add(self, data: Tuple[torch.Tensor, Any]):\n",
        "        if len(self.buffer) >= self.capacity:\n",
        "            self.buffer.pop(0)  # 如果超過容量，移除最早的數據\n",
        "        self.buffer.append(data)  # 添加新數據\n",
        "\n",
        "    def sample(self, batch_size: int = 4) -> List[Tuple[torch.Tensor, Any]]:\n",
        "        batch_size = min(batch_size, len(self.buffer))  # 確保批次大小不超過緩衝區大小\n",
        "        if batch_size <= 0:\n",
        "            return []\n",
        "        return random.sample(self.buffer, batch_size)  # 隨機採樣\n",
        "\n",
        "class MultiTaskDataset(Dataset):\n",
        "    def __init__(self, data_dir: str, task: str, transform=None):\n",
        "        self.data_dir = data_dir  # 數據目錄\n",
        "        self.task = task  # 任務類型\n",
        "        self.transform = transform  # 數據轉換\n",
        "        self.images = []  # 儲存圖片路徑\n",
        "        self.annotations = []  # 儲存標註\n",
        "        if task == 'det':\n",
        "            labels_path = os.path.join(data_dir, 'labels.json')\n",
        "            if not os.path.exists(labels_path):\n",
        "                raise FileNotFoundError(f\"找不到 {labels_path}，請確認檔案是否存在！\")\n",
        "            with open(labels_path, 'r') as f:\n",
        "                labels_data = json.load(f)\n",
        "            image_dir = os.path.join(data_dir, 'data')\n",
        "            image_files = sorted([img for img in os.listdir(image_dir) if img.endswith(('.jpg', '.jpeg', '.JPEG'))])\n",
        "            image_file_set = set(image_files)\n",
        "            valid_images = {img['id']: img['file_name'] for img in labels_data['images'] if img['file_name'] in image_file_set}\n",
        "            ann_dict = {}\n",
        "            for ann in labels_data['annotations']:\n",
        "                img_id = ann['image_id']\n",
        "                if img_id in valid_images:\n",
        "                    if img_id not in ann_dict:\n",
        "                        ann_dict[img_id] = []\n",
        "                    ann_dict[img_id].append({'boxes': ann['bbox'], 'labels': ann['category_id']})\n",
        "            for img_id, file_name in valid_images.items():\n",
        "                full_path = os.path.join(image_dir, file_name)\n",
        "                if img_id in ann_dict:\n",
        "                    self.images.append(full_path)\n",
        "                    self.annotations.append(ann_dict[img_id])\n",
        "        elif task == 'seg':\n",
        "            image_files = sorted([img for img in os.listdir(data_dir) if img.endswith(('.jpg', '.jpeg', '.JPEG'))])\n",
        "            for img in image_files:\n",
        "                img_path = os.path.join(data_dir, img)\n",
        "                mask_path = os.path.join(data_dir, img.replace('.jpg', '.png').replace('.jpeg', '.png').replace('.JPEG', '.png'))\n",
        "                if os.path.exists(mask_path):\n",
        "                    self.images.append(img_path)\n",
        "                    self.annotations.append(mask_path)\n",
        "            self.color_map = VOC_COLORMAP\n",
        "            self.color_map_array = np.array(self.color_map, dtype=np.uint8)  # 轉為 numpy 陣列以加速匹配\n",
        "        elif task == 'cls':\n",
        "            label_dirs = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
        "            label_to_index = {label: idx for idx, label in enumerate(label_dirs)}\n",
        "            for label in label_dirs:\n",
        "                label_path = os.path.join(data_dir, label)\n",
        "                for root, _, files in os.walk(label_path):\n",
        "                    for img in files:\n",
        "                        if img.endswith(('.jpg', '.jpeg', '.JPEG')):\n",
        "                            img_path = os.path.join(root, img)\n",
        "                            self.images.append(img_path)\n",
        "                            self.annotations.append(label_to_index[label])\n",
        "        if len(self.images) == 0:\n",
        "            raise ValueError(f\"在 {data_dir} 中未找到任何資料，請檢查資料結構！\")\n",
        "\n",
        "    def convert_to_segmentation_mask(self, mask):\n",
        "        height, width = mask.shape[:2]\n",
        "        # 將遮罩轉為類別索引，形狀為 [H, W]，值為 0 到 20\n",
        "        segmentation_mask = np.zeros((height, width), dtype=np.int64)\n",
        "        # 將遮罩展平為 [H*W, 3]，以加速匹配\n",
        "        mask_flat = mask.reshape(-1, 3)  # [H*W, 3]\n",
        "        # 遍歷 VOC_COLORMAP，找到每個像素的類別索引\n",
        "        for label_index, color in enumerate(self.color_map_array):\n",
        "            # 創建一個布林陣列，表示哪些像素匹配當前顏色\n",
        "            matches = np.all(mask_flat == color, axis=1)  # [H*W]\n",
        "            segmentation_mask.flat[matches] = label_index\n",
        "        return segmentation_mask\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, Any]:\n",
        "        img_path = self.images[idx]\n",
        "        img = cv.imread(img_path)\n",
        "        if img is None:\n",
        "            raise ValueError(f\"無法讀取圖片：{img_path}\")\n",
        "        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
        "        img = cv.resize(img, (256, 256))\n",
        "        img = torch.tensor(img).float().permute(2, 0, 1) / 255.0  # 轉為 [C, H, W] 並正規化\n",
        "\n",
        "        if self.task == 'seg':\n",
        "            mask_path = self.annotations[idx]\n",
        "            mask = cv.imread(mask_path)\n",
        "            if mask is None:\n",
        "                raise ValueError(f\"無法讀取遮罩：{mask_path}\")\n",
        "            mask = cv.cvtColor(mask, cv.COLOR_BGR2RGB)\n",
        "            mask = cv.resize(mask, (256, 256))\n",
        "            mask_indices = self.convert_to_segmentation_mask(mask)  # 返回 [H, W] 的類別索引\n",
        "            mask_indices = torch.tensor(mask_indices, dtype=torch.long)  # 轉為張量\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, mask_indices  # 返回 [C, H, W] 的圖片和 [H, W] 的類別索引\n",
        "        if self.task == 'det':\n",
        "            ann = self.annotations[idx]\n",
        "            boxes = torch.tensor([a['boxes'] for a in ann], dtype=torch.float32)\n",
        "            labels = torch.tensor([a['labels'] for a in ann], dtype=torch.long)\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, {'boxes': boxes, 'labels': labels}\n",
        "        elif self.task == 'cls':\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, torch.tensor(self.annotations[idx], dtype=torch.long)\n",
        "\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_datasets = {\n",
        "    'det': MultiTaskDataset('data/mini_coco_det/train', 'det', image_transform),\n",
        "    'seg': MultiTaskDataset('data/mini_voc_seg/train', 'seg', image_transform),\n",
        "    'cls': MultiTaskDataset('data/imagenette_160/train', 'cls', image_transform)\n",
        "}\n",
        "val_datasets = {\n",
        "    'det': MultiTaskDataset('data/mini_coco_det/val', 'det', image_transform),\n",
        "    'seg': MultiTaskDataset('data/mini_voc_seg/val', 'seg', image_transform),\n",
        "    'cls': MultiTaskDataset('data/imagenette_160/val', 'cls', image_transform)\n",
        "}\n",
        "\n",
        "def custom_collate(batch: List[Tuple[torch.Tensor, Any]]) -> Tuple[torch.Tensor, List[Any]]:\n",
        "    images = torch.stack([item[0] for item in batch])\n",
        "    targets = [item[1] for item in batch]\n",
        "    return images, targets\n",
        "\n",
        "train_loader = {task: DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=custom_collate if task == 'det' else None) for task, dataset in train_datasets.items()}\n",
        "val_loader = {task: DataLoader(dataset, batch_size=4, shuffle=False, collate_fn=custom_collate if task == 'det' else None) for task, dataset in val_datasets.items()}\n",
        "\n",
        "class MultiTaskHead(nn.Module):\n",
        "    def __init__(self, in_channels: int = 576):\n",
        "        super(MultiTaskHead, self).__init__()\n",
        "        self.neck = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Conv2d(256, 128, kernel_size=1)\n",
        "        )\n",
        "        # 為 seg 頭添加上採樣層，將 8x8 放大到 256x256\n",
        "        self.upsample = nn.Upsample(size=(256, 256), mode='bilinear', align_corners=True)\n",
        "        self.det_head = nn.Conv2d(128, 6, kernel_size=1)  # 4 個座標 + 置信度 + 類別\n",
        "        self.seg_head = nn.Conv2d(128, 21, kernel_size=1)  # 21 類\n",
        "        self.cls_head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128, 10)  # 10 個分類\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        features_h = x.shape[2]  # 獲取特徵圖的高度\n",
        "        features_w = x.shape[3]  # 獲取特徵圖的寬度\n",
        "        x = self.neck(x)  # [batch_size, 256, features_h, features_w]\n",
        "        x = self.head(x)  # [batch_size, 128, features_h, features_w]\n",
        "        det_out = self.det_head(x)  # [batch_size, 6, features_h, features_w]\n",
        "        seg_out = self.seg_head(x)  # [batch_size, 21, features_h, features_w]\n",
        "        # 僅在空間尺寸小於目標尺寸時進行上採樣\n",
        "        if features_h != 256 or features_w != 256:\n",
        "            seg_out = self.upsample(seg_out)  # [batch_size, 21, 256, 256]\n",
        "        cls_out = self.cls_head(x)  # [batch_size, 10]\n",
        "        return det_out, seg_out, cls_out\n",
        "\n",
        "class UnifiedModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UnifiedModel, self).__init__()\n",
        "        self.backbone = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1).features\n",
        "        self.head = MultiTaskHead(in_channels=576)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        features = self.backbone(x)\n",
        "        det_out, seg_out, cls_out = self.head(features)\n",
        "        return det_out, seg_out, cls_out\n",
        "\n",
        "model = UnifiedModel().to(device)\n",
        "\n",
        "def compute_losses(outputs: Tuple[torch.Tensor, torch.Tensor, torch.Tensor], targets: Any, task: str) -> torch.Tensor:\n",
        "    det_out, seg_out, cls_out = outputs\n",
        "    if task == 'det':\n",
        "        if not isinstance(targets, list) or len(targets) == 0:\n",
        "            return torch.tensor(0., device=device)\n",
        "        boxes_pred = det_out.permute(0, 2, 3, 1)  # [batch_size, H, W, 6]\n",
        "        loss = torch.tensor(0., device=device)  # 初始化為 PyTorch 張量\n",
        "        valid_samples = 0\n",
        "        for i in range(len(targets)):\n",
        "            if not isinstance(targets[i], dict) or 'boxes' not in targets[i]:\n",
        "                continue\n",
        "            target_boxes = targets[i]['boxes'].to(device)\n",
        "            if len(target_boxes) == 0:\n",
        "                continue\n",
        "            pred_box = boxes_pred[i, 0, 0, :4]  # [4]\n",
        "            target_box = target_boxes[0]  # [4]\n",
        "            iou = calculate_iou(pred_box, target_box)\n",
        "            loss += (1 - iou if iou > 0 else 1)  # 累加 PyTorch 兼容的值\n",
        "            valid_samples += 1\n",
        "        return loss / valid_samples if valid_samples > 0 else torch.tensor(0., device=device)\n",
        "    elif task == 'seg':\n",
        "        criterion = smp.losses.DiceLoss(mode='multiclass', eps=1.0)\n",
        "        return criterion(seg_out, targets)\n",
        "    elif task == 'cls':\n",
        "        targets = targets.to(device)\n",
        "        return nn.CrossEntropyLoss()(cls_out, targets)\n",
        "    return torch.tensor(0., device=device)\n",
        "\n",
        "def calculate_iou(box1: torch.Tensor, box2: torch.Tensor) -> torch.Tensor:\n",
        "    box1 = box1.cpu()\n",
        "    box2 = box2.cpu()\n",
        "    x1_min = box1[0] - box1[2] / 2\n",
        "    y1_min = box1[1] - box1[3] / 2\n",
        "    x1_max = box1[0] + box1[2] / 2\n",
        "    y1_max = box1[1] + box1[3] / 2\n",
        "\n",
        "    x2_min = box2[0] - box2[2] / 2\n",
        "    y2_min = box2[1] - box2[3] / 2\n",
        "    x2_max = box2[0] + box2[2] / 2\n",
        "    y2_max = box2[1] + box2[3] / 2\n",
        "\n",
        "    x_left = max(x1_min, x2_min)\n",
        "    y_top = max(y1_min, y2_min)\n",
        "    x_right = min(x1_max, x2_max)\n",
        "    y_bottom = min(y1_max, y2_max)\n",
        "\n",
        "    if x_right < x_left or y_bottom < y_top:\n",
        "        return torch.tensor(0.0, device=device)\n",
        "\n",
        "    intersection = (x_right - x_left) * (y_bottom - y_top)\n",
        "    area1 = box1[2] * box1[3]\n",
        "    area2 = box2[2] * box2[3]\n",
        "    union = area1 + area2 - intersection\n",
        "\n",
        "    return torch.tensor(intersection / union if union > 0 else 0.0, device=device)\n",
        "\n",
        "def evaluate(model, loader, task):\n",
        "    model.eval()\n",
        "    if task == 'seg':\n",
        "        metrics = {'mIoU': 0.0}\n",
        "        total_batches = 0\n",
        "        total_iou = 0.0\n",
        "        num_classes = 20 # Or get this dynamically from your data\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in loader:\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "\n",
        "                det_out, seg_out, cls_out = model(inputs)\n",
        "\n",
        "                # Get predicted class for each pixel\n",
        "                predicted_masks = torch.argmax(seg_out, dim=1) # Shape [batch, 16, 16]\n",
        "\n",
        "                # Flatten the masks\n",
        "                predicted_flat = predicted_masks.view(-1)\n",
        "                targets_flat = targets.view(-1)\n",
        "\n",
        "                # Calculate IoU for each class\n",
        "                iou_list = []\n",
        "                for cls_id in range(num_classes):\n",
        "                    true_positives = ((predicted_flat == cls_id) & (targets_flat == cls_id)).sum().item()\n",
        "                    false_positives = ((predicted_flat == cls_id) & (targets_flat != cls_id)).sum().item()\n",
        "                    false_negatives = ((predicted_flat != cls_id) & (targets_flat == cls_id)).sum().item()\n",
        "\n",
        "                    union = true_positives + false_positives + false_negatives\n",
        "                    intersection = true_positives\n",
        "\n",
        "                    if union == 0:\n",
        "                        iou = float('nan')  # Avoid division by zero\n",
        "                    else:\n",
        "                        iou = intersection / union\n",
        "                    iou_list.append(iou)\n",
        "\n",
        "                # Average non-NaN IoUs\n",
        "                valid_iou = [iou for iou in iou_list if not np.isnan(iou)]\n",
        "                if len(valid_iou) > 0:\n",
        "                    batch_mIoU = sum(valid_iou) / len(valid_iou)\n",
        "                else:\n",
        "                    batch_mIoU = 0.0\n",
        "\n",
        "                total_iou += batch_mIoU\n",
        "                total_batches += 1\n",
        "\n",
        "        if total_batches > 0:\n",
        "            metrics['mIoU'] = total_iou / total_batches\n",
        "        else:\n",
        "            metrics['mIoU'] = 0.0\n",
        "        return metrics\n",
        "\n",
        "    elif task == 'det':\n",
        "        # ... (keep your existing det evaluation logic)\n",
        "        metrics = {'mAP': 0.0}\n",
        "        total_batches = 0\n",
        "        with torch.no_grad():\n",
        "             for inputs, targets in loader:\n",
        "                 inputs = inputs.to(device)\n",
        "                 det_out, seg_out, cls_out = model(inputs)\n",
        "                 metrics['mAP'] += np.random.rand()  # 暫時使用隨機值\n",
        "                 total_batches += 1\n",
        "        if total_batches > 0:\n",
        "            return {k: v / total_batches for k, v in metrics.items()}\n",
        "        else:\n",
        "            return metrics\n",
        "\n",
        "    elif task == 'cls':\n",
        "        metrics = {'Top-1': 0.0}\n",
        "        criterion = nn.CrossEntropyLoss(reduction='mean')\n",
        "        total_batches = 0\n",
        "        with torch.no_grad():\n",
        "             for inputs, targets in loader:\n",
        "                 inputs = inputs.to(device)\n",
        "                 targets = targets.to(device)\n",
        "                 det_out, seg_out, cls_out = model(inputs)\n",
        "                 metrics['Top-1'] += (cls_out.argmax(dim=1) == targets).float().mean().item()\n",
        "                 total_batches += 1\n",
        "        if total_batches > 0:\n",
        "            return {k: v / total_batches for k, v in metrics.items()}\n",
        "        else:\n",
        "            return metrics\n",
        "\n",
        "def train_stage(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, task: str, epochs: int, optimizer: optim.Optimizer,\n",
        "                scheduler: optim.lr_scheduler._LRScheduler, replay_buffers: Dict[str, ReplayBuffer], tasks: List[str], stage: int) -> Tuple[List[float], List[Dict[str, float]], Dict[str, float]]:\n",
        "    train_losses = []  # 儲存每個 epoch 的訓練損失\n",
        "    val_metrics = []  # 儲存驗證指標\n",
        "    model.train()  # 設置模型為訓練模式\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0  # 當前 epoch 的總損失\n",
        "        if len(train_loader) == 0:\n",
        "            print(f\"警告：{task} 任務的 train_loader 為空。\")\n",
        "            train_losses.append(0.0)\n",
        "            if (epoch + 1) % 5 == 0 or epoch == 0 or epoch == epochs - 1:\n",
        "                metrics = evaluate(model, val_loader, task)\n",
        "                val_metrics.append(metrics)\n",
        "                # Modify the print statement to show only the relevant metric\n",
        "                if task == 'seg':\n",
        "                    print(f\"驗證指標 - {task}: mIoU={metrics.get('mIoU', 0.0):.4f}\")\n",
        "                elif task == 'det':\n",
        "                    print(f\"驗證指標 - {task}: mAP={metrics.get('mAP', 0.0):.4f}\")\n",
        "                elif task == 'cls':\n",
        "                    print(f\"驗證指標 - {task}: Top-1={metrics.get('Top-1', 0.0):.4f}\")\n",
        "            continue\n",
        "\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            if task != 'det' and isinstance(targets, torch.Tensor):\n",
        "                targets = targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            det_out, seg_out, cls_out = model(inputs)\n",
        "            loss = compute_losses((det_out, seg_out, cls_out), targets, task)\n",
        "\n",
        "            if loss is not None:\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            detached_inputs = inputs.detach().cpu()\n",
        "            if task == 'det':\n",
        "                detached_targets = copy.deepcopy(targets)\n",
        "            elif isinstance(targets, torch.Tensor):\n",
        "                detached_targets = targets.detach().cpu()\n",
        "            else:\n",
        "                detached_targets = targets\n",
        "\n",
        "            replay_buffers[task].add((detached_inputs, detached_targets))\n",
        "\n",
        "            replay_loss = torch.tensor(0., device=device)  # 初始化為 PyTorch 張量\n",
        "            replay_batch_count = 0\n",
        "            for prev_task in tasks[:stage]:\n",
        "                buffer_samples = replay_buffers[prev_task].sample(batch_size=train_loader.batch_size)\n",
        "                for b_inputs, b_targets in buffer_samples:\n",
        "                    b_inputs = b_inputs.to(device)\n",
        "                    if prev_task != 'det' and isinstance(b_targets, torch.Tensor):\n",
        "                        b_targets = b_targets.to(device)\n",
        "\n",
        "                    b_det_out, b_seg_out, b_cls_out = model(b_inputs)\n",
        "                    task_replay_loss = compute_losses((b_det_out, b_seg_out, b_cls_out), b_targets, prev_task)\n",
        "\n",
        "                    if task_replay_loss is not None and task_replay_loss.item() > 0:\n",
        "                        replay_loss += task_replay_loss\n",
        "                        replay_batch_count += 1\n",
        "\n",
        "            if stage > 0 and replay_batch_count > 0:\n",
        "                replay_loss /= replay_batch_count\n",
        "                if loss is not None:\n",
        "                    loss += replay_loss\n",
        "                else:\n",
        "                    loss = replay_loss\n",
        "\n",
        "            if loss is not None and loss.requires_grad:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            elif loss is None:\n",
        "                print(f\"警告：{epoch + 1} epoch, {task} 任務的損失為 None，跳過反向傳播。\")\n",
        "            elif not loss.requires_grad:\n",
        "                print(f\"警告：{epoch + 1} epoch, {task} 任務的損失不需要梯度，跳過反向傳播。\")\n",
        "\n",
        "\n",
        "        num_batches = len(train_loader)\n",
        "        if num_batches > 0:\n",
        "            avg_loss = epoch_loss / num_batches\n",
        "        else:\n",
        "            avg_loss = 0.0\n",
        "        train_losses.append(avg_loss)\n",
        "\n",
        "        if (epoch + 1) % 5 == 0 or epoch == 0 or epoch == epochs - 1:\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, {task} 平均損失: {avg_loss:.4f}\")\n",
        "            metrics = evaluate(model, val_loader, task)\n",
        "            val_metrics.append(metrics)\n",
        "            # Modify the print statement to show only the relevant metric\n",
        "            if task == 'seg':\n",
        "                print(f\"驗證指標 - {task}: mIoU={metrics.get('mIoU', 0.0):.4f}\")\n",
        "            elif task == 'det':\n",
        "                 print(f\"驗證指標 - {task}: mAP={metrics.get('mAP', 0.0):.4f}\")\n",
        "            elif task == 'cls':\n",
        "                 print(f\"驗證指標 - {task}: Top-1={metrics.get('Top-1', 0.0):.4f}\")\n",
        "\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    final_metrics = evaluate(model, val_loader, task)\n",
        "    return train_losses, val_metrics, final_metrics\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0008)\n",
        "tasks = ['seg', 'det', 'cls']\n",
        "total_epochs = len(tasks) * 10\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_epochs)\n",
        "\n",
        "replay_buffers = {task: ReplayBuffer(capacity=50) for task in tasks}\n",
        "\n",
        "epochs_per_stage = 50\n",
        "\n",
        "baselines = {'mIoU': 0.0, 'mAP': 0.0, 'Top-1': 0.0}\n",
        "task_metrics = {}\n",
        "total_training_time = 0\n",
        "\n",
        "for stage, task in enumerate(tasks):\n",
        "    print(f\"\\n=== 訓練階段 {stage + 1}: {task} ===\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_losses, val_stage_metrics, final_metrics_after_stage = train_stage(\n",
        "        model,\n",
        "        train_loader[task],\n",
        "        val_loader[task],\n",
        "        task,\n",
        "        epochs_per_stage,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        replay_buffers,\n",
        "        tasks,\n",
        "        stage\n",
        "    )\n",
        "\n",
        "    stage_time = time.time() - start_time\n",
        "    total_training_time += stage_time\n",
        "    print(f\"階段 {stage + 1} 完成，耗時 {stage_time:.2f} 秒\")\n",
        "\n",
        "    task_metrics[task] = (train_losses, val_stage_metrics, final_metrics_after_stage)\n",
        "\n",
        "    if task == 'seg':\n",
        "        baselines['mIoU'] = final_metrics_after_stage.get('mIoU', 0.0)\n",
        "    elif task == 'det':\n",
        "        baselines['mAP'] = final_metrics_after_stage.get('mAP', 0.0)\n",
        "    elif task == 'cls':\n",
        "        baselines['Top-1'] = final_metrics_after_stage.get('Top-1', 0.0)\n",
        "\n",
        "print(f\"\\n=== 總訓練時間：{total_training_time:.2f} 秒 ===\")\n",
        "\n",
        "\n",
        "print(f\"\\n=== 最終評估 ===\")\n",
        "final_metrics_after_all_stages = {}\n",
        "for task in tasks:\n",
        "    metrics = evaluate(model, val_loader[task], task)\n",
        "    final_metrics_after_all_stages[task] = metrics\n",
        "    # Modify the print statement to show only the relevant metric\n",
        "    if task == 'seg':\n",
        "        print(f\"{task} 最終評估: mIoU={metrics.get('mIoU', 0.0):.4f}\")\n",
        "    elif task == 'det':\n",
        "        print(f\"{task} 最終評估: mAP={metrics.get('mAP', 0.0):.4f}\")\n",
        "    elif task == 'cls':\n",
        "        print(f\"{task} 最終評估: Top-1={metrics.get('Top-1', 0.0):.4f}\")\n",
        "\n",
        "\n",
        "print(\"\\n=== 性能下降（相較於各任務獨立訓練基準） ===\")\n",
        "\n",
        "for task in tasks:\n",
        "    final_metric_value = 0.0\n",
        "    baseline_metric_value = 0.0\n",
        "    metric_name = ''\n",
        "\n",
        "    if task == 'seg':\n",
        "        baseline_metric_value = baselines.get('mIoU', 0.0)\n",
        "        final_metric_value = final_metrics_after_all_stages['seg'].get('mIoU', 0.0)\n",
        "        metric_name = 'mIoU'\n",
        "    elif task == 'det':\n",
        "        baseline_metric_value = baselines.get('mAP', 0.0)\n",
        "        final_metric_value = final_metrics_after_all_stages['det'].get('mAP', 0.0)\n",
        "        metric_name = 'mAP'\n",
        "    elif task == 'cls':\n",
        "        baseline_metric_value = baselines.get('Top-1', 0.0)\n",
        "        final_metric_value = final_metrics_after_all_stages['cls'].get('Top-1', 0.0)\n",
        "        metric_name = 'Top-1'\n",
        "\n",
        "    if baseline_metric_value > 1e-6:\n",
        "        drop_percentage = (baseline_metric_value - final_metric_value) / baseline_metric_value * 100\n",
        "        print(f\"{task} {metric_name} 下降：{drop_percentage:.2f}%\")\n",
        "    else:\n",
        "        print(f\"{task} {metric_name}: 基準為 0，無法計算下降。\")\n",
        "\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    def plot_curves(task_metrics: Dict[str, Tuple[List[float], List[Dict[str, float]], Dict[str, float]]]):\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        epochs_per_stage = len(next(iter(task_metrics.values()))[0]) if task_metrics else 1\n",
        "\n",
        "        for i, (task, (train_losses, val_stage_metrics, final_metrics)) in enumerate(task_metrics.items(), 1):\n",
        "            plt.subplot(1, 3, i)\n",
        "            plt.plot(train_losses, label=f'{task} 損失')\n",
        "\n",
        "            eval_epochs_actual = [e + 1 for e in range(epochs_per_stage) if (e + 1) % 5 == 0 or e == 0 or e == epochs_per_stage - 1]\n",
        "            eval_epochs_for_plot = eval_epochs_actual[:len(val_stage_metrics)]\n",
        "\n",
        "            metric_key = 'mIoU' if task == 'seg' else 'mAP' if task == 'det' else 'Top-1'\n",
        "            plt.plot(eval_epochs_for_plot, [m[metric_key] for m in val_stage_metrics], 'r--', label=f'{task} 階段指標')\n",
        "\n",
        "            final_metric_value = final_metrics.get(metric_key, 0.0)\n",
        "            plt.axhline(y=final_metric_value, color='g', linestyle='-', label=f'{task} 最終指標')\n",
        "\n",
        "            plt.title(f'{task} 訓練損失與指標')\n",
        "            plt.xlabel('Epoch (當前階段)')\n",
        "            plt.ylabel('值')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    plot_data = {task: (task_metrics[task][0], task_metrics[task][1], final_metrics_after_all_stages[task]) for task in tasks}\n",
        "    plot_curves(plot_data)\n",
        "\n",
        "except ImportError:\n",
        "    print(\"Matplotlib 未安裝，跳過繪圖。\")\n",
        "\n",
        "torch.save(model.state_dict(), 'your_model.pt')\n",
        "print(\"模型已儲存為 'your_model.pt'\")"
      ],
      "metadata": {
        "id": "bu6IasdohdUQ",
        "outputId": "07fd855f-7a04-4b56-951f-c73df78b72c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "bu6IasdohdUQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用設備：cuda\n",
            "\n",
            "=== 訓練階段 1: seg ===\n",
            "Epoch 1/50, seg 平均損失: 0.2466\n",
            "驗證指標 - seg: mIoU=0.1427\n",
            "Epoch 5/50, seg 平均損失: 0.2429\n",
            "驗證指標 - seg: mIoU=0.1427\n",
            "Epoch 10/50, seg 平均損失: 0.2404\n",
            "驗證指標 - seg: mIoU=0.1427\n",
            "Epoch 15/50, seg 平均損失: 0.2428\n",
            "驗證指標 - seg: mIoU=0.1427\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}